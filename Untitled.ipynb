{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab445516",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 20:12:37.889938: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-08 20:12:37.890188: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-08 20:12:38.338510: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-08 20:12:39.511395: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-08 20:12:44.704970: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, precision_score, recall_score\n",
    "import xgboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "631a5638",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06b62e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "df = pd.read_excel(f'{path}/data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f22d3af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HB</th>\n",
       "      <th>Ultimate strength, Gpa</th>\n",
       "      <th>Youngs modulus, Gpa</th>\n",
       "      <th>Density, g/cc</th>\n",
       "      <th>Specific Heat of capacity, J/g*C</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>N</th>\n",
       "      <th>O</th>\n",
       "      <th>Mg</th>\n",
       "      <th>...</th>\n",
       "      <th>Zr</th>\n",
       "      <th>Ce</th>\n",
       "      <th>W</th>\n",
       "      <th>Pb</th>\n",
       "      <th>Sb</th>\n",
       "      <th>Bi</th>\n",
       "      <th>Cd</th>\n",
       "      <th>Te</th>\n",
       "      <th>Sn</th>\n",
       "      <th>Ta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>146</td>\n",
       "      <td>0.540</td>\n",
       "      <td>200</td>\n",
       "      <td>7.870</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95</td>\n",
       "      <td>0.330</td>\n",
       "      <td>206</td>\n",
       "      <td>7.872</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>167</td>\n",
       "      <td>0.540</td>\n",
       "      <td>200</td>\n",
       "      <td>7.870</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>105</td>\n",
       "      <td>0.365</td>\n",
       "      <td>205</td>\n",
       "      <td>7.870</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>0.370</td>\n",
       "      <td>205</td>\n",
       "      <td>7.870</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    HB  Ultimate strength, Gpa  Youngs modulus, Gpa  Density, g/cc  \\\n",
       "0  146                   0.540                  200          7.870   \n",
       "1   95                   0.330                  206          7.872   \n",
       "2  167                   0.540                  200          7.870   \n",
       "3  105                   0.365                  205          7.870   \n",
       "4  105                   0.370                  205          7.870   \n",
       "\n",
       "   Specific Heat of capacity, J/g*C    B     C    N    O   Mg  ...   Zr   Ce  \\\n",
       "0                             0.440  0.0  0.00  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1                             0.481  0.0  0.08  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "2                             0.472  0.0  0.13  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "3                             0.448  0.0  0.13  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "4                             0.472  0.0  0.15  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "     W   Pb   Sb   Bi   Cd   Te   Sn   Ta  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "999f718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6649c9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HB</th>\n",
       "      <th>Ultimate strength, Gpa</th>\n",
       "      <th>Youngs modulus, Gpa</th>\n",
       "      <th>Density, g/cc</th>\n",
       "      <th>Specific Heat of capacity, J/g*C</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>N</th>\n",
       "      <th>O</th>\n",
       "      <th>Mg</th>\n",
       "      <th>...</th>\n",
       "      <th>Zr</th>\n",
       "      <th>Ce</th>\n",
       "      <th>W</th>\n",
       "      <th>Pb</th>\n",
       "      <th>Sb</th>\n",
       "      <th>Bi</th>\n",
       "      <th>Cd</th>\n",
       "      <th>Te</th>\n",
       "      <th>Sn</th>\n",
       "      <th>Ta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>146</td>\n",
       "      <td>0.540</td>\n",
       "      <td>200</td>\n",
       "      <td>7.870</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95</td>\n",
       "      <td>0.330</td>\n",
       "      <td>206</td>\n",
       "      <td>7.872</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>167</td>\n",
       "      <td>0.540</td>\n",
       "      <td>200</td>\n",
       "      <td>7.870</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>105</td>\n",
       "      <td>0.365</td>\n",
       "      <td>205</td>\n",
       "      <td>7.870</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>0.370</td>\n",
       "      <td>205</td>\n",
       "      <td>7.870</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    HB  Ultimate strength, Gpa  Youngs modulus, Gpa  Density, g/cc  \\\n",
       "0  146                   0.540                  200          7.870   \n",
       "1   95                   0.330                  206          7.872   \n",
       "2  167                   0.540                  200          7.870   \n",
       "3  105                   0.365                  205          7.870   \n",
       "4  105                   0.370                  205          7.870   \n",
       "\n",
       "   Specific Heat of capacity, J/g*C    B     C    N    O   Mg  ...   Zr   Ce  \\\n",
       "0                             0.440  0.0  0.00  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1                             0.481  0.0  0.08  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "2                             0.472  0.0  0.13  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "3                             0.448  0.0  0.13  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "4                             0.472  0.0  0.15  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "     W   Pb   Sb   Bi   Cd   Te   Sn   Ta  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c00186c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_dim = 5\n",
    "output_dim = len(df.columns) - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d06b42e3-d5d3-4f59-9f89-7b98be4ec353",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names_of_cols = [f'has_{i}' for i in df[df.columns[5:]]]\n",
    "df[names_of_cols] = df.loc[:, df.columns[5:]].apply(lambda x: x.map(lambda t: 1 if t>0 else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4db09986-440b-4767-8db1-1d311c68f160",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HB</th>\n",
       "      <th>Ultimate strength, Gpa</th>\n",
       "      <th>Youngs modulus, Gpa</th>\n",
       "      <th>Density, g/cc</th>\n",
       "      <th>Specific Heat of capacity, J/g*C</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>N</th>\n",
       "      <th>O</th>\n",
       "      <th>Mg</th>\n",
       "      <th>...</th>\n",
       "      <th>has_Zr</th>\n",
       "      <th>has_Ce</th>\n",
       "      <th>has_W</th>\n",
       "      <th>has_Pb</th>\n",
       "      <th>has_Sb</th>\n",
       "      <th>has_Bi</th>\n",
       "      <th>has_Cd</th>\n",
       "      <th>has_Te</th>\n",
       "      <th>has_Sn</th>\n",
       "      <th>has_Ta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>146</td>\n",
       "      <td>0.540</td>\n",
       "      <td>200</td>\n",
       "      <td>7.870</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95</td>\n",
       "      <td>0.330</td>\n",
       "      <td>206</td>\n",
       "      <td>7.872</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0800</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>167</td>\n",
       "      <td>0.540</td>\n",
       "      <td>200</td>\n",
       "      <td>7.870</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1300</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>105</td>\n",
       "      <td>0.365</td>\n",
       "      <td>205</td>\n",
       "      <td>7.870</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1300</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>0.370</td>\n",
       "      <td>205</td>\n",
       "      <td>7.870</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>478</td>\n",
       "      <td>1.870</td>\n",
       "      <td>188</td>\n",
       "      <td>7.990</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>256</td>\n",
       "      <td>0.917</td>\n",
       "      <td>199</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0301</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>188</td>\n",
       "      <td>0.709</td>\n",
       "      <td>195</td>\n",
       "      <td>7.860</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0865</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>160</td>\n",
       "      <td>0.345</td>\n",
       "      <td>77</td>\n",
       "      <td>6.450</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>307</td>\n",
       "      <td>1.100</td>\n",
       "      <td>200</td>\n",
       "      <td>7.930</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     HB  Ultimate strength, Gpa  Youngs modulus, Gpa  Density, g/cc  \\\n",
       "0   146                   0.540                  200          7.870   \n",
       "1    95                   0.330                  206          7.872   \n",
       "2   167                   0.540                  200          7.870   \n",
       "3   105                   0.365                  205          7.870   \n",
       "4   105                   0.370                  205          7.870   \n",
       "..  ...                     ...                  ...            ...   \n",
       "61  478                   1.870                  188          7.990   \n",
       "62  256                   0.917                  199          7.800   \n",
       "63  188                   0.709                  195          7.860   \n",
       "64  160                   0.345                   77          6.450   \n",
       "65  307                   1.100                  200          7.930   \n",
       "\n",
       "    Specific Heat of capacity, J/g*C       B       C      N     O   Mg  ...  \\\n",
       "0                              0.440  0.0000  0.0000  0.000  0.00  0.0  ...   \n",
       "1                              0.481  0.0000  0.0800  0.000  0.00  0.0  ...   \n",
       "2                              0.472  0.0000  0.1300  0.000  0.00  0.0  ...   \n",
       "3                              0.448  0.0000  0.1300  0.000  0.00  0.0  ...   \n",
       "4                              0.472  0.0000  0.1500  0.000  0.00  0.0  ...   \n",
       "..                               ...     ...     ...    ...   ...  ...  ...   \n",
       "61                             0.427  0.0030  0.0322  0.000  0.00  0.0  ...   \n",
       "62                             0.481  0.0000  0.0301  0.186  0.00  0.0  ...   \n",
       "63                             0.497  0.0000  0.0865  0.153  0.25  0.0  ...   \n",
       "64                             0.460  0.0030  0.0200  0.015  0.00  0.0  ...   \n",
       "65                             0.455  0.0055  0.1080  0.145  0.00  0.0  ...   \n",
       "\n",
       "    has_Zr  has_Ce  has_W  has_Pb  has_Sb  has_Bi  has_Cd  has_Te  has_Sn  \\\n",
       "0        0       0      0       0       0       0       0       0       0   \n",
       "1        0       0      0       0       0       0       0       0       0   \n",
       "2        0       0      0       0       0       0       0       0       0   \n",
       "3        0       0      0       0       0       0       0       0       0   \n",
       "4        0       0      0       0       0       0       0       0       0   \n",
       "..     ...     ...    ...     ...     ...     ...     ...     ...     ...   \n",
       "61       1       0      0       0       0       0       0       0       0   \n",
       "62       0       0      0       0       0       0       0       0       0   \n",
       "63       0       0      0       0       0       0       0       0       1   \n",
       "64       0       0      1       0       0       0       0       0       0   \n",
       "65       0       0      1       0       0       0       0       0       0   \n",
       "\n",
       "    has_Ta  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "..     ...  \n",
       "61       0  \n",
       "62       0  \n",
       "63       1  \n",
       "64       0  \n",
       "65       1  \n",
       "\n",
       "[61 rows x 69 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98953e0-a465-4e49-add6-e32882353cb9",
   "metadata": {},
   "source": [
    "## Classic models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2038d8-7458-40ea-8ea4-a890b2ee8abd",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a52e71a1-08b7-4ec3-a7ad-b5ba8715f1d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HB</th>\n",
       "      <th>Ultimate strength, Gpa</th>\n",
       "      <th>Youngs modulus, Gpa</th>\n",
       "      <th>Density, g/cc</th>\n",
       "      <th>Specific Heat of capacity, J/g*C</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>N</th>\n",
       "      <th>O</th>\n",
       "      <th>Mg</th>\n",
       "      <th>...</th>\n",
       "      <th>has_Zr</th>\n",
       "      <th>has_Ce</th>\n",
       "      <th>has_W</th>\n",
       "      <th>has_Pb</th>\n",
       "      <th>has_Sb</th>\n",
       "      <th>has_Bi</th>\n",
       "      <th>has_Cd</th>\n",
       "      <th>has_Te</th>\n",
       "      <th>has_Sn</th>\n",
       "      <th>has_Ta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>146</td>\n",
       "      <td>0.540</td>\n",
       "      <td>200</td>\n",
       "      <td>7.870</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95</td>\n",
       "      <td>0.330</td>\n",
       "      <td>206</td>\n",
       "      <td>7.872</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0800</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>167</td>\n",
       "      <td>0.540</td>\n",
       "      <td>200</td>\n",
       "      <td>7.870</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1300</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>105</td>\n",
       "      <td>0.365</td>\n",
       "      <td>205</td>\n",
       "      <td>7.870</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1300</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>0.370</td>\n",
       "      <td>205</td>\n",
       "      <td>7.870</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>478</td>\n",
       "      <td>1.870</td>\n",
       "      <td>188</td>\n",
       "      <td>7.990</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>256</td>\n",
       "      <td>0.917</td>\n",
       "      <td>199</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0301</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>188</td>\n",
       "      <td>0.709</td>\n",
       "      <td>195</td>\n",
       "      <td>7.860</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0865</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>160</td>\n",
       "      <td>0.345</td>\n",
       "      <td>77</td>\n",
       "      <td>6.450</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>307</td>\n",
       "      <td>1.100</td>\n",
       "      <td>200</td>\n",
       "      <td>7.930</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     HB  Ultimate strength, Gpa  Youngs modulus, Gpa  Density, g/cc  \\\n",
       "0   146                   0.540                  200          7.870   \n",
       "1    95                   0.330                  206          7.872   \n",
       "2   167                   0.540                  200          7.870   \n",
       "3   105                   0.365                  205          7.870   \n",
       "4   105                   0.370                  205          7.870   \n",
       "..  ...                     ...                  ...            ...   \n",
       "61  478                   1.870                  188          7.990   \n",
       "62  256                   0.917                  199          7.800   \n",
       "63  188                   0.709                  195          7.860   \n",
       "64  160                   0.345                   77          6.450   \n",
       "65  307                   1.100                  200          7.930   \n",
       "\n",
       "    Specific Heat of capacity, J/g*C       B       C      N     O   Mg  ...  \\\n",
       "0                              0.440  0.0000  0.0000  0.000  0.00  0.0  ...   \n",
       "1                              0.481  0.0000  0.0800  0.000  0.00  0.0  ...   \n",
       "2                              0.472  0.0000  0.1300  0.000  0.00  0.0  ...   \n",
       "3                              0.448  0.0000  0.1300  0.000  0.00  0.0  ...   \n",
       "4                              0.472  0.0000  0.1500  0.000  0.00  0.0  ...   \n",
       "..                               ...     ...     ...    ...   ...  ...  ...   \n",
       "61                             0.427  0.0030  0.0322  0.000  0.00  0.0  ...   \n",
       "62                             0.481  0.0000  0.0301  0.186  0.00  0.0  ...   \n",
       "63                             0.497  0.0000  0.0865  0.153  0.25  0.0  ...   \n",
       "64                             0.460  0.0030  0.0200  0.015  0.00  0.0  ...   \n",
       "65                             0.455  0.0055  0.1080  0.145  0.00  0.0  ...   \n",
       "\n",
       "    has_Zr  has_Ce  has_W  has_Pb  has_Sb  has_Bi  has_Cd  has_Te  has_Sn  \\\n",
       "0        0       0      0       0       0       0       0       0       0   \n",
       "1        0       0      0       0       0       0       0       0       0   \n",
       "2        0       0      0       0       0       0       0       0       0   \n",
       "3        0       0      0       0       0       0       0       0       0   \n",
       "4        0       0      0       0       0       0       0       0       0   \n",
       "..     ...     ...    ...     ...     ...     ...     ...     ...     ...   \n",
       "61       1       0      0       0       0       0       0       0       0   \n",
       "62       0       0      0       0       0       0       0       0       0   \n",
       "63       0       0      0       0       0       0       0       0       1   \n",
       "64       0       0      1       0       0       0       0       0       0   \n",
       "65       0       0      1       0       0       0       0       0       0   \n",
       "\n",
       "    has_Ta  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "..     ...  \n",
       "61       0  \n",
       "62       0  \n",
       "63       1  \n",
       "64       0  \n",
       "65       1  \n",
       "\n",
       "[61 rows x 69 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75ddba7d-7a55-48f4-84f2-a6ab32b897f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns[5:]\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(df[['HB', 'Ultimate strength, Gpa', 'Youngs modulus, Gpa', 'Density, g/cc', 'Specific Heat of capacity, J/g*C']], df[features], test_size=0.2, random_state=42, shuffle=True)\n",
    "y_train_reg, y_train_class = y_train[y_train.columns[:32]],  y_train[y_train.columns[32:]]\n",
    "y_test_reg, y_test_class = y_test[y_test.columns[:32]],  y_test[y_test.columns[32:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ed43533e-27c1-4e6a-926c-d8578389b587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(n_estimators=80)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(n_estimators=80)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(n_estimators=80)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_frst = RandomForestRegressor(n_estimators=80)\n",
    "rnd_frst.fit(x_train, y_train_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "911dc383-10bd-4de8-a9a8-b278abcaeeba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clsf = RandomForestClassifier()\n",
    "rnd_clsf.fit(x_train, y_train_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55b21799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showing_error_reg(model, x, y, typee: str):\n",
    "    print(f'{typee} errors:')\n",
    "    for i in range(y.values.shape[1]):\n",
    "        abs_val = mean_absolute_error(y.values[:, i], model.predict(x)[:, i])\n",
    "        mean_val = y[y.columns[i]].mean()\n",
    "        if mean_val == 0.0:\n",
    "            mean_val = 1\n",
    "        print(round((abs_val/mean_val)*100, 2), '%', ': ', y.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae0593a8-dab9-4ef1-b24f-2ef0214366c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showing_error_class_acc(model, x, y, typee: str):\n",
    "    print(f'{typee} errors:')\n",
    "    for i in range(len(y.columns)):\n",
    "        acc = accuracy_score(y.values[:, i], model.predict(x)[:, i])\n",
    "        print('Accuracy', acc, ': ', y.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fb8bb03-9344-432b-aabe-f1463e5225c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showing_error_class_pr(model, x, y, typee: str):\n",
    "    print(f'{typee} errors:')\n",
    "    for i in range(len(y.columns)):\n",
    "        acc = precision_score(y.values[:, i], model.predict(x)[:, i])\n",
    "        print('Precision', acc, ': ', y.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f15ecc82-9809-43da-968b-638afb2678b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showing_error_class_rec(model, x, y, typee: str):\n",
    "    print(f'{typee} errors:')\n",
    "    for i in range(len(y.columns)):\n",
    "        acc = recall_score(y.values[:, i], model.predict(x)[:, i])\n",
    "        print('Recall', acc, ': ', y.columns[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04c7525-f195-40c9-bfa5-11b6f63e6365",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Другие модели классификатора "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8210c668-c7a9-4440-9158-76571dc235f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg = joblib.load('RandomForest_regression.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64c49937-3df5-46a7-8882-8189fc154d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_train = np.hstack([x_train, model_reg.predict(x_train)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0537a11-95c6-4866-b5dd-1b14db2f1893",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_test = np.hstack([x_test, model_reg.predict(x_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "160cd0a8-8d94-4622-a679-51d9359ccf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation='relu', input_dim=nn_train.shape[1]),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(y_train_class.values.shape[1], activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80bef9fa-1b51-4540-9da9-9b88c3aeb289",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 7ms/step - loss: 11.9164 - accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.8294 - accuracy: 0.0208\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.3673 - accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.2859 - accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.8830 - accuracy: 0.1250\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.2548 - accuracy: 0.1667\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.0068 - accuracy: 0.0625\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4796 - accuracy: 0.2292\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.4924 - accuracy: 0.0833\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.2360 - accuracy: 0.1250\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.1938 - accuracy: 0.0833\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.2901 - accuracy: 0.1250\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0545 - accuracy: 0.1667\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.9159 - accuracy: 0.1875\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.0257 - accuracy: 0.1250\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.8399 - accuracy: 0.1250\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.7565 - accuracy: 0.1875\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.8848 - accuracy: 0.2292\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.7775 - accuracy: 0.2708\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.7258 - accuracy: 0.2708\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.7006 - accuracy: 0.1875\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6080 - accuracy: 0.2917\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6232 - accuracy: 0.2292\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.4764 - accuracy: 0.2500\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.5541 - accuracy: 0.2917\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4919 - accuracy: 0.2917\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.5017 - accuracy: 0.1875\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4245 - accuracy: 0.2917\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4571 - accuracy: 0.2708\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.3788 - accuracy: 0.2083\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4299 - accuracy: 0.3333\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4138 - accuracy: 0.2083\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4054 - accuracy: 0.2083\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.3587 - accuracy: 0.1875\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.3217 - accuracy: 0.1875\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.3294 - accuracy: 0.1667\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2905 - accuracy: 0.2292\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.3138 - accuracy: 0.2083\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.2904 - accuracy: 0.2708\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.2796 - accuracy: 0.2500\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2315 - accuracy: 0.2500\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2904 - accuracy: 0.2500\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2503 - accuracy: 0.1458\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2560 - accuracy: 0.1875\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2857 - accuracy: 0.2292\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2606 - accuracy: 0.3125\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2425 - accuracy: 0.2292\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2404 - accuracy: 0.2083\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.2373 - accuracy: 0.2500\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2247 - accuracy: 0.1875\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2308 - accuracy: 0.1875\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2125 - accuracy: 0.2292\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2331 - accuracy: 0.3542\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.2159 - accuracy: 0.3542\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.2211 - accuracy: 0.3958\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2369 - accuracy: 0.2917\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2136 - accuracy: 0.2500\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.2210 - accuracy: 0.3125\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2163 - accuracy: 0.3125\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.2195 - accuracy: 0.2500\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.2016 - accuracy: 0.1875\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2076 - accuracy: 0.3125\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.2167 - accuracy: 0.1875\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.2191 - accuracy: 0.2292\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2018 - accuracy: 0.1875\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2126 - accuracy: 0.2500\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.2127 - accuracy: 0.2917\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1983 - accuracy: 0.2917\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1975 - accuracy: 0.1875\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2035 - accuracy: 0.2292\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.2025 - accuracy: 0.1250\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1891 - accuracy: 0.1667\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1994 - accuracy: 0.1042\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.2001 - accuracy: 0.3125\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1866 - accuracy: 0.1875\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1870 - accuracy: 0.2500\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1867 - accuracy: 0.2292\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1909 - accuracy: 0.1250\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1746 - accuracy: 0.2083\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1943 - accuracy: 0.1875\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1759 - accuracy: 0.1458\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1982 - accuracy: 0.2708\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1963 - accuracy: 0.1875\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1789 - accuracy: 0.2083\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1718 - accuracy: 0.1667\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1779 - accuracy: 0.2708\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2015 - accuracy: 0.2292\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1731 - accuracy: 0.1667\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1631 - accuracy: 0.1667\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1836 - accuracy: 0.2292\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1719 - accuracy: 0.2083\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1807 - accuracy: 0.1875\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1736 - accuracy: 0.2292\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1793 - accuracy: 0.1458\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1718 - accuracy: 0.2292\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1830 - accuracy: 0.1250\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1624 - accuracy: 0.1458\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1731 - accuracy: 0.1667\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1702 - accuracy: 0.2917\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1691 - accuracy: 0.2500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f59444b3eb0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy'])\n",
    "clf_model.fit(nn_train, y_train_class, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "85e457de-6902-42cc-915c-88064f6ac9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model.save('NN_classifier_v2.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169ef34a-0646-4e22-8803-235f474026c1",
   "metadata": {},
   "source": [
    "#### Other classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "58f0db0f-114a-41e8-9010-f6d29b474bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf = RandomForestClassifier()\n",
    "rnd_clf.fit(nn_train, y_train_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4bab38f6-fc84-4821-8616-8a44707456cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rnd_clf_THIS_MODEL.joblib']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rnd_clf, 'rnd_clf_THIS_MODEL.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1240e5c7-b626-4a99-b112-9f9873517905",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4ed57d72-ab78-418d-9b23-b5e71378473e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train errors:\n",
      "36.73 % :  B\n",
      "17.92 % :  C\n",
      "49.6 % :  N\n",
      "69.72 % :  O\n",
      "33.75 % :  Mg\n",
      "35.86 % :  Al\n",
      "30.63 % :  Si\n",
      "14.74 % :  P\n",
      "27.52 % :  S\n",
      "92.5 % :  K\n",
      "37.99 % :  Ti\n",
      "38.58 % :  V\n",
      "34.48 % :  Cr\n",
      "20.58 % :  Mn\n",
      "1.49 % :  Fe\n",
      "50.13 % :  Co\n",
      "26.37 % :  Ni\n",
      "30.73 % :  Cu\n",
      "33.75 % :  As\n",
      "75.67 % :  Se\n",
      "23.44 % :  Mo\n",
      "51.12 % :  Nb\n",
      "57.69 % :  Zr\n",
      "59.6 % :  Ce\n",
      "49.86 % :  W\n",
      "72.74 % :  Pb\n",
      "33.75 % :  Sb\n",
      "33.75 % :  Bi\n",
      "33.75 % :  Cd\n",
      "33.75 % :  Te\n",
      "77.35 % :  Sn\n",
      "55.34 % :  Ta\n"
     ]
    }
   ],
   "source": [
    "showing_error_reg(rnd_frst, x_train, y_train_reg, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73f209aa-4c50-430d-91e0-8387d61af783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test errors:\n",
      "100.05 % :  B\n",
      "59.76 % :  C\n",
      "68.89 % :  N\n",
      "99.45 % :  O\n",
      "74.5 % :  Mg\n",
      "87.95 % :  Al\n",
      "57.3 % :  Si\n",
      "38.52 % :  P\n",
      "79.46 % :  S\n",
      "116.0 % :  K\n",
      "115.88 % :  Ti\n",
      "90.48 % :  V\n",
      "54.82 % :  Cr\n",
      "44.33 % :  Mn\n",
      "4.2 % :  Fe\n",
      "114.76 % :  Co\n",
      "68.84 % :  Ni\n",
      "61.09 % :  Cu\n",
      "73.0 % :  As\n",
      "91.18 % :  Se\n",
      "50.93 % :  Mo\n",
      "89.22 % :  Nb\n",
      "100.19 % :  Zr\n",
      "94.23 % :  Ce\n",
      "96.07 % :  W\n",
      "109.19 % :  Pb\n",
      "73.0 % :  Sb\n",
      "73.0 % :  Bi\n",
      "73.0 % :  Cd\n",
      "73.0 % :  Te\n",
      "84.53 % :  Sn\n",
      "134.69 % :  Ta\n"
     ]
    }
   ],
   "source": [
    "showing_error_reg(rnd_frst, x_test, y_test_reg, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "08776f38-395e-4fc5-ba56-6b9be022c7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RandomForest_regression.joblib']"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rnd_frst, \"RandomForest_regression.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1430bbc-8332-4c73-b2e7-beaa5f842397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train errors:\n",
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mshowing_error_class_acc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m, in \u001b[0;36mshowing_error_class_acc\u001b[0;34m(model, x, y, typee)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypee\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m errors:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y\u001b[38;5;241m.\u001b[39mcolumns)):\n\u001b[0;32m----> 4\u001b[0m     acc \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, acc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m, y\u001b[38;5;241m.\u001b[39mcolumns[i])\n",
      "File \u001b[0;32m~/Рабочий стол/ML/env/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/Рабочий стол/ML/env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:220\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Рабочий стол/ML/env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     90\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     95\u001b[0m             type_true, type_pred\n\u001b[1;32m     96\u001b[0m         )\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[1;32m    100\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "showing_error_class_acc(clf_model, x_train, y_train_class, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "395bc0bd-e409-4454-8a52-040d92debc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test errors:\n",
      "Accuracy 0.8461538461538461 :  has_B\n",
      "Accuracy 0.9230769230769231 :  has_C\n",
      "Accuracy 1.0 :  has_N\n",
      "Accuracy 0.9230769230769231 :  has_O\n",
      "Accuracy 0.9230769230769231 :  has_Mg\n",
      "Accuracy 0.9230769230769231 :  has_Al\n",
      "Accuracy 0.9230769230769231 :  has_Si\n",
      "Accuracy 0.9230769230769231 :  has_P\n",
      "Accuracy 0.9230769230769231 :  has_S\n",
      "Accuracy 0.9230769230769231 :  has_K\n",
      "Accuracy 1.0 :  has_Ti\n",
      "Accuracy 0.7692307692307693 :  has_V\n",
      "Accuracy 0.9230769230769231 :  has_Cr\n",
      "Accuracy 0.9230769230769231 :  has_Mn\n",
      "Accuracy 1.0 :  has_Fe\n",
      "Accuracy 0.9230769230769231 :  has_Co\n",
      "Accuracy 0.9230769230769231 :  has_Ni\n",
      "Accuracy 0.9230769230769231 :  has_Cu\n",
      "Accuracy 0.9230769230769231 :  has_As\n",
      "Accuracy 0.8461538461538461 :  has_Se\n",
      "Accuracy 0.9230769230769231 :  has_Mo\n",
      "Accuracy 1.0 :  has_Nb\n",
      "Accuracy 0.9230769230769231 :  has_Zr\n",
      "Accuracy 0.8461538461538461 :  has_Ce\n",
      "Accuracy 0.6923076923076923 :  has_W\n",
      "Accuracy 0.7692307692307693 :  has_Pb\n",
      "Accuracy 0.9230769230769231 :  has_Sb\n",
      "Accuracy 0.9230769230769231 :  has_Bi\n",
      "Accuracy 0.9230769230769231 :  has_Cd\n",
      "Accuracy 0.9230769230769231 :  has_Te\n",
      "Accuracy 0.8461538461538461 :  has_Sn\n",
      "Accuracy 0.6923076923076923 :  has_Ta\n"
     ]
    }
   ],
   "source": [
    "showing_error_class_acc(xgb_cls, x_test, y_test_class, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5b1c4d82-dc96-4d62-9830-9a49cf38c0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XGBoost_classifier.joblib']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_cls, \"XGBoost_classifier.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ca5387-08b3-43d4-8241-cbda6a6e6890",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d04e3b0-0ea2-40d7-bb0d-eb6e8529d6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Result:\n",
    "    def __init__(self, model, x, y_reg, y_class, typee: str):\n",
    "        self.model = model\n",
    "        self.y_reg = y_reg\n",
    "        self.y_class = y_class\n",
    "        self.typee = typee\n",
    "        self.x = x\n",
    "\n",
    "    \n",
    "    def show_results(self):\n",
    "        results = []\n",
    "        for i in range(self.y_class.values.shape[0]):\n",
    "            result = []\n",
    "            for j in range(self.y_class.values.shape[1]):\n",
    "                if self.y_class.values[i, j] >= 0.3:\n",
    "                    result.append(self.model.predict(self.x)[i, j])\n",
    "                elif self.y_class.values[i, j] < 0.3:\n",
    "                    result.append(0)\n",
    "            results.append(result)\n",
    "        return np.array(results), self.y_reg.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y_class.values.shape[0]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "c966452e-3cbe-4645-aaac-078f2a788877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>N</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>P</th>\n",
       "      <th>S</th>\n",
       "      <th>Cr</th>\n",
       "      <th>Mn</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Ni</th>\n",
       "      <th>Cu</th>\n",
       "      <th>Mo</th>\n",
       "      <th>Nb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>predicted</th>\n",
       "      <td>0.099132</td>\n",
       "      <td>0.065549</td>\n",
       "      <td>1.01215</td>\n",
       "      <td>0.828138</td>\n",
       "      <td>0.027967</td>\n",
       "      <td>0.024046</td>\n",
       "      <td>15.015625</td>\n",
       "      <td>0.907313</td>\n",
       "      <td>74.545</td>\n",
       "      <td>5.961825</td>\n",
       "      <td>3.315</td>\n",
       "      <td>2.2847</td>\n",
       "      <td>0.3319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true</th>\n",
       "      <td>0.061900</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.13000</td>\n",
       "      <td>0.828000</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>14.800000</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>74.200</td>\n",
       "      <td>6.440000</td>\n",
       "      <td>3.590</td>\n",
       "      <td>2.3900</td>\n",
       "      <td>0.3810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  C         N       Al        Si         P         S  \\\n",
       "predicted  0.099132  0.065549  1.01215  0.828138  0.027967  0.024046   \n",
       "true       0.061900  0.010000  1.13000  0.828000  0.027000  0.022600   \n",
       "\n",
       "                  Cr        Mn      Fe        Ni     Cu      Mo      Nb  \n",
       "predicted  15.015625  0.907313  74.545  5.961825  3.315  2.2847  0.3319  \n",
       "true       14.800000  0.843000  74.200  6.440000  3.590  2.3900  0.3810  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_train = Result(rnd_frst, x_train, y_train_reg, y_train_class, 'train')\n",
    "prediction_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ed0716c2-0fc2-4edf-95b5-2004f8927c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>O</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>P</th>\n",
       "      <th>S</th>\n",
       "      <th>Ti</th>\n",
       "      <th>V</th>\n",
       "      <th>...</th>\n",
       "      <th>Mn</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Ni</th>\n",
       "      <th>Cu</th>\n",
       "      <th>As</th>\n",
       "      <th>Se</th>\n",
       "      <th>Mo</th>\n",
       "      <th>Ce</th>\n",
       "      <th>Pb</th>\n",
       "      <th>Sb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>predicted</th>\n",
       "      <td>0.00052</td>\n",
       "      <td>2.392576</td>\n",
       "      <td>0.016063</td>\n",
       "      <td>0.030281</td>\n",
       "      <td>2.386712</td>\n",
       "      <td>2.159175</td>\n",
       "      <td>0.066921</td>\n",
       "      <td>0.045957</td>\n",
       "      <td>0.058338</td>\n",
       "      <td>0.404603</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8377</td>\n",
       "      <td>83.9785</td>\n",
       "      <td>7.332063</td>\n",
       "      <td>1.126887</td>\n",
       "      <td>0.01425</td>\n",
       "      <td>0.029813</td>\n",
       "      <td>0.968137</td>\n",
       "      <td>0.072675</td>\n",
       "      <td>0.004563</td>\n",
       "      <td>0.012397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true</th>\n",
       "      <td>0.00060</td>\n",
       "      <td>3.130000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>3.260000</td>\n",
       "      <td>2.330000</td>\n",
       "      <td>0.088800</td>\n",
       "      <td>0.069800</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.478000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7370</td>\n",
       "      <td>86.8000</td>\n",
       "      <td>6.350000</td>\n",
       "      <td>0.994000</td>\n",
       "      <td>0.02000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.797000</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>0.002140</td>\n",
       "      <td>0.017400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 B         C         O        Mg        Al        Si  \\\n",
       "predicted  0.00052  2.392576  0.016063  0.030281  2.386712  2.159175   \n",
       "true       0.00060  3.130000  0.005000  0.045000  3.260000  2.330000   \n",
       "\n",
       "                  P         S        Ti         V  ...      Mn       Fe  \\\n",
       "predicted  0.066921  0.045957  0.058338  0.404603  ...  0.8377  83.9785   \n",
       "true       0.088800  0.069800  0.040000  0.478000  ...  0.7370  86.8000   \n",
       "\n",
       "                 Ni        Cu       As        Se        Mo        Ce  \\\n",
       "predicted  7.332063  1.126887  0.01425  0.029813  0.968137  0.072675   \n",
       "true       6.350000  0.994000  0.02000  0.030000  0.797000  0.102000   \n",
       "\n",
       "                 Pb        Sb  \n",
       "predicted  0.004563  0.012397  \n",
       "true       0.002140  0.017400  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_test = Result(rnd_frst, x_test, y_test_reg, y_test_class, 'test')\n",
    "prediction_test[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "b4b83533-1d5c-4f22-ae89-5eeaf2668a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тестовый экземпляр 0 -----------------------------------------------------------------------------------------\n",
      "                Fe\n",
      "predicted   95.118\n",
      "true       100.000\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Тестовый экземпляр 1 -----------------------------------------------------------------------------------------\n",
      "                  C     P     S       Mn         Fe\n",
      "predicted  0.189875  0.04  0.05  0.60125  99.563375\n",
      "true       0.180000  0.04  0.05  0.60000  99.570000\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Тестовый экземпляр 2 -----------------------------------------------------------------------------------------\n",
      "                  C        Si         P         S         V        Cr  \\\n",
      "predicted  0.782253  0.625775  0.019125  0.064989  1.539125  5.865125   \n",
      "true       0.446000  0.769000  0.030000  0.017800  0.890000  4.440000   \n",
      "\n",
      "                 Mn         Fe      Co      Ni     Cu        Mo        W  \n",
      "predicted  0.810088  89.625125  1.2465  1.5598  0.474  1.265463  0.70275  \n",
      "true       0.454000  89.600000  3.5000  0.7030  0.250  1.940000  7.55000  \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Тестовый экземпляр 3 -----------------------------------------------------------------------------------------\n",
      "                  C     P      S        Mn      Fe    Pb\n",
      "predicted  0.519875  0.04  0.059  0.940625  98.917  0.00\n",
      "true       0.200000  0.04  0.130  1.300000  98.630  0.36\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Тестовый экземпляр 4 -----------------------------------------------------------------------------------------\n",
      "                  C         P         S        Mn         Fe\n",
      "predicted  0.228575  0.075083  0.139845  1.008512  99.302625\n",
      "true       0.130000  0.120000  0.150000  0.900000  99.230000\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Тестовый экземпляр 5 -----------------------------------------------------------------------------------------\n",
      "                  B         C        N         O        Al        Si  \\\n",
      "predicted  0.000397  0.158252  0.13058  0.053125  0.289788  0.963175   \n",
      "true       0.314000  0.143000  0.23000  0.132000  1.810000  1.080000   \n",
      "\n",
      "                  P         S      K      Ti  ...        Co         Ni  \\\n",
      "predicted  0.040258  0.041546  0.006  0.1566  ...  0.791963   6.209938   \n",
      "true       0.042700  0.036600  0.060  0.7830  ...  4.410000  10.900000   \n",
      "\n",
      "                 Cu        Se      Mo        Nb        Zr     Ce       W  \\\n",
      "predicted  1.291287  0.043437  2.1908  0.242394  0.001016  0.020  0.2295   \n",
      "true       1.930000  0.135000  2.1200  0.625000  0.766000  0.723  1.4200   \n",
      "\n",
      "                 Pb  \n",
      "predicted  0.009413  \n",
      "true       0.040800  \n",
      "\n",
      "[2 rows x 24 columns]\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Тестовый экземпляр 6 -----------------------------------------------------------------------------------------\n",
      "                  C         P         S        Mn         Fe\n",
      "predicted  0.547225  0.039954  0.109058  1.264538  98.497375\n",
      "true       0.980000  0.040000  0.050000  0.900000  98.550000\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Тестовый экземпляр 7 -----------------------------------------------------------------------------------------\n",
      "                 B         C         O        Mg        Al        Si  \\\n",
      "predicted  0.00052  2.392576  0.016063  0.030281  2.386712  2.159175   \n",
      "true       0.00060  3.130000  0.005000  0.045000  3.260000  2.330000   \n",
      "\n",
      "                  P         S        Ti         V  ...      Mn       Fe  \\\n",
      "predicted  0.066921  0.045957  0.058338  0.404603  ...  0.8377  83.9785   \n",
      "true       0.088800  0.069800  0.040000  0.478000  ...  0.7370  86.8000   \n",
      "\n",
      "                 Ni        Cu       As        Se        Mo        Ce  \\\n",
      "predicted  7.332063  1.126887  0.01425  0.029813  0.968137  0.072675   \n",
      "true       6.350000  0.994000  0.02000  0.030000  0.797000  0.102000   \n",
      "\n",
      "                 Pb        Sb  \n",
      "predicted  0.004563  0.012397  \n",
      "true       0.002140  0.017400  \n",
      "\n",
      "[2 rows x 21 columns]\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Тестовый экземпляр 8 -----------------------------------------------------------------------------------------\n",
      "                 C         P         S        Mn      Fe\n",
      "predicted  0.24365  0.042028  0.073865  0.952837  99.151\n",
      "true       0.23000  0.040000  0.050000  0.900000  99.230\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Тестовый экземпляр 9 -----------------------------------------------------------------------------------------\n",
      "                 C      Si         P         S         V        Cr        Mn  \\\n",
      "predicted  0.65775  1.0432  0.030614  0.083605  0.525012  3.975662  1.057587   \n",
      "true       1.03000  0.3100  0.029500  0.029500  0.211000  0.265000  0.425000   \n",
      "\n",
      "                  Fe       Ni       Cu      Mo         W  \n",
      "predicted  94.887625  1.93945  0.27725  0.6045  0.286875  \n",
      "true       98.100000  0.25000  0.25000  0.1000  0.150000  \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Тестовый экземпляр 10 -----------------------------------------------------------------------------------------\n",
      "                 C         P         S     Mn      Fe\n",
      "predicted  0.34825  0.049981  0.091922  1.003  99.037\n",
      "true       0.29000  0.040000  0.050000  1.500  98.580\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Тестовый экземпляр 11 -----------------------------------------------------------------------------------------\n",
      "                  B         C         N        Al        Si         P  \\\n",
      "predicted  0.000337  1.195998  0.040302  1.217825  1.263937  0.059275   \n",
      "true       0.003000  0.020000  0.015000  0.040000  0.050000  0.010000   \n",
      "\n",
      "                  S         V        Cr        Mn      Fe       Ni       Cu  \\\n",
      "predicted  0.060822  0.208705  6.490237  1.101274  86.008  5.45185  0.94365   \n",
      "true       0.001000  0.020000  0.050000  0.150000  56.200  0.20000  0.10000   \n",
      "\n",
      "                 Mo        Nb         W  \n",
      "predicted  1.027275  0.151013  0.171875  \n",
      "true       0.027000  0.060000  0.100000  \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Тестовый экземпляр 12 -----------------------------------------------------------------------------------------\n",
      "                 C      P        S       Mn        Fe\n",
      "predicted  0.31675  0.049  0.08225  0.97375  99.12225\n",
      "true       0.34000  0.040  0.05000  0.90000  99.13000\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(prediction_test)):\n",
    "    print('Тестовый экземпляр', i, '-----------------------------------------------------------------------------------------')\n",
    "    print(prediction_test[i])\n",
    "    print('--------------------------------------------------------------------------------------------------------------')\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4b796b-ae0f-49c9-9589-f3d982cb322b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## NN (Bad results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a2f8304",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.LSTM(input_dim, 10, 5, batch_first=True, dtype=torch.float64)\n",
    "        self.flat = torch.nn.Flatten()\n",
    "        self.lin1 = torch.nn.Linear(250, 100, dtype=torch.float64)\n",
    "        self.lin2 = torch.nn.Linear(100, output_dim, dtype=torch.float64)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x, (h, cell) = self.rnn(x)\n",
    "        x = torch.nn.functional.tanh(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        y = self.lin2(x)\n",
    "        return y\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04de18ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (rnn): LSTM(1, 100, batch_first=True)\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin1): Linear(in_features=500, out_features=100, bias=True)\n",
       "  (lin2): Linear(in_features=100, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(1, output_dim)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf2ec984",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(df.columns)[5:37]\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(df[['HB', 'Ultimate strength, Gpa', 'Youngs modulus, Gpa', 'Density, g/cc', 'Specific Heat of capacity, J/g*C']], df[features], test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df814fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessing(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_train, y_train, x_test, y_test, subset='train'):\n",
    "        self.subset = subset\n",
    "        self.scaler = StandardScaler()\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.scaler.fit(self.x_train)\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.subset == 'train':\n",
    "            return len(self.x_train)\n",
    "        else:\n",
    "            return len(self.x_test)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.subset == 'train':\n",
    "            ret_x = self.scaler.transform(self.x_train)[index]\n",
    "            ret_x = ret_x.reshape((-1, 1))\n",
    "            return torch.from_numpy(ret_x), torch.from_numpy(self.y_train.values[index]) #.cuda()\n",
    "        else:\n",
    "            ret_x = self.scaler.transform(self.x_test)[index]\n",
    "            ret_x = ret_x.reshape((-1, 1))\n",
    "            return torch.from_numpy(ret_x), torch.from_numpy(self.y_test.values[index]) #.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "880b01f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataPreprocessing(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e766481",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = len(data)\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0de8c18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- epoch = 1/1000 --------\n",
      "batch: 1; loss = 279.890066042676\n",
      "-------- loss = 279.890066042676 --------\n",
      "-------- epoch = 2/1000 --------\n",
      "batch: 1; loss = 278.04692527365165\n",
      "-------- loss = 278.04692527365165 --------\n",
      "-------- epoch = 3/1000 --------\n",
      "batch: 1; loss = 274.7524157248982\n",
      "-------- loss = 274.7524157248982 --------\n",
      "-------- epoch = 4/1000 --------\n",
      "batch: 1; loss = 268.01118780774203\n",
      "-------- loss = 268.01118780774203 --------\n",
      "-------- epoch = 5/1000 --------\n",
      "batch: 1; loss = 254.60168832884418\n",
      "-------- loss = 254.60168832884418 --------\n",
      "-------- epoch = 6/1000 --------\n",
      "batch: 1; loss = 229.20812235235846\n",
      "-------- loss = 229.20812235235846 --------\n",
      "-------- epoch = 7/1000 --------\n",
      "batch: 1; loss = 185.609413665068\n",
      "-------- loss = 185.609413665068 --------\n",
      "-------- epoch = 8/1000 --------\n",
      "batch: 1; loss = 124.73079735399854\n",
      "-------- loss = 124.73079735399854 --------\n",
      "-------- epoch = 9/1000 --------\n",
      "batch: 1; loss = 62.48150188360285\n",
      "-------- loss = 62.48150188360285 --------\n",
      "-------- epoch = 10/1000 --------\n",
      "batch: 1; loss = 22.723145293527608\n",
      "-------- loss = 22.723145293527608 --------\n",
      "-------- epoch = 11/1000 --------\n",
      "batch: 1; loss = 23.642798323578685\n",
      "-------- loss = 23.642798323578685 --------\n",
      "-------- epoch = 12/1000 --------\n",
      "batch: 1; loss = 44.35836409224237\n",
      "-------- loss = 44.35836409224237 --------\n",
      "-------- epoch = 13/1000 --------\n",
      "batch: 1; loss = 52.144341913491324\n",
      "-------- loss = 52.144341913491324 --------\n",
      "-------- epoch = 14/1000 --------\n",
      "batch: 1; loss = 41.82795368026103\n",
      "-------- loss = 41.82795368026103 --------\n",
      "-------- epoch = 15/1000 --------\n",
      "batch: 1; loss = 26.16231964029788\n",
      "-------- loss = 26.16231964029788 --------\n",
      "-------- epoch = 16/1000 --------\n",
      "batch: 1; loss = 14.499585112687162\n",
      "-------- loss = 14.499585112687162 --------\n",
      "-------- epoch = 17/1000 --------\n",
      "batch: 1; loss = 11.908237025656021\n",
      "-------- loss = 11.908237025656021 --------\n",
      "-------- epoch = 18/1000 --------\n",
      "batch: 1; loss = 15.755837840775333\n",
      "-------- loss = 15.755837840775333 --------\n",
      "-------- epoch = 19/1000 --------\n",
      "batch: 1; loss = 21.135026966161384\n",
      "-------- loss = 21.135026966161384 --------\n",
      "-------- epoch = 20/1000 --------\n",
      "batch: 1; loss = 24.837577840530187\n",
      "-------- loss = 24.837577840530187 --------\n",
      "-------- epoch = 21/1000 --------\n",
      "batch: 1; loss = 25.0487830808795\n",
      "-------- loss = 25.0487830808795 --------\n",
      "-------- epoch = 22/1000 --------\n",
      "batch: 1; loss = 21.392679189130245\n",
      "-------- loss = 21.392679189130245 --------\n",
      "-------- epoch = 23/1000 --------\n",
      "batch: 1; loss = 15.981391389745149\n",
      "-------- loss = 15.981391389745149 --------\n",
      "-------- epoch = 24/1000 --------\n",
      "batch: 1; loss = 11.056932131888056\n",
      "-------- loss = 11.056932131888056 --------\n",
      "-------- epoch = 25/1000 --------\n",
      "batch: 1; loss = 9.059017626876775\n",
      "-------- loss = 9.059017626876775 --------\n",
      "-------- epoch = 26/1000 --------\n",
      "batch: 1; loss = 10.13675656669418\n",
      "-------- loss = 10.13675656669418 --------\n",
      "-------- epoch = 27/1000 --------\n",
      "batch: 1; loss = 12.02996148628417\n",
      "-------- loss = 12.02996148628417 --------\n",
      "-------- epoch = 28/1000 --------\n",
      "batch: 1; loss = 11.963530997808915\n",
      "-------- loss = 11.963530997808915 --------\n",
      "-------- epoch = 29/1000 --------\n",
      "batch: 1; loss = 10.553983031433487\n",
      "-------- loss = 10.553983031433487 --------\n",
      "-------- epoch = 30/1000 --------\n",
      "batch: 1; loss = 8.82888052467996\n",
      "-------- loss = 8.82888052467996 --------\n",
      "-------- epoch = 31/1000 --------\n",
      "batch: 1; loss = 8.183233552836102\n",
      "-------- loss = 8.183233552836102 --------\n",
      "-------- epoch = 32/1000 --------\n",
      "batch: 1; loss = 9.009386641378287\n",
      "-------- loss = 9.009386641378287 --------\n",
      "-------- epoch = 33/1000 --------\n",
      "batch: 1; loss = 10.228992056969416\n",
      "-------- loss = 10.228992056969416 --------\n",
      "-------- epoch = 34/1000 --------\n",
      "batch: 1; loss = 10.705275387132309\n",
      "-------- loss = 10.705275387132309 --------\n",
      "-------- epoch = 35/1000 --------\n",
      "batch: 1; loss = 10.216367985042456\n",
      "-------- loss = 10.216367985042456 --------\n",
      "-------- epoch = 36/1000 --------\n",
      "batch: 1; loss = 9.218876501114755\n",
      "-------- loss = 9.218876501114755 --------\n",
      "-------- epoch = 37/1000 --------\n",
      "batch: 1; loss = 8.17906083949451\n",
      "-------- loss = 8.17906083949451 --------\n",
      "-------- epoch = 38/1000 --------\n",
      "batch: 1; loss = 7.601815392587079\n",
      "-------- loss = 7.601815392587079 --------\n",
      "-------- epoch = 39/1000 --------\n",
      "batch: 1; loss = 7.69472529201559\n",
      "-------- loss = 7.69472529201559 --------\n",
      "-------- epoch = 40/1000 --------\n",
      "batch: 1; loss = 7.946472489315835\n",
      "-------- loss = 7.946472489315835 --------\n",
      "-------- epoch = 41/1000 --------\n",
      "batch: 1; loss = 7.931108501288772\n",
      "-------- loss = 7.931108501288772 --------\n",
      "-------- epoch = 42/1000 --------\n",
      "batch: 1; loss = 7.654493789915395\n",
      "-------- loss = 7.654493789915395 --------\n",
      "-------- epoch = 43/1000 --------\n",
      "batch: 1; loss = 7.413792277649944\n",
      "-------- loss = 7.413792277649944 --------\n",
      "-------- epoch = 44/1000 --------\n",
      "batch: 1; loss = 7.490051789299997\n",
      "-------- loss = 7.490051789299997 --------\n",
      "-------- epoch = 45/1000 --------\n",
      "batch: 1; loss = 7.816890604346457\n",
      "-------- loss = 7.816890604346457 --------\n",
      "-------- epoch = 46/1000 --------\n",
      "batch: 1; loss = 8.052057409523178\n",
      "-------- loss = 8.052057409523178 --------\n",
      "-------- epoch = 47/1000 --------\n",
      "batch: 1; loss = 7.971593474954364\n",
      "-------- loss = 7.971593474954364 --------\n",
      "-------- epoch = 48/1000 --------\n",
      "batch: 1; loss = 7.649187274242166\n",
      "-------- loss = 7.649187274242166 --------\n",
      "-------- epoch = 49/1000 --------\n",
      "batch: 1; loss = 7.368869213795466\n",
      "-------- loss = 7.368869213795466 --------\n",
      "-------- epoch = 50/1000 --------\n",
      "batch: 1; loss = 7.293088359937568\n",
      "-------- loss = 7.293088359937568 --------\n",
      "-------- epoch = 51/1000 --------\n",
      "batch: 1; loss = 7.344935917162085\n",
      "-------- loss = 7.344935917162085 --------\n",
      "-------- epoch = 52/1000 --------\n",
      "batch: 1; loss = 7.31664558033014\n",
      "-------- loss = 7.31664558033014 --------\n",
      "-------- epoch = 53/1000 --------\n",
      "batch: 1; loss = 7.216579449630378\n",
      "-------- loss = 7.216579449630378 --------\n",
      "-------- epoch = 54/1000 --------\n",
      "batch: 1; loss = 7.176716919462936\n",
      "-------- loss = 7.176716919462936 --------\n",
      "-------- epoch = 55/1000 --------\n",
      "batch: 1; loss = 7.275445555807263\n",
      "-------- loss = 7.275445555807263 --------\n",
      "-------- epoch = 56/1000 --------\n",
      "batch: 1; loss = 7.415508710788831\n",
      "-------- loss = 7.415508710788831 --------\n",
      "-------- epoch = 57/1000 --------\n",
      "batch: 1; loss = 7.424631562169065\n",
      "-------- loss = 7.424631562169065 --------\n",
      "-------- epoch = 58/1000 --------\n",
      "batch: 1; loss = 7.275459965898938\n",
      "-------- loss = 7.275459965898938 --------\n",
      "-------- epoch = 59/1000 --------\n",
      "batch: 1; loss = 7.126633939564601\n",
      "-------- loss = 7.126633939564601 --------\n",
      "-------- epoch = 60/1000 --------\n",
      "batch: 1; loss = 7.087614889802425\n",
      "-------- loss = 7.087614889802425 --------\n",
      "-------- epoch = 61/1000 --------\n",
      "batch: 1; loss = 7.10131374512719\n",
      "-------- loss = 7.10131374512719 --------\n",
      "-------- epoch = 62/1000 --------\n",
      "batch: 1; loss = 7.077909576030901\n",
      "-------- loss = 7.077909576030901 --------\n",
      "-------- epoch = 63/1000 --------\n",
      "batch: 1; loss = 7.056691626305103\n",
      "-------- loss = 7.056691626305103 --------\n",
      "-------- epoch = 64/1000 --------\n",
      "batch: 1; loss = 7.120913815276271\n",
      "-------- loss = 7.120913815276271 --------\n",
      "-------- epoch = 65/1000 --------\n",
      "batch: 1; loss = 7.191984142499812\n",
      "-------- loss = 7.191984142499812 --------\n",
      "-------- epoch = 66/1000 --------\n",
      "batch: 1; loss = 7.163692838259326\n",
      "-------- loss = 7.163692838259326 --------\n",
      "-------- epoch = 67/1000 --------\n",
      "batch: 1; loss = 7.063737715199305\n",
      "-------- loss = 7.063737715199305 --------\n",
      "-------- epoch = 68/1000 --------\n",
      "batch: 1; loss = 7.009280302674697\n",
      "-------- loss = 7.009280302674697 --------\n",
      "-------- epoch = 69/1000 --------\n",
      "batch: 1; loss = 7.010391033311284\n",
      "-------- loss = 7.010391033311284 --------\n",
      "-------- epoch = 70/1000 --------\n",
      "batch: 1; loss = 6.995776747254035\n",
      "-------- loss = 6.995776747254035 --------\n",
      "-------- epoch = 71/1000 --------\n",
      "batch: 1; loss = 7.006095377668996\n",
      "-------- loss = 7.006095377668996 --------\n",
      "-------- epoch = 72/1000 --------\n",
      "batch: 1; loss = 7.059143214546452\n",
      "-------- loss = 7.059143214546452 --------\n",
      "-------- epoch = 73/1000 --------\n",
      "batch: 1; loss = 7.054999205852083\n",
      "-------- loss = 7.054999205852083 --------\n",
      "-------- epoch = 74/1000 --------\n",
      "batch: 1; loss = 6.988889325622146\n",
      "-------- loss = 6.988889325622146 --------\n",
      "-------- epoch = 75/1000 --------\n",
      "batch: 1; loss = 6.960471778654051\n",
      "-------- loss = 6.960471778654051 --------\n",
      "-------- epoch = 76/1000 --------\n",
      "batch: 1; loss = 6.952391069402818\n",
      "-------- loss = 6.952391069402818 --------\n",
      "-------- epoch = 77/1000 --------\n",
      "batch: 1; loss = 6.95449743557375\n",
      "-------- loss = 6.95449743557375 --------\n",
      "-------- epoch = 78/1000 --------\n",
      "batch: 1; loss = 6.99506489645787\n",
      "-------- loss = 6.99506489645787 --------\n",
      "-------- epoch = 79/1000 --------\n",
      "batch: 1; loss = 6.973431060041876\n",
      "-------- loss = 6.973431060041876 --------\n",
      "-------- epoch = 80/1000 --------\n",
      "batch: 1; loss = 6.9187520627570045\n",
      "-------- loss = 6.9187520627570045 --------\n",
      "-------- epoch = 81/1000 --------\n",
      "batch: 1; loss = 6.912885620519586\n",
      "-------- loss = 6.912885620519586 --------\n",
      "-------- epoch = 82/1000 --------\n",
      "batch: 1; loss = 6.90634728795641\n",
      "-------- loss = 6.90634728795641 --------\n",
      "-------- epoch = 83/1000 --------\n",
      "batch: 1; loss = 6.937242449264236\n",
      "-------- loss = 6.937242449264236 --------\n",
      "-------- epoch = 84/1000 --------\n",
      "batch: 1; loss = 6.905754379809401\n",
      "-------- loss = 6.905754379809401 --------\n",
      "-------- epoch = 85/1000 --------\n",
      "batch: 1; loss = 6.874090011892991\n",
      "-------- loss = 6.874090011892991 --------\n",
      "-------- epoch = 86/1000 --------\n",
      "batch: 1; loss = 6.86591453993045\n",
      "-------- loss = 6.86591453993045 --------\n",
      "-------- epoch = 87/1000 --------\n",
      "batch: 1; loss = 6.890120452654418\n",
      "-------- loss = 6.890120452654418 --------\n",
      "-------- epoch = 88/1000 --------\n",
      "batch: 1; loss = 6.8632643942008285\n",
      "-------- loss = 6.8632643942008285 --------\n",
      "-------- epoch = 89/1000 --------\n",
      "batch: 1; loss = 6.835954427101669\n",
      "-------- loss = 6.835954427101669 --------\n",
      "-------- epoch = 90/1000 --------\n",
      "batch: 1; loss = 6.8339434738786595\n",
      "-------- loss = 6.8339434738786595 --------\n",
      "-------- epoch = 91/1000 --------\n",
      "batch: 1; loss = 6.8512087813310005\n",
      "-------- loss = 6.8512087813310005 --------\n",
      "-------- epoch = 92/1000 --------\n",
      "batch: 1; loss = 6.805530407569546\n",
      "-------- loss = 6.805530407569546 --------\n",
      "-------- epoch = 93/1000 --------\n",
      "batch: 1; loss = 6.796637874476936\n",
      "-------- loss = 6.796637874476936 --------\n",
      "-------- epoch = 94/1000 --------\n",
      "batch: 1; loss = 6.818992365526948\n",
      "-------- loss = 6.818992365526948 --------\n",
      "-------- epoch = 95/1000 --------\n",
      "batch: 1; loss = 6.7762386603635925\n",
      "-------- loss = 6.7762386603635925 --------\n",
      "-------- epoch = 96/1000 --------\n",
      "batch: 1; loss = 6.764596913541546\n",
      "-------- loss = 6.764596913541546 --------\n",
      "-------- epoch = 97/1000 --------\n",
      "batch: 1; loss = 6.786107689051277\n",
      "-------- loss = 6.786107689051277 --------\n",
      "-------- epoch = 98/1000 --------\n",
      "batch: 1; loss = 6.7414685179731\n",
      "-------- loss = 6.7414685179731 --------\n",
      "-------- epoch = 99/1000 --------\n",
      "batch: 1; loss = 6.744623757907416\n",
      "-------- loss = 6.744623757907416 --------\n",
      "-------- epoch = 100/1000 --------\n",
      "batch: 1; loss = 6.72683418408036\n",
      "-------- loss = 6.72683418408036 --------\n",
      "-------- epoch = 101/1000 --------\n",
      "batch: 1; loss = 6.706861779095392\n",
      "-------- loss = 6.706861779095392 --------\n",
      "-------- epoch = 102/1000 --------\n",
      "batch: 1; loss = 6.720210634063517\n",
      "-------- loss = 6.720210634063517 --------\n",
      "-------- epoch = 103/1000 --------\n",
      "batch: 1; loss = 6.69002141497321\n",
      "-------- loss = 6.69002141497321 --------\n",
      "-------- epoch = 104/1000 --------\n",
      "batch: 1; loss = 6.77379974143989\n",
      "-------- loss = 6.77379974143989 --------\n",
      "-------- epoch = 105/1000 --------\n",
      "batch: 1; loss = 6.75400505216464\n",
      "-------- loss = 6.75400505216464 --------\n",
      "-------- epoch = 106/1000 --------\n",
      "batch: 1; loss = 6.963620256089936\n",
      "-------- loss = 6.963620256089936 --------\n",
      "-------- epoch = 107/1000 --------\n",
      "batch: 1; loss = 6.721818118394562\n",
      "-------- loss = 6.721818118394562 --------\n",
      "-------- epoch = 108/1000 --------\n",
      "batch: 1; loss = 6.63935061759545\n",
      "-------- loss = 6.63935061759545 --------\n",
      "-------- epoch = 109/1000 --------\n",
      "batch: 1; loss = 6.713993816584371\n",
      "-------- loss = 6.713993816584371 --------\n",
      "-------- epoch = 110/1000 --------\n",
      "batch: 1; loss = 6.701535843769154\n",
      "-------- loss = 6.701535843769154 --------\n",
      "-------- epoch = 111/1000 --------\n",
      "batch: 1; loss = 6.71519121366373\n",
      "-------- loss = 6.71519121366373 --------\n",
      "-------- epoch = 112/1000 --------\n",
      "batch: 1; loss = 6.58378972474224\n",
      "-------- loss = 6.58378972474224 --------\n",
      "-------- epoch = 113/1000 --------\n",
      "batch: 1; loss = 6.603584003615615\n",
      "-------- loss = 6.603584003615615 --------\n",
      "-------- epoch = 114/1000 --------\n",
      "batch: 1; loss = 6.724142075229683\n",
      "-------- loss = 6.724142075229683 --------\n",
      "-------- epoch = 115/1000 --------\n",
      "batch: 1; loss = 6.56479688979823\n",
      "-------- loss = 6.56479688979823 --------\n",
      "-------- epoch = 116/1000 --------\n",
      "batch: 1; loss = 6.533720475456477\n",
      "-------- loss = 6.533720475456477 --------\n",
      "-------- epoch = 117/1000 --------\n",
      "batch: 1; loss = 6.61167537419131\n",
      "-------- loss = 6.61167537419131 --------\n",
      "-------- epoch = 118/1000 --------\n",
      "batch: 1; loss = 6.552095121961417\n",
      "-------- loss = 6.552095121961417 --------\n",
      "-------- epoch = 119/1000 --------\n",
      "batch: 1; loss = 6.5492028055594735\n",
      "-------- loss = 6.5492028055594735 --------\n",
      "-------- epoch = 120/1000 --------\n",
      "batch: 1; loss = 6.481133040562216\n",
      "-------- loss = 6.481133040562216 --------\n",
      "-------- epoch = 121/1000 --------\n",
      "batch: 1; loss = 6.473747860911626\n",
      "-------- loss = 6.473747860911626 --------\n",
      "-------- epoch = 122/1000 --------\n",
      "batch: 1; loss = 6.544913465140038\n",
      "-------- loss = 6.544913465140038 --------\n",
      "-------- epoch = 123/1000 --------\n",
      "batch: 1; loss = 6.4495069323625875\n",
      "-------- loss = 6.4495069323625875 --------\n",
      "-------- epoch = 124/1000 --------\n",
      "batch: 1; loss = 6.435399807857479\n",
      "-------- loss = 6.435399807857479 --------\n",
      "-------- epoch = 125/1000 --------\n",
      "batch: 1; loss = 6.401518591681291\n",
      "-------- loss = 6.401518591681291 --------\n",
      "-------- epoch = 126/1000 --------\n",
      "batch: 1; loss = 6.381337315723449\n",
      "-------- loss = 6.381337315723449 --------\n",
      "-------- epoch = 127/1000 --------\n",
      "batch: 1; loss = 6.432764074035309\n",
      "-------- loss = 6.432764074035309 --------\n",
      "-------- epoch = 128/1000 --------\n",
      "batch: 1; loss = 6.357747275365259\n",
      "-------- loss = 6.357747275365259 --------\n",
      "-------- epoch = 129/1000 --------\n",
      "batch: 1; loss = 6.3678306926023955\n",
      "-------- loss = 6.3678306926023955 --------\n",
      "-------- epoch = 130/1000 --------\n",
      "batch: 1; loss = 6.287316876962201\n",
      "-------- loss = 6.287316876962201 --------\n",
      "-------- epoch = 131/1000 --------\n",
      "batch: 1; loss = 6.2679879852846865\n",
      "-------- loss = 6.2679879852846865 --------\n",
      "-------- epoch = 132/1000 --------\n",
      "batch: 1; loss = 6.25292669512332\n",
      "-------- loss = 6.25292669512332 --------\n",
      "-------- epoch = 133/1000 --------\n",
      "batch: 1; loss = 6.218954844222378\n",
      "-------- loss = 6.218954844222378 --------\n",
      "-------- epoch = 134/1000 --------\n",
      "batch: 1; loss = 6.255587004587157\n",
      "-------- loss = 6.255587004587157 --------\n",
      "-------- epoch = 135/1000 --------\n",
      "batch: 1; loss = 6.207469153402056\n",
      "-------- loss = 6.207469153402056 --------\n",
      "-------- epoch = 136/1000 --------\n",
      "batch: 1; loss = 6.306080996364055\n",
      "-------- loss = 6.306080996364055 --------\n",
      "-------- epoch = 137/1000 --------\n",
      "batch: 1; loss = 6.249010065858229\n",
      "-------- loss = 6.249010065858229 --------\n",
      "-------- epoch = 138/1000 --------\n",
      "batch: 1; loss = 6.400907045023611\n",
      "-------- loss = 6.400907045023611 --------\n",
      "-------- epoch = 139/1000 --------\n",
      "batch: 1; loss = 6.248906006989286\n",
      "-------- loss = 6.248906006989286 --------\n",
      "-------- epoch = 140/1000 --------\n",
      "batch: 1; loss = 6.2583586821766275\n",
      "-------- loss = 6.2583586821766275 --------\n",
      "-------- epoch = 141/1000 --------\n",
      "batch: 1; loss = 6.052361467132364\n",
      "-------- loss = 6.052361467132364 --------\n",
      "-------- epoch = 142/1000 --------\n",
      "batch: 1; loss = 6.020641301627788\n",
      "-------- loss = 6.020641301627788 --------\n",
      "-------- epoch = 143/1000 --------\n",
      "batch: 1; loss = 6.054706789128626\n",
      "-------- loss = 6.054706789128626 --------\n",
      "-------- epoch = 144/1000 --------\n",
      "batch: 1; loss = 6.040699728170979\n",
      "-------- loss = 6.040699728170979 --------\n",
      "-------- epoch = 145/1000 --------\n",
      "batch: 1; loss = 6.1222515564721744\n",
      "-------- loss = 6.1222515564721744 --------\n",
      "-------- epoch = 146/1000 --------\n",
      "batch: 1; loss = 6.008068604870779\n",
      "-------- loss = 6.008068604870779 --------\n",
      "-------- epoch = 147/1000 --------\n",
      "batch: 1; loss = 6.043607973610528\n",
      "-------- loss = 6.043607973610528 --------\n",
      "-------- epoch = 148/1000 --------\n",
      "batch: 1; loss = 5.912281375055122\n",
      "-------- loss = 5.912281375055122 --------\n",
      "-------- epoch = 149/1000 --------\n",
      "batch: 1; loss = 5.873347989108729\n",
      "-------- loss = 5.873347989108729 --------\n",
      "-------- epoch = 150/1000 --------\n",
      "batch: 1; loss = 5.895582107392669\n",
      "-------- loss = 5.895582107392669 --------\n",
      "-------- epoch = 151/1000 --------\n",
      "batch: 1; loss = 5.874812846547329\n",
      "-------- loss = 5.874812846547329 --------\n",
      "-------- epoch = 152/1000 --------\n",
      "batch: 1; loss = 5.935032145367443\n",
      "-------- loss = 5.935032145367443 --------\n",
      "-------- epoch = 153/1000 --------\n",
      "batch: 1; loss = 5.879181782290092\n",
      "-------- loss = 5.879181782290092 --------\n",
      "-------- epoch = 154/1000 --------\n",
      "batch: 1; loss = 5.959038991964232\n",
      "-------- loss = 5.959038991964232 --------\n",
      "-------- epoch = 155/1000 --------\n",
      "batch: 1; loss = 5.848003689773406\n",
      "-------- loss = 5.848003689773406 --------\n",
      "-------- epoch = 156/1000 --------\n",
      "batch: 1; loss = 5.883949828108573\n",
      "-------- loss = 5.883949828108573 --------\n",
      "-------- epoch = 157/1000 --------\n",
      "batch: 1; loss = 5.7840376764829555\n",
      "-------- loss = 5.7840376764829555 --------\n",
      "-------- epoch = 158/1000 --------\n",
      "batch: 1; loss = 5.7846379898520714\n",
      "-------- loss = 5.7846379898520714 --------\n",
      "-------- epoch = 159/1000 --------\n",
      "batch: 1; loss = 5.716243731235998\n",
      "-------- loss = 5.716243731235998 --------\n",
      "-------- epoch = 160/1000 --------\n",
      "batch: 1; loss = 5.714682524864025\n",
      "-------- loss = 5.714682524864025 --------\n",
      "-------- epoch = 161/1000 --------\n",
      "batch: 1; loss = 5.6813713390685905\n",
      "-------- loss = 5.6813713390685905 --------\n",
      "-------- epoch = 162/1000 --------\n",
      "batch: 1; loss = 5.66376708264782\n",
      "-------- loss = 5.66376708264782 --------\n",
      "-------- epoch = 163/1000 --------\n",
      "batch: 1; loss = 5.663700113548713\n",
      "-------- loss = 5.663700113548713 --------\n",
      "-------- epoch = 164/1000 --------\n",
      "batch: 1; loss = 5.638596034334808\n",
      "-------- loss = 5.638596034334808 --------\n",
      "-------- epoch = 165/1000 --------\n",
      "batch: 1; loss = 5.667157625139641\n",
      "-------- loss = 5.667157625139641 --------\n",
      "-------- epoch = 166/1000 --------\n",
      "batch: 1; loss = 5.649419015453429\n",
      "-------- loss = 5.649419015453429 --------\n",
      "-------- epoch = 167/1000 --------\n",
      "batch: 1; loss = 5.816535246591435\n",
      "-------- loss = 5.816535246591435 --------\n",
      "-------- epoch = 168/1000 --------\n",
      "batch: 1; loss = 5.921299438451849\n",
      "-------- loss = 5.921299438451849 --------\n",
      "-------- epoch = 169/1000 --------\n",
      "batch: 1; loss = 6.618099540709175\n",
      "-------- loss = 6.618099540709175 --------\n",
      "-------- epoch = 170/1000 --------\n",
      "batch: 1; loss = 6.758452518105428\n",
      "-------- loss = 6.758452518105428 --------\n",
      "-------- epoch = 171/1000 --------\n",
      "batch: 1; loss = 6.94062832060279\n",
      "-------- loss = 6.94062832060279 --------\n",
      "-------- epoch = 172/1000 --------\n",
      "batch: 1; loss = 5.711206949461524\n",
      "-------- loss = 5.711206949461524 --------\n",
      "-------- epoch = 173/1000 --------\n",
      "batch: 1; loss = 5.624797004053877\n",
      "-------- loss = 5.624797004053877 --------\n",
      "-------- epoch = 174/1000 --------\n",
      "batch: 1; loss = 6.425122337788963\n",
      "-------- loss = 6.425122337788963 --------\n",
      "-------- epoch = 175/1000 --------\n",
      "batch: 1; loss = 5.940459784435073\n",
      "-------- loss = 5.940459784435073 --------\n",
      "-------- epoch = 176/1000 --------\n",
      "batch: 1; loss = 5.51453360390143\n",
      "-------- loss = 5.51453360390143 --------\n",
      "-------- epoch = 177/1000 --------\n",
      "batch: 1; loss = 5.845846141732587\n",
      "-------- loss = 5.845846141732587 --------\n",
      "-------- epoch = 178/1000 --------\n",
      "batch: 1; loss = 5.8413385724422975\n",
      "-------- loss = 5.8413385724422975 --------\n",
      "-------- epoch = 179/1000 --------\n",
      "batch: 1; loss = 5.566055768721918\n",
      "-------- loss = 5.566055768721918 --------\n",
      "-------- epoch = 180/1000 --------\n",
      "batch: 1; loss = 5.58729612504856\n",
      "-------- loss = 5.58729612504856 --------\n",
      "-------- epoch = 181/1000 --------\n",
      "batch: 1; loss = 5.707063404945281\n",
      "-------- loss = 5.707063404945281 --------\n",
      "-------- epoch = 182/1000 --------\n",
      "batch: 1; loss = 5.594778621530506\n",
      "-------- loss = 5.594778621530506 --------\n",
      "-------- epoch = 183/1000 --------\n",
      "batch: 1; loss = 5.467325502769001\n",
      "-------- loss = 5.467325502769001 --------\n",
      "-------- epoch = 184/1000 --------\n",
      "batch: 1; loss = 5.589770347471988\n",
      "-------- loss = 5.589770347471988 --------\n",
      "-------- epoch = 185/1000 --------\n",
      "batch: 1; loss = 5.5980426143229485\n",
      "-------- loss = 5.5980426143229485 --------\n",
      "-------- epoch = 186/1000 --------\n",
      "batch: 1; loss = 5.40071066785124\n",
      "-------- loss = 5.40071066785124 --------\n",
      "-------- epoch = 187/1000 --------\n",
      "batch: 1; loss = 5.483872537816201\n",
      "-------- loss = 5.483872537816201 --------\n",
      "-------- epoch = 188/1000 --------\n",
      "batch: 1; loss = 5.5793833172804765\n",
      "-------- loss = 5.5793833172804765 --------\n",
      "-------- epoch = 189/1000 --------\n",
      "batch: 1; loss = 5.362733741692683\n",
      "-------- loss = 5.362733741692683 --------\n",
      "-------- epoch = 190/1000 --------\n",
      "batch: 1; loss = 5.382280852807643\n",
      "-------- loss = 5.382280852807643 --------\n",
      "-------- epoch = 191/1000 --------\n",
      "batch: 1; loss = 5.525616133381685\n",
      "-------- loss = 5.525616133381685 --------\n",
      "-------- epoch = 192/1000 --------\n",
      "batch: 1; loss = 5.349498166035617\n",
      "-------- loss = 5.349498166035617 --------\n",
      "-------- epoch = 193/1000 --------\n",
      "batch: 1; loss = 5.302621807010451\n",
      "-------- loss = 5.302621807010451 --------\n",
      "-------- epoch = 194/1000 --------\n",
      "batch: 1; loss = 5.41234015766292\n",
      "-------- loss = 5.41234015766292 --------\n",
      "-------- epoch = 195/1000 --------\n",
      "batch: 1; loss = 5.339938091411113\n",
      "-------- loss = 5.339938091411113 --------\n",
      "-------- epoch = 196/1000 --------\n",
      "batch: 1; loss = 5.289099485752955\n",
      "-------- loss = 5.289099485752955 --------\n",
      "-------- epoch = 197/1000 --------\n",
      "batch: 1; loss = 5.271913117407785\n",
      "-------- loss = 5.271913117407785 --------\n",
      "-------- epoch = 198/1000 --------\n",
      "batch: 1; loss = 5.277922921627713\n",
      "-------- loss = 5.277922921627713 --------\n",
      "-------- epoch = 199/1000 --------\n",
      "batch: 1; loss = 5.314128856401149\n",
      "-------- loss = 5.314128856401149 --------\n",
      "-------- epoch = 200/1000 --------\n",
      "batch: 1; loss = 5.208982435995519\n",
      "-------- loss = 5.208982435995519 --------\n",
      "-------- epoch = 201/1000 --------\n",
      "batch: 1; loss = 5.18787188153626\n",
      "-------- loss = 5.18787188153626 --------\n",
      "-------- epoch = 202/1000 --------\n",
      "batch: 1; loss = 5.227452638931007\n",
      "-------- loss = 5.227452638931007 --------\n",
      "-------- epoch = 203/1000 --------\n",
      "batch: 1; loss = 5.193569087129396\n",
      "-------- loss = 5.193569087129396 --------\n",
      "-------- epoch = 204/1000 --------\n",
      "batch: 1; loss = 5.207327692646571\n",
      "-------- loss = 5.207327692646571 --------\n",
      "-------- epoch = 205/1000 --------\n",
      "batch: 1; loss = 5.126435652322026\n",
      "-------- loss = 5.126435652322026 --------\n",
      "-------- epoch = 206/1000 --------\n",
      "batch: 1; loss = 5.106851566101613\n",
      "-------- loss = 5.106851566101613 --------\n",
      "-------- epoch = 207/1000 --------\n",
      "batch: 1; loss = 5.114639118836737\n",
      "-------- loss = 5.114639118836737 --------\n",
      "-------- epoch = 208/1000 --------\n",
      "batch: 1; loss = 5.0907380659797035\n",
      "-------- loss = 5.0907380659797035 --------\n",
      "-------- epoch = 209/1000 --------\n",
      "batch: 1; loss = 5.125786793115957\n",
      "-------- loss = 5.125786793115957 --------\n",
      "-------- epoch = 210/1000 --------\n",
      "batch: 1; loss = 5.065066227145876\n",
      "-------- loss = 5.065066227145876 --------\n",
      "-------- epoch = 211/1000 --------\n",
      "batch: 1; loss = 5.074209162364129\n",
      "-------- loss = 5.074209162364129 --------\n",
      "-------- epoch = 212/1000 --------\n",
      "batch: 1; loss = 5.007803552754125\n",
      "-------- loss = 5.007803552754125 --------\n",
      "-------- epoch = 213/1000 --------\n",
      "batch: 1; loss = 5.000085166455826\n",
      "-------- loss = 5.000085166455826 --------\n",
      "-------- epoch = 214/1000 --------\n",
      "batch: 1; loss = 4.9560841435263745\n",
      "-------- loss = 4.9560841435263745 --------\n",
      "-------- epoch = 215/1000 --------\n",
      "batch: 1; loss = 4.93930514196152\n",
      "-------- loss = 4.93930514196152 --------\n",
      "-------- epoch = 216/1000 --------\n",
      "batch: 1; loss = 4.915806083329304\n",
      "-------- loss = 4.915806083329304 --------\n",
      "-------- epoch = 217/1000 --------\n",
      "batch: 1; loss = 4.890358551707548\n",
      "-------- loss = 4.890358551707548 --------\n",
      "-------- epoch = 218/1000 --------\n",
      "batch: 1; loss = 4.880841342858649\n",
      "-------- loss = 4.880841342858649 --------\n",
      "-------- epoch = 219/1000 --------\n",
      "batch: 1; loss = 4.849299104627955\n",
      "-------- loss = 4.849299104627955 --------\n",
      "-------- epoch = 220/1000 --------\n",
      "batch: 1; loss = 4.868703179041674\n",
      "-------- loss = 4.868703179041674 --------\n",
      "-------- epoch = 221/1000 --------\n",
      "batch: 1; loss = 4.848491330946101\n",
      "-------- loss = 4.848491330946101 --------\n",
      "-------- epoch = 222/1000 --------\n",
      "batch: 1; loss = 5.000726109918095\n",
      "-------- loss = 5.000726109918095 --------\n",
      "-------- epoch = 223/1000 --------\n",
      "batch: 1; loss = 5.160958909486472\n",
      "-------- loss = 5.160958909486472 --------\n",
      "-------- epoch = 224/1000 --------\n",
      "batch: 1; loss = 6.022443723338326\n",
      "-------- loss = 6.022443723338326 --------\n",
      "-------- epoch = 225/1000 --------\n",
      "batch: 1; loss = 6.749472124636331\n",
      "-------- loss = 6.749472124636331 --------\n",
      "-------- epoch = 226/1000 --------\n",
      "batch: 1; loss = 7.967642543387594\n",
      "-------- loss = 7.967642543387594 --------\n",
      "-------- epoch = 227/1000 --------\n",
      "batch: 1; loss = 5.623485062918756\n",
      "-------- loss = 5.623485062918756 --------\n",
      "-------- epoch = 228/1000 --------\n",
      "batch: 1; loss = 4.702792075140371\n",
      "-------- loss = 4.702792075140371 --------\n",
      "-------- epoch = 229/1000 --------\n",
      "batch: 1; loss = 5.9500531225145705\n",
      "-------- loss = 5.9500531225145705 --------\n",
      "-------- epoch = 230/1000 --------\n",
      "batch: 1; loss = 5.418588604503401\n",
      "-------- loss = 5.418588604503401 --------\n",
      "-------- epoch = 231/1000 --------\n",
      "batch: 1; loss = 4.657984923562551\n",
      "-------- loss = 4.657984923562551 --------\n",
      "-------- epoch = 232/1000 --------\n",
      "batch: 1; loss = 5.296623656810763\n",
      "-------- loss = 5.296623656810763 --------\n",
      "-------- epoch = 233/1000 --------\n",
      "batch: 1; loss = 4.999621421417868\n",
      "-------- loss = 4.999621421417868 --------\n",
      "-------- epoch = 234/1000 --------\n",
      "batch: 1; loss = 4.584825118055183\n",
      "-------- loss = 4.584825118055183 --------\n",
      "-------- epoch = 235/1000 --------\n",
      "batch: 1; loss = 5.041948576973071\n",
      "-------- loss = 5.041948576973071 --------\n",
      "-------- epoch = 236/1000 --------\n",
      "batch: 1; loss = 4.685187609554083\n",
      "-------- loss = 4.685187609554083 --------\n",
      "-------- epoch = 237/1000 --------\n",
      "batch: 1; loss = 4.526043959112445\n",
      "-------- loss = 4.526043959112445 --------\n",
      "-------- epoch = 238/1000 --------\n",
      "batch: 1; loss = 4.842181614424504\n",
      "-------- loss = 4.842181614424504 --------\n",
      "-------- epoch = 239/1000 --------\n",
      "batch: 1; loss = 4.492402755526851\n",
      "-------- loss = 4.492402755526851 --------\n",
      "-------- epoch = 240/1000 --------\n",
      "batch: 1; loss = 4.48790436002565\n",
      "-------- loss = 4.48790436002565 --------\n",
      "-------- epoch = 241/1000 --------\n",
      "batch: 1; loss = 4.66290532663299\n",
      "-------- loss = 4.66290532663299 --------\n",
      "-------- epoch = 242/1000 --------\n",
      "batch: 1; loss = 4.383719418558807\n",
      "-------- loss = 4.383719418558807 --------\n",
      "-------- epoch = 243/1000 --------\n",
      "batch: 1; loss = 4.438422025761775\n",
      "-------- loss = 4.438422025761775 --------\n",
      "-------- epoch = 244/1000 --------\n",
      "batch: 1; loss = 4.508337233494884\n",
      "-------- loss = 4.508337233494884 --------\n",
      "-------- epoch = 245/1000 --------\n",
      "batch: 1; loss = 4.322520808111787\n",
      "-------- loss = 4.322520808111787 --------\n",
      "-------- epoch = 246/1000 --------\n",
      "batch: 1; loss = 4.367158451963635\n",
      "-------- loss = 4.367158451963635 --------\n",
      "-------- epoch = 247/1000 --------\n",
      "batch: 1; loss = 4.396820364107614\n",
      "-------- loss = 4.396820364107614 --------\n",
      "-------- epoch = 248/1000 --------\n",
      "batch: 1; loss = 4.281341082485414\n",
      "-------- loss = 4.281341082485414 --------\n",
      "-------- epoch = 249/1000 --------\n",
      "batch: 1; loss = 4.294200127963858\n",
      "-------- loss = 4.294200127963858 --------\n",
      "-------- epoch = 250/1000 --------\n",
      "batch: 1; loss = 4.322988567284116\n",
      "-------- loss = 4.322988567284116 --------\n",
      "-------- epoch = 251/1000 --------\n",
      "batch: 1; loss = 4.24786146606808\n",
      "-------- loss = 4.24786146606808 --------\n",
      "-------- epoch = 252/1000 --------\n",
      "batch: 1; loss = 4.231417016285443\n",
      "-------- loss = 4.231417016285443 --------\n",
      "-------- epoch = 253/1000 --------\n",
      "batch: 1; loss = 4.279539136848034\n",
      "-------- loss = 4.279539136848034 --------\n",
      "-------- epoch = 254/1000 --------\n",
      "batch: 1; loss = 4.211164631392746\n",
      "-------- loss = 4.211164631392746 --------\n",
      "-------- epoch = 255/1000 --------\n",
      "batch: 1; loss = 4.179549807897057\n",
      "-------- loss = 4.179549807897057 --------\n",
      "-------- epoch = 256/1000 --------\n",
      "batch: 1; loss = 4.241842183440626\n",
      "-------- loss = 4.241842183440626 --------\n",
      "-------- epoch = 257/1000 --------\n",
      "batch: 1; loss = 4.168733278495799\n",
      "-------- loss = 4.168733278495799 --------\n",
      "-------- epoch = 258/1000 --------\n",
      "batch: 1; loss = 4.146490055844163\n",
      "-------- loss = 4.146490055844163 --------\n",
      "-------- epoch = 259/1000 --------\n",
      "batch: 1; loss = 4.178686200751893\n",
      "-------- loss = 4.178686200751893 --------\n",
      "-------- epoch = 260/1000 --------\n",
      "batch: 1; loss = 4.135711986205436\n",
      "-------- loss = 4.135711986205436 --------\n",
      "-------- epoch = 261/1000 --------\n",
      "batch: 1; loss = 4.135635137995\n",
      "-------- loss = 4.135635137995 --------\n",
      "-------- epoch = 262/1000 --------\n",
      "batch: 1; loss = 4.125118521406432\n",
      "-------- loss = 4.125118521406432 --------\n",
      "-------- epoch = 263/1000 --------\n",
      "batch: 1; loss = 4.108906342362568\n",
      "-------- loss = 4.108906342362568 --------\n",
      "-------- epoch = 264/1000 --------\n",
      "batch: 1; loss = 4.119257569858331\n",
      "-------- loss = 4.119257569858331 --------\n",
      "-------- epoch = 265/1000 --------\n",
      "batch: 1; loss = 4.078846759155561\n",
      "-------- loss = 4.078846759155561 --------\n",
      "-------- epoch = 266/1000 --------\n",
      "batch: 1; loss = 4.0741928517372\n",
      "-------- loss = 4.0741928517372 --------\n",
      "-------- epoch = 267/1000 --------\n",
      "batch: 1; loss = 4.093291645592918\n",
      "-------- loss = 4.093291645592918 --------\n",
      "-------- epoch = 268/1000 --------\n",
      "batch: 1; loss = 4.0639166370798865\n",
      "-------- loss = 4.0639166370798865 --------\n",
      "-------- epoch = 269/1000 --------\n",
      "batch: 1; loss = 4.058418251632082\n",
      "-------- loss = 4.058418251632082 --------\n",
      "-------- epoch = 270/1000 --------\n",
      "batch: 1; loss = 4.052211971287593\n",
      "-------- loss = 4.052211971287593 --------\n",
      "-------- epoch = 271/1000 --------\n",
      "batch: 1; loss = 4.043237459971258\n",
      "-------- loss = 4.043237459971258 --------\n",
      "-------- epoch = 272/1000 --------\n",
      "batch: 1; loss = 4.044126108890116\n",
      "-------- loss = 4.044126108890116 --------\n",
      "-------- epoch = 273/1000 --------\n",
      "batch: 1; loss = 4.0156659020542795\n",
      "-------- loss = 4.0156659020542795 --------\n",
      "-------- epoch = 274/1000 --------\n",
      "batch: 1; loss = 4.01226860452928\n",
      "-------- loss = 4.01226860452928 --------\n",
      "-------- epoch = 275/1000 --------\n",
      "batch: 1; loss = 4.004133897345767\n",
      "-------- loss = 4.004133897345767 --------\n",
      "-------- epoch = 276/1000 --------\n",
      "batch: 1; loss = 3.992631496935099\n",
      "-------- loss = 3.992631496935099 --------\n",
      "-------- epoch = 277/1000 --------\n",
      "batch: 1; loss = 3.9993558846441633\n",
      "-------- loss = 3.9993558846441633 --------\n",
      "-------- epoch = 278/1000 --------\n",
      "batch: 1; loss = 3.9829029138357286\n",
      "-------- loss = 3.9829029138357286 --------\n",
      "-------- epoch = 279/1000 --------\n",
      "batch: 1; loss = 3.9810906335852825\n",
      "-------- loss = 3.9810906335852825 --------\n",
      "-------- epoch = 280/1000 --------\n",
      "batch: 1; loss = 3.9635395315149107\n",
      "-------- loss = 3.9635395315149107 --------\n",
      "-------- epoch = 281/1000 --------\n",
      "batch: 1; loss = 3.9560703849597285\n",
      "-------- loss = 3.9560703849597285 --------\n",
      "-------- epoch = 282/1000 --------\n",
      "batch: 1; loss = 3.953929909780811\n",
      "-------- loss = 3.953929909780811 --------\n",
      "-------- epoch = 283/1000 --------\n",
      "batch: 1; loss = 3.9401846866580645\n",
      "-------- loss = 3.9401846866580645 --------\n",
      "-------- epoch = 284/1000 --------\n",
      "batch: 1; loss = 3.9452947815782387\n",
      "-------- loss = 3.9452947815782387 --------\n",
      "-------- epoch = 285/1000 --------\n",
      "batch: 1; loss = 3.928953717443921\n",
      "-------- loss = 3.928953717443921 --------\n",
      "-------- epoch = 286/1000 --------\n",
      "batch: 1; loss = 3.930798931197008\n",
      "-------- loss = 3.930798931197008 --------\n",
      "-------- epoch = 287/1000 --------\n",
      "batch: 1; loss = 3.9118568261312885\n",
      "-------- loss = 3.9118568261312885 --------\n",
      "-------- epoch = 288/1000 --------\n",
      "batch: 1; loss = 3.912714103969039\n",
      "-------- loss = 3.912714103969039 --------\n",
      "-------- epoch = 289/1000 --------\n",
      "batch: 1; loss = 3.8954266452210504\n",
      "-------- loss = 3.8954266452210504 --------\n",
      "-------- epoch = 290/1000 --------\n",
      "batch: 1; loss = 3.8941730409601507\n",
      "-------- loss = 3.8941730409601507 --------\n",
      "-------- epoch = 291/1000 --------\n",
      "batch: 1; loss = 3.880364630195862\n",
      "-------- loss = 3.880364630195862 --------\n",
      "-------- epoch = 292/1000 --------\n",
      "batch: 1; loss = 3.879937141449002\n",
      "-------- loss = 3.879937141449002 --------\n",
      "-------- epoch = 293/1000 --------\n",
      "batch: 1; loss = 3.863831603811813\n",
      "-------- loss = 3.863831603811813 --------\n",
      "-------- epoch = 294/1000 --------\n",
      "batch: 1; loss = 3.8673343957773816\n",
      "-------- loss = 3.8673343957773816 --------\n",
      "-------- epoch = 295/1000 --------\n",
      "batch: 1; loss = 3.852120769192922\n",
      "-------- loss = 3.852120769192922 --------\n",
      "-------- epoch = 296/1000 --------\n",
      "batch: 1; loss = 3.857071525356801\n",
      "-------- loss = 3.857071525356801 --------\n",
      "-------- epoch = 297/1000 --------\n",
      "batch: 1; loss = 3.8367057508856117\n",
      "-------- loss = 3.8367057508856117 --------\n",
      "-------- epoch = 298/1000 --------\n",
      "batch: 1; loss = 3.8443221443310276\n",
      "-------- loss = 3.8443221443310276 --------\n",
      "-------- epoch = 299/1000 --------\n",
      "batch: 1; loss = 3.8229840493386185\n",
      "-------- loss = 3.8229840493386185 --------\n",
      "-------- epoch = 300/1000 --------\n",
      "batch: 1; loss = 3.845193123122561\n",
      "-------- loss = 3.845193123122561 --------\n",
      "-------- epoch = 301/1000 --------\n",
      "batch: 1; loss = 3.827187748903168\n",
      "-------- loss = 3.827187748903168 --------\n",
      "-------- epoch = 302/1000 --------\n",
      "batch: 1; loss = 3.887713354523166\n",
      "-------- loss = 3.887713354523166 --------\n",
      "-------- epoch = 303/1000 --------\n",
      "batch: 1; loss = 3.8976821799964267\n",
      "-------- loss = 3.8976821799964267 --------\n",
      "-------- epoch = 304/1000 --------\n",
      "batch: 1; loss = 4.089451959742804\n",
      "-------- loss = 4.089451959742804 --------\n",
      "-------- epoch = 305/1000 --------\n",
      "batch: 1; loss = 4.207327163882355\n",
      "-------- loss = 4.207327163882355 --------\n",
      "-------- epoch = 306/1000 --------\n",
      "batch: 1; loss = 4.69104818394365\n",
      "-------- loss = 4.69104818394365 --------\n",
      "-------- epoch = 307/1000 --------\n",
      "batch: 1; loss = 4.850907135310548\n",
      "-------- loss = 4.850907135310548 --------\n",
      "-------- epoch = 308/1000 --------\n",
      "batch: 1; loss = 5.116618561494069\n",
      "-------- loss = 5.116618561494069 --------\n",
      "-------- epoch = 309/1000 --------\n",
      "batch: 1; loss = 4.264936474641957\n",
      "-------- loss = 4.264936474641957 --------\n",
      "-------- epoch = 310/1000 --------\n",
      "batch: 1; loss = 3.818723198622839\n",
      "-------- loss = 3.818723198622839 --------\n",
      "-------- epoch = 311/1000 --------\n",
      "batch: 1; loss = 3.987965159451665\n",
      "-------- loss = 3.987965159451665 --------\n",
      "-------- epoch = 312/1000 --------\n",
      "batch: 1; loss = 4.2360185192036175\n",
      "-------- loss = 4.2360185192036175 --------\n",
      "-------- epoch = 313/1000 --------\n",
      "batch: 1; loss = 4.267662621979145\n",
      "-------- loss = 4.267662621979145 --------\n",
      "-------- epoch = 314/1000 --------\n",
      "batch: 1; loss = 3.7546049048023415\n",
      "-------- loss = 3.7546049048023415 --------\n",
      "-------- epoch = 315/1000 --------\n",
      "batch: 1; loss = 3.745123147611426\n",
      "-------- loss = 3.745123147611426 --------\n",
      "-------- epoch = 316/1000 --------\n",
      "batch: 1; loss = 4.088630128687653\n",
      "-------- loss = 4.088630128687653 --------\n",
      "-------- epoch = 317/1000 --------\n",
      "batch: 1; loss = 3.988000744448208\n",
      "-------- loss = 3.988000744448208 --------\n",
      "-------- epoch = 318/1000 --------\n",
      "batch: 1; loss = 3.798862663036862\n",
      "-------- loss = 3.798862663036862 --------\n",
      "-------- epoch = 319/1000 --------\n",
      "batch: 1; loss = 3.6944724855475894\n",
      "-------- loss = 3.6944724855475894 --------\n",
      "-------- epoch = 320/1000 --------\n",
      "batch: 1; loss = 3.8359304555832328\n",
      "-------- loss = 3.8359304555832328 --------\n",
      "-------- epoch = 321/1000 --------\n",
      "batch: 1; loss = 3.945047441595545\n",
      "-------- loss = 3.945047441595545 --------\n",
      "-------- epoch = 322/1000 --------\n",
      "batch: 1; loss = 3.69732474029842\n",
      "-------- loss = 3.69732474029842 --------\n",
      "-------- epoch = 323/1000 --------\n",
      "batch: 1; loss = 3.6564384358962734\n",
      "-------- loss = 3.6564384358962734 --------\n",
      "-------- epoch = 324/1000 --------\n",
      "batch: 1; loss = 3.8061225094980906\n",
      "-------- loss = 3.8061225094980906 --------\n",
      "-------- epoch = 325/1000 --------\n",
      "batch: 1; loss = 3.735735762348803\n",
      "-------- loss = 3.735735762348803 --------\n",
      "-------- epoch = 326/1000 --------\n",
      "batch: 1; loss = 3.650978188427443\n",
      "-------- loss = 3.650978188427443 --------\n",
      "-------- epoch = 327/1000 --------\n",
      "batch: 1; loss = 3.6058120462400987\n",
      "-------- loss = 3.6058120462400987 --------\n",
      "-------- epoch = 328/1000 --------\n",
      "batch: 1; loss = 3.6355543143948474\n",
      "-------- loss = 3.6355543143948474 --------\n",
      "-------- epoch = 329/1000 --------\n",
      "batch: 1; loss = 3.7010816411953873\n",
      "-------- loss = 3.7010816411953873 --------\n",
      "-------- epoch = 330/1000 --------\n",
      "batch: 1; loss = 3.581366957578355\n",
      "-------- loss = 3.581366957578355 --------\n",
      "-------- epoch = 331/1000 --------\n",
      "batch: 1; loss = 3.5364534415069238\n",
      "-------- loss = 3.5364534415069238 --------\n",
      "-------- epoch = 332/1000 --------\n",
      "batch: 1; loss = 3.5749164717340727\n",
      "-------- loss = 3.5749164717340727 --------\n",
      "-------- epoch = 333/1000 --------\n",
      "batch: 1; loss = 3.573594342069258\n",
      "-------- loss = 3.573594342069258 --------\n",
      "-------- epoch = 334/1000 --------\n",
      "batch: 1; loss = 3.590846465122878\n",
      "-------- loss = 3.590846465122878 --------\n",
      "-------- epoch = 335/1000 --------\n",
      "batch: 1; loss = 3.500518007648234\n",
      "-------- loss = 3.500518007648234 --------\n",
      "-------- epoch = 336/1000 --------\n",
      "batch: 1; loss = 3.47553598489675\n",
      "-------- loss = 3.47553598489675 --------\n",
      "-------- epoch = 337/1000 --------\n",
      "batch: 1; loss = 3.4903657945975293\n",
      "-------- loss = 3.4903657945975293 --------\n",
      "-------- epoch = 338/1000 --------\n",
      "batch: 1; loss = 3.470951136071468\n",
      "-------- loss = 3.470951136071468 --------\n",
      "-------- epoch = 339/1000 --------\n",
      "batch: 1; loss = 3.499931660400972\n",
      "-------- loss = 3.499931660400972 --------\n",
      "-------- epoch = 340/1000 --------\n",
      "batch: 1; loss = 3.448570491895739\n",
      "-------- loss = 3.448570491895739 --------\n",
      "-------- epoch = 341/1000 --------\n",
      "batch: 1; loss = 3.4342216041651974\n",
      "-------- loss = 3.4342216041651974 --------\n",
      "-------- epoch = 342/1000 --------\n",
      "batch: 1; loss = 3.387342418305487\n",
      "-------- loss = 3.387342418305487 --------\n",
      "-------- epoch = 343/1000 --------\n",
      "batch: 1; loss = 3.3769675605670137\n",
      "-------- loss = 3.3769675605670137 --------\n",
      "-------- epoch = 344/1000 --------\n",
      "batch: 1; loss = 3.38438138738493\n",
      "-------- loss = 3.38438138738493 --------\n",
      "-------- epoch = 345/1000 --------\n",
      "batch: 1; loss = 3.365425735270158\n",
      "-------- loss = 3.365425735270158 --------\n",
      "-------- epoch = 346/1000 --------\n",
      "batch: 1; loss = 3.4017254551289238\n",
      "-------- loss = 3.4017254551289238 --------\n",
      "-------- epoch = 347/1000 --------\n",
      "batch: 1; loss = 3.379060369906856\n",
      "-------- loss = 3.379060369906856 --------\n",
      "-------- epoch = 348/1000 --------\n",
      "batch: 1; loss = 3.4208934303710894\n",
      "-------- loss = 3.4208934303710894 --------\n",
      "-------- epoch = 349/1000 --------\n",
      "batch: 1; loss = 3.361000578620331\n",
      "-------- loss = 3.361000578620331 --------\n",
      "-------- epoch = 350/1000 --------\n",
      "batch: 1; loss = 3.371867050129357\n",
      "-------- loss = 3.371867050129357 --------\n",
      "-------- epoch = 351/1000 --------\n",
      "batch: 1; loss = 3.2927259737549908\n",
      "-------- loss = 3.2927259737549908 --------\n",
      "-------- epoch = 352/1000 --------\n",
      "batch: 1; loss = 3.2787513499500633\n",
      "-------- loss = 3.2787513499500633 --------\n",
      "-------- epoch = 353/1000 --------\n",
      "batch: 1; loss = 3.242190228921608\n",
      "-------- loss = 3.242190228921608 --------\n",
      "-------- epoch = 354/1000 --------\n",
      "batch: 1; loss = 3.2274493589724256\n",
      "-------- loss = 3.2274493589724256 --------\n",
      "-------- epoch = 355/1000 --------\n",
      "batch: 1; loss = 3.212633673106535\n",
      "-------- loss = 3.212633673106535 --------\n",
      "-------- epoch = 356/1000 --------\n",
      "batch: 1; loss = 3.2029150185405313\n",
      "-------- loss = 3.2029150185405313 --------\n",
      "-------- epoch = 357/1000 --------\n",
      "batch: 1; loss = 3.196046932591757\n",
      "-------- loss = 3.196046932591757 --------\n",
      "-------- epoch = 358/1000 --------\n",
      "batch: 1; loss = 3.174824921154435\n",
      "-------- loss = 3.174824921154435 --------\n",
      "-------- epoch = 359/1000 --------\n",
      "batch: 1; loss = 3.1758051239456733\n",
      "-------- loss = 3.1758051239456733 --------\n",
      "-------- epoch = 360/1000 --------\n",
      "batch: 1; loss = 3.146123536378706\n",
      "-------- loss = 3.146123536378706 --------\n",
      "-------- epoch = 361/1000 --------\n",
      "batch: 1; loss = 3.158828289521287\n",
      "-------- loss = 3.158828289521287 --------\n",
      "-------- epoch = 362/1000 --------\n",
      "batch: 1; loss = 3.1499610782172645\n",
      "-------- loss = 3.1499610782172645 --------\n",
      "-------- epoch = 363/1000 --------\n",
      "batch: 1; loss = 3.238846412585138\n",
      "-------- loss = 3.238846412585138 --------\n",
      "-------- epoch = 364/1000 --------\n",
      "batch: 1; loss = 3.3358687106851637\n",
      "-------- loss = 3.3358687106851637 --------\n",
      "-------- epoch = 365/1000 --------\n",
      "batch: 1; loss = 3.7059220722115085\n",
      "-------- loss = 3.7059220722115085 --------\n",
      "-------- epoch = 366/1000 --------\n",
      "batch: 1; loss = 3.9815394589812008\n",
      "-------- loss = 3.9815394589812008 --------\n",
      "-------- epoch = 367/1000 --------\n",
      "batch: 1; loss = 4.53885532660462\n",
      "-------- loss = 4.53885532660462 --------\n",
      "-------- epoch = 368/1000 --------\n",
      "batch: 1; loss = 4.152584076201864\n",
      "-------- loss = 4.152584076201864 --------\n",
      "-------- epoch = 369/1000 --------\n",
      "batch: 1; loss = 3.6447473592922903\n",
      "-------- loss = 3.6447473592922903 --------\n",
      "-------- epoch = 370/1000 --------\n",
      "batch: 1; loss = 3.0888619118150555\n",
      "-------- loss = 3.0888619118150555 --------\n",
      "-------- epoch = 371/1000 --------\n",
      "batch: 1; loss = 3.349719109308939\n",
      "-------- loss = 3.349719109308939 --------\n",
      "-------- epoch = 372/1000 --------\n",
      "batch: 1; loss = 3.8726403011183845\n",
      "-------- loss = 3.8726403011183845 --------\n",
      "-------- epoch = 373/1000 --------\n",
      "batch: 1; loss = 3.406284542621727\n",
      "-------- loss = 3.406284542621727 --------\n",
      "-------- epoch = 374/1000 --------\n",
      "batch: 1; loss = 3.0471440093408755\n",
      "-------- loss = 3.0471440093408755 --------\n",
      "-------- epoch = 375/1000 --------\n",
      "batch: 1; loss = 3.1567354885329624\n",
      "-------- loss = 3.1567354885329624 --------\n",
      "-------- epoch = 376/1000 --------\n",
      "batch: 1; loss = 3.3747089285970358\n",
      "-------- loss = 3.3747089285970358 --------\n",
      "-------- epoch = 377/1000 --------\n",
      "batch: 1; loss = 3.4541193287742167\n",
      "-------- loss = 3.4541193287742167 --------\n",
      "-------- epoch = 378/1000 --------\n",
      "batch: 1; loss = 3.029487696537588\n",
      "-------- loss = 3.029487696537588 --------\n",
      "-------- epoch = 379/1000 --------\n",
      "batch: 1; loss = 3.0027006200104025\n",
      "-------- loss = 3.0027006200104025 --------\n",
      "-------- epoch = 380/1000 --------\n",
      "batch: 1; loss = 3.2923138431650014\n",
      "-------- loss = 3.2923138431650014 --------\n",
      "-------- epoch = 381/1000 --------\n",
      "batch: 1; loss = 3.1416931487991504\n",
      "-------- loss = 3.1416931487991504 --------\n",
      "-------- epoch = 382/1000 --------\n",
      "batch: 1; loss = 2.9720910744278277\n",
      "-------- loss = 2.9720910744278277 --------\n",
      "-------- epoch = 383/1000 --------\n",
      "batch: 1; loss = 3.0195489445118517\n",
      "-------- loss = 3.0195489445118517 --------\n",
      "-------- epoch = 384/1000 --------\n",
      "batch: 1; loss = 3.089411987025363\n",
      "-------- loss = 3.089411987025363 --------\n",
      "-------- epoch = 385/1000 --------\n",
      "batch: 1; loss = 3.084447181466601\n",
      "-------- loss = 3.084447181466601 --------\n",
      "-------- epoch = 386/1000 --------\n",
      "batch: 1; loss = 2.91651570261782\n",
      "-------- loss = 2.91651570261782 --------\n",
      "-------- epoch = 387/1000 --------\n",
      "batch: 1; loss = 2.9411877189703013\n",
      "-------- loss = 2.9411877189703013 --------\n",
      "-------- epoch = 388/1000 --------\n",
      "batch: 1; loss = 3.0831878098536065\n",
      "-------- loss = 3.0831878098536065 --------\n",
      "-------- epoch = 389/1000 --------\n",
      "batch: 1; loss = 2.973332691203915\n",
      "-------- loss = 2.973332691203915 --------\n",
      "-------- epoch = 390/1000 --------\n",
      "batch: 1; loss = 2.910650819827579\n",
      "-------- loss = 2.910650819827579 --------\n",
      "-------- epoch = 391/1000 --------\n",
      "batch: 1; loss = 2.8932863053036133\n",
      "-------- loss = 2.8932863053036133 --------\n",
      "-------- epoch = 392/1000 --------\n",
      "batch: 1; loss = 2.905818165927172\n",
      "-------- loss = 2.905818165927172 --------\n",
      "-------- epoch = 393/1000 --------\n",
      "batch: 1; loss = 2.9469953799764554\n",
      "-------- loss = 2.9469953799764554 --------\n",
      "-------- epoch = 394/1000 --------\n",
      "batch: 1; loss = 2.8802305218732545\n",
      "-------- loss = 2.8802305218732545 --------\n",
      "-------- epoch = 395/1000 --------\n",
      "batch: 1; loss = 2.849315970373352\n",
      "-------- loss = 2.849315970373352 --------\n",
      "-------- epoch = 396/1000 --------\n",
      "batch: 1; loss = 2.8589754086965438\n",
      "-------- loss = 2.8589754086965438 --------\n",
      "-------- epoch = 397/1000 --------\n",
      "batch: 1; loss = 2.8581430001982064\n",
      "-------- loss = 2.8581430001982064 --------\n",
      "-------- epoch = 398/1000 --------\n",
      "batch: 1; loss = 2.8852495275820975\n",
      "-------- loss = 2.8852495275820975 --------\n",
      "-------- epoch = 399/1000 --------\n",
      "batch: 1; loss = 2.8363148371922158\n",
      "-------- loss = 2.8363148371922158 --------\n",
      "-------- epoch = 400/1000 --------\n",
      "batch: 1; loss = 2.8289589802052806\n",
      "-------- loss = 2.8289589802052806 --------\n",
      "-------- epoch = 401/1000 --------\n",
      "batch: 1; loss = 2.7963684767978556\n",
      "-------- loss = 2.7963684767978556 --------\n",
      "-------- epoch = 402/1000 --------\n",
      "batch: 1; loss = 2.7880258764235\n",
      "-------- loss = 2.7880258764235 --------\n",
      "-------- epoch = 403/1000 --------\n",
      "batch: 1; loss = 2.812145483256455\n",
      "-------- loss = 2.812145483256455 --------\n",
      "-------- epoch = 404/1000 --------\n",
      "batch: 1; loss = 2.807230994301468\n",
      "-------- loss = 2.807230994301468 --------\n",
      "-------- epoch = 405/1000 --------\n",
      "batch: 1; loss = 2.852340950508003\n",
      "-------- loss = 2.852340950508003 --------\n",
      "-------- epoch = 406/1000 --------\n",
      "batch: 1; loss = 2.816679520592219\n",
      "-------- loss = 2.816679520592219 --------\n",
      "-------- epoch = 407/1000 --------\n",
      "batch: 1; loss = 2.8379726789761768\n",
      "-------- loss = 2.8379726789761768 --------\n",
      "-------- epoch = 408/1000 --------\n",
      "batch: 1; loss = 2.799176155133262\n",
      "-------- loss = 2.799176155133262 --------\n",
      "-------- epoch = 409/1000 --------\n",
      "batch: 1; loss = 2.8086898617533027\n",
      "-------- loss = 2.8086898617533027 --------\n",
      "-------- epoch = 410/1000 --------\n",
      "batch: 1; loss = 2.764012956068793\n",
      "-------- loss = 2.764012956068793 --------\n",
      "-------- epoch = 411/1000 --------\n",
      "batch: 1; loss = 2.755658521587414\n",
      "-------- loss = 2.755658521587414 --------\n",
      "-------- epoch = 412/1000 --------\n",
      "batch: 1; loss = 2.740274225582039\n",
      "-------- loss = 2.740274225582039 --------\n",
      "-------- epoch = 413/1000 --------\n",
      "batch: 1; loss = 2.7307166204098476\n",
      "-------- loss = 2.7307166204098476 --------\n",
      "-------- epoch = 414/1000 --------\n",
      "batch: 1; loss = 2.748665938730476\n",
      "-------- loss = 2.748665938730476 --------\n",
      "-------- epoch = 415/1000 --------\n",
      "batch: 1; loss = 2.7518506933707543\n",
      "-------- loss = 2.7518506933707543 --------\n",
      "-------- epoch = 416/1000 --------\n",
      "batch: 1; loss = 2.849749301162154\n",
      "-------- loss = 2.849749301162154 --------\n",
      "-------- epoch = 417/1000 --------\n",
      "batch: 1; loss = 2.8943445237089342\n",
      "-------- loss = 2.8943445237089342 --------\n",
      "-------- epoch = 418/1000 --------\n",
      "batch: 1; loss = 3.107606101168523\n",
      "-------- loss = 3.107606101168523 --------\n",
      "-------- epoch = 419/1000 --------\n",
      "batch: 1; loss = 3.1697767192703594\n",
      "-------- loss = 3.1697767192703594 --------\n",
      "-------- epoch = 420/1000 --------\n",
      "batch: 1; loss = 3.3639700335434566\n",
      "-------- loss = 3.3639700335434566 --------\n",
      "-------- epoch = 421/1000 --------\n",
      "batch: 1; loss = 3.151134159134493\n",
      "-------- loss = 3.151134159134493 --------\n",
      "-------- epoch = 422/1000 --------\n",
      "batch: 1; loss = 2.992012095380772\n",
      "-------- loss = 2.992012095380772 --------\n",
      "-------- epoch = 423/1000 --------\n",
      "batch: 1; loss = 2.7244199641802864\n",
      "-------- loss = 2.7244199641802864 --------\n",
      "-------- epoch = 424/1000 --------\n",
      "batch: 1; loss = 2.7320188982343434\n",
      "-------- loss = 2.7320188982343434 --------\n",
      "-------- epoch = 425/1000 --------\n",
      "batch: 1; loss = 2.955212460310714\n",
      "-------- loss = 2.955212460310714 --------\n",
      "-------- epoch = 426/1000 --------\n",
      "batch: 1; loss = 3.0168606946847674\n",
      "-------- loss = 3.0168606946847674 --------\n",
      "-------- epoch = 427/1000 --------\n",
      "batch: 1; loss = 3.1267605756678614\n",
      "-------- loss = 3.1267605756678614 --------\n",
      "-------- epoch = 428/1000 --------\n",
      "batch: 1; loss = 2.9327899215776987\n",
      "-------- loss = 2.9327899215776987 --------\n",
      "-------- epoch = 429/1000 --------\n",
      "batch: 1; loss = 2.816821316984642\n",
      "-------- loss = 2.816821316984642 --------\n",
      "-------- epoch = 430/1000 --------\n",
      "batch: 1; loss = 2.673148179491791\n",
      "-------- loss = 2.673148179491791 --------\n",
      "-------- epoch = 431/1000 --------\n",
      "batch: 1; loss = 2.68592889613053\n",
      "-------- loss = 2.68592889613053 --------\n",
      "-------- epoch = 432/1000 --------\n",
      "batch: 1; loss = 2.7963424428539168\n",
      "-------- loss = 2.7963424428539168 --------\n",
      "-------- epoch = 433/1000 --------\n",
      "batch: 1; loss = 2.780269725235263\n",
      "-------- loss = 2.780269725235263 --------\n",
      "-------- epoch = 434/1000 --------\n",
      "batch: 1; loss = 2.786197065730457\n",
      "-------- loss = 2.786197065730457 --------\n",
      "-------- epoch = 435/1000 --------\n",
      "batch: 1; loss = 2.673653158191915\n",
      "-------- loss = 2.673653158191915 --------\n",
      "-------- epoch = 436/1000 --------\n",
      "batch: 1; loss = 2.649302068731837\n",
      "-------- loss = 2.649302068731837 --------\n",
      "-------- epoch = 437/1000 --------\n",
      "batch: 1; loss = 2.6754115512307095\n",
      "-------- loss = 2.6754115512307095 --------\n",
      "-------- epoch = 438/1000 --------\n",
      "batch: 1; loss = 2.683828086693685\n",
      "-------- loss = 2.683828086693685 --------\n",
      "-------- epoch = 439/1000 --------\n",
      "batch: 1; loss = 2.746273394085978\n",
      "-------- loss = 2.746273394085978 --------\n",
      "-------- epoch = 440/1000 --------\n",
      "batch: 1; loss = 2.7225120047750972\n",
      "-------- loss = 2.7225120047750972 --------\n",
      "-------- epoch = 441/1000 --------\n",
      "batch: 1; loss = 2.7382923074198726\n",
      "-------- loss = 2.7382923074198726 --------\n",
      "-------- epoch = 442/1000 --------\n",
      "batch: 1; loss = 2.667499990740831\n",
      "-------- loss = 2.667499990740831 --------\n",
      "-------- epoch = 443/1000 --------\n",
      "batch: 1; loss = 2.6531447148378025\n",
      "-------- loss = 2.6531447148378025 --------\n",
      "-------- epoch = 444/1000 --------\n",
      "batch: 1; loss = 2.601610916535532\n",
      "-------- loss = 2.601610916535532 --------\n",
      "-------- epoch = 445/1000 --------\n",
      "batch: 1; loss = 2.5925236372035463\n",
      "-------- loss = 2.5925236372035463 --------\n",
      "-------- epoch = 446/1000 --------\n",
      "batch: 1; loss = 2.633025103975102\n",
      "-------- loss = 2.633025103975102 --------\n",
      "-------- epoch = 447/1000 --------\n",
      "batch: 1; loss = 2.6470700635627216\n",
      "-------- loss = 2.6470700635627216 --------\n",
      "-------- epoch = 448/1000 --------\n",
      "batch: 1; loss = 2.7623656353073103\n",
      "-------- loss = 2.7623656353073103 --------\n",
      "-------- epoch = 449/1000 --------\n",
      "batch: 1; loss = 2.79567457198603\n",
      "-------- loss = 2.79567457198603 --------\n",
      "-------- epoch = 450/1000 --------\n",
      "batch: 1; loss = 2.961269257896339\n",
      "-------- loss = 2.961269257896339 --------\n",
      "-------- epoch = 451/1000 --------\n",
      "batch: 1; loss = 2.9460759185471694\n",
      "-------- loss = 2.9460759185471694 --------\n",
      "-------- epoch = 452/1000 --------\n",
      "batch: 1; loss = 3.0067488365421418\n",
      "-------- loss = 3.0067488365421418 --------\n",
      "-------- epoch = 453/1000 --------\n",
      "batch: 1; loss = 2.789776328053218\n",
      "-------- loss = 2.789776328053218 --------\n",
      "-------- epoch = 454/1000 --------\n",
      "batch: 1; loss = 2.6680107062125615\n",
      "-------- loss = 2.6680107062125615 --------\n",
      "-------- epoch = 455/1000 --------\n",
      "batch: 1; loss = 2.568236891451197\n",
      "-------- loss = 2.568236891451197 --------\n",
      "-------- epoch = 456/1000 --------\n",
      "batch: 1; loss = 2.6063455596844967\n",
      "-------- loss = 2.6063455596844967 --------\n",
      "-------- epoch = 457/1000 --------\n",
      "batch: 1; loss = 2.7755574642284877\n",
      "-------- loss = 2.7755574642284877 --------\n",
      "-------- epoch = 458/1000 --------\n",
      "batch: 1; loss = 2.847603302384797\n",
      "-------- loss = 2.847603302384797 --------\n",
      "-------- epoch = 459/1000 --------\n",
      "batch: 1; loss = 3.0045939607576315\n",
      "-------- loss = 3.0045939607576315 --------\n",
      "-------- epoch = 460/1000 --------\n",
      "batch: 1; loss = 2.915412025388642\n",
      "-------- loss = 2.915412025388642 --------\n",
      "-------- epoch = 461/1000 --------\n",
      "batch: 1; loss = 2.856056583528522\n",
      "-------- loss = 2.856056583528522 --------\n",
      "-------- epoch = 462/1000 --------\n",
      "batch: 1; loss = 2.598341128793014\n",
      "-------- loss = 2.598341128793014 --------\n",
      "-------- epoch = 463/1000 --------\n",
      "batch: 1; loss = 2.5395342224066297\n",
      "-------- loss = 2.5395342224066297 --------\n",
      "-------- epoch = 464/1000 --------\n",
      "batch: 1; loss = 2.6390076216967704\n",
      "-------- loss = 2.6390076216967704 --------\n",
      "-------- epoch = 465/1000 --------\n",
      "batch: 1; loss = 2.703547964669219\n",
      "-------- loss = 2.703547964669219 --------\n",
      "-------- epoch = 466/1000 --------\n",
      "batch: 1; loss = 2.8265548123381894\n",
      "-------- loss = 2.8265548123381894 --------\n",
      "-------- epoch = 467/1000 --------\n",
      "batch: 1; loss = 2.67385034400583\n",
      "-------- loss = 2.67385034400583 --------\n",
      "-------- epoch = 468/1000 --------\n",
      "batch: 1; loss = 2.616061845998638\n",
      "-------- loss = 2.616061845998638 --------\n",
      "-------- epoch = 469/1000 --------\n",
      "batch: 1; loss = 2.515194224929227\n",
      "-------- loss = 2.515194224929227 --------\n",
      "-------- epoch = 470/1000 --------\n",
      "batch: 1; loss = 2.501205193877373\n",
      "-------- loss = 2.501205193877373 --------\n",
      "-------- epoch = 471/1000 --------\n",
      "batch: 1; loss = 2.5602601100964013\n",
      "-------- loss = 2.5602601100964013 --------\n",
      "-------- epoch = 472/1000 --------\n",
      "batch: 1; loss = 2.595493910421251\n",
      "-------- loss = 2.595493910421251 --------\n",
      "-------- epoch = 473/1000 --------\n",
      "batch: 1; loss = 2.7022536102408528\n",
      "-------- loss = 2.7022536102408528 --------\n",
      "-------- epoch = 474/1000 --------\n",
      "batch: 1; loss = 2.660162487921687\n",
      "-------- loss = 2.660162487921687 --------\n",
      "-------- epoch = 475/1000 --------\n",
      "batch: 1; loss = 2.6744097339402835\n",
      "-------- loss = 2.6744097339402835 --------\n",
      "-------- epoch = 476/1000 --------\n",
      "batch: 1; loss = 2.534549257013079\n",
      "-------- loss = 2.534549257013079 --------\n",
      "-------- epoch = 477/1000 --------\n",
      "batch: 1; loss = 2.4858004037713433\n",
      "-------- loss = 2.4858004037713433 --------\n",
      "-------- epoch = 478/1000 --------\n",
      "batch: 1; loss = 2.466233147423467\n",
      "-------- loss = 2.466233147423467 --------\n",
      "-------- epoch = 479/1000 --------\n",
      "batch: 1; loss = 2.4789544474407417\n",
      "-------- loss = 2.4789544474407417 --------\n",
      "-------- epoch = 480/1000 --------\n",
      "batch: 1; loss = 2.5725438828626186\n",
      "-------- loss = 2.5725438828626186 --------\n",
      "-------- epoch = 481/1000 --------\n",
      "batch: 1; loss = 2.6040331381643878\n",
      "-------- loss = 2.6040331381643878 --------\n",
      "-------- epoch = 482/1000 --------\n",
      "batch: 1; loss = 2.792795905314618\n",
      "-------- loss = 2.792795905314618 --------\n",
      "-------- epoch = 483/1000 --------\n",
      "batch: 1; loss = 2.86546314740596\n",
      "-------- loss = 2.86546314740596 --------\n",
      "-------- epoch = 484/1000 --------\n",
      "batch: 1; loss = 3.064064086288162\n",
      "-------- loss = 3.064064086288162 --------\n",
      "-------- epoch = 485/1000 --------\n",
      "batch: 1; loss = 2.8935732232757103\n",
      "-------- loss = 2.8935732232757103 --------\n",
      "-------- epoch = 486/1000 --------\n",
      "batch: 1; loss = 2.7553470052351767\n",
      "-------- loss = 2.7553470052351767 --------\n",
      "-------- epoch = 487/1000 --------\n",
      "batch: 1; loss = 2.4791895803136805\n",
      "-------- loss = 2.4791895803136805 --------\n",
      "-------- epoch = 488/1000 --------\n",
      "batch: 1; loss = 2.457262777254439\n",
      "-------- loss = 2.457262777254439 --------\n",
      "-------- epoch = 489/1000 --------\n",
      "batch: 1; loss = 2.6470965802757487\n",
      "-------- loss = 2.6470965802757487 --------\n",
      "-------- epoch = 490/1000 --------\n",
      "batch: 1; loss = 2.759645214857377\n",
      "-------- loss = 2.759645214857377 --------\n",
      "-------- epoch = 491/1000 --------\n",
      "batch: 1; loss = 2.934015446836757\n",
      "-------- loss = 2.934015446836757 --------\n",
      "-------- epoch = 492/1000 --------\n",
      "batch: 1; loss = 2.8165387563965716\n",
      "-------- loss = 2.8165387563965716 --------\n",
      "-------- epoch = 493/1000 --------\n",
      "batch: 1; loss = 2.708787163731468\n",
      "-------- loss = 2.708787163731468 --------\n",
      "-------- epoch = 494/1000 --------\n",
      "batch: 1; loss = 2.472000254753373\n",
      "-------- loss = 2.472000254753373 --------\n",
      "-------- epoch = 495/1000 --------\n",
      "batch: 1; loss = 2.447873969375233\n",
      "-------- loss = 2.447873969375233 --------\n",
      "-------- epoch = 496/1000 --------\n",
      "batch: 1; loss = 2.5806273835298996\n",
      "-------- loss = 2.5806273835298996 --------\n",
      "-------- epoch = 497/1000 --------\n",
      "batch: 1; loss = 2.6247594038370363\n",
      "-------- loss = 2.6247594038370363 --------\n",
      "-------- epoch = 498/1000 --------\n",
      "batch: 1; loss = 2.6889321241418043\n",
      "-------- loss = 2.6889321241418043 --------\n",
      "-------- epoch = 499/1000 --------\n",
      "batch: 1; loss = 2.5219942982067387\n",
      "-------- loss = 2.5219942982067387 --------\n",
      "-------- epoch = 500/1000 --------\n",
      "batch: 1; loss = 2.4546260129623234\n",
      "-------- loss = 2.4546260129623234 --------\n",
      "-------- epoch = 501/1000 --------\n",
      "batch: 1; loss = 2.4101930064120562\n",
      "-------- loss = 2.4101930064120562 --------\n",
      "-------- epoch = 502/1000 --------\n",
      "batch: 1; loss = 2.4181976475696585\n",
      "-------- loss = 2.4181976475696585 --------\n",
      "-------- epoch = 503/1000 --------\n",
      "batch: 1; loss = 2.49095689004297\n",
      "-------- loss = 2.49095689004297 --------\n",
      "-------- epoch = 504/1000 --------\n",
      "batch: 1; loss = 2.5156658925467092\n",
      "-------- loss = 2.5156658925467092 --------\n",
      "-------- epoch = 505/1000 --------\n",
      "batch: 1; loss = 2.607142796946604\n",
      "-------- loss = 2.607142796946604 --------\n",
      "-------- epoch = 506/1000 --------\n",
      "batch: 1; loss = 2.5635383585825475\n",
      "-------- loss = 2.5635383585825475 --------\n",
      "-------- epoch = 507/1000 --------\n",
      "batch: 1; loss = 2.581394286744677\n",
      "-------- loss = 2.581394286744677 --------\n",
      "-------- epoch = 508/1000 --------\n",
      "batch: 1; loss = 2.4507182478311003\n",
      "-------- loss = 2.4507182478311003 --------\n",
      "-------- epoch = 509/1000 --------\n",
      "batch: 1; loss = 2.401208380054945\n",
      "-------- loss = 2.401208380054945 --------\n",
      "-------- epoch = 510/1000 --------\n",
      "batch: 1; loss = 2.3591091947622123\n",
      "-------- loss = 2.3591091947622123 --------\n",
      "-------- epoch = 511/1000 --------\n",
      "batch: 1; loss = 2.3735088241868834\n",
      "-------- loss = 2.3735088241868834 --------\n",
      "-------- epoch = 512/1000 --------\n",
      "batch: 1; loss = 2.4915567387497215\n",
      "-------- loss = 2.4915567387497215 --------\n",
      "-------- epoch = 513/1000 --------\n",
      "batch: 1; loss = 2.5798550031447935\n",
      "-------- loss = 2.5798550031447935 --------\n",
      "-------- epoch = 514/1000 --------\n",
      "batch: 1; loss = 2.8808658388056685\n",
      "-------- loss = 2.8808658388056685 --------\n",
      "-------- epoch = 515/1000 --------\n",
      "batch: 1; loss = 3.043967220763332\n",
      "-------- loss = 3.043967220763332 --------\n",
      "-------- epoch = 516/1000 --------\n",
      "batch: 1; loss = 3.285028128244947\n",
      "-------- loss = 3.285028128244947 --------\n",
      "-------- epoch = 517/1000 --------\n",
      "batch: 1; loss = 2.8723784372560215\n",
      "-------- loss = 2.8723784372560215 --------\n",
      "-------- epoch = 518/1000 --------\n",
      "batch: 1; loss = 2.5545311968836164\n",
      "-------- loss = 2.5545311968836164 --------\n",
      "-------- epoch = 519/1000 --------\n",
      "batch: 1; loss = 2.4027964425649877\n",
      "-------- loss = 2.4027964425649877 --------\n",
      "-------- epoch = 520/1000 --------\n",
      "batch: 1; loss = 2.5751907267390246\n",
      "-------- loss = 2.5751907267390246 --------\n",
      "-------- epoch = 521/1000 --------\n",
      "batch: 1; loss = 2.87777206838819\n",
      "-------- loss = 2.87777206838819 --------\n",
      "-------- epoch = 522/1000 --------\n",
      "batch: 1; loss = 2.6926320957354934\n",
      "-------- loss = 2.6926320957354934 --------\n",
      "-------- epoch = 523/1000 --------\n",
      "batch: 1; loss = 2.532498495896174\n",
      "-------- loss = 2.532498495896174 --------\n",
      "-------- epoch = 524/1000 --------\n",
      "batch: 1; loss = 2.371507729520976\n",
      "-------- loss = 2.371507729520976 --------\n",
      "-------- epoch = 525/1000 --------\n",
      "batch: 1; loss = 2.422136263918563\n",
      "-------- loss = 2.422136263918563 --------\n",
      "-------- epoch = 526/1000 --------\n",
      "batch: 1; loss = 2.608907459006886\n",
      "-------- loss = 2.608907459006886 --------\n",
      "-------- epoch = 527/1000 --------\n",
      "batch: 1; loss = 2.582603568296929\n",
      "-------- loss = 2.582603568296929 --------\n",
      "-------- epoch = 528/1000 --------\n",
      "batch: 1; loss = 2.53341946477509\n",
      "-------- loss = 2.53341946477509 --------\n",
      "-------- epoch = 529/1000 --------\n",
      "batch: 1; loss = 2.3517192561996567\n",
      "-------- loss = 2.3517192561996567 --------\n",
      "-------- epoch = 530/1000 --------\n",
      "batch: 1; loss = 2.3233592464100816\n",
      "-------- loss = 2.3233592464100816 --------\n",
      "-------- epoch = 531/1000 --------\n",
      "batch: 1; loss = 2.426339424463076\n",
      "-------- loss = 2.426339424463076 --------\n",
      "-------- epoch = 532/1000 --------\n",
      "batch: 1; loss = 2.468311585127545\n",
      "-------- loss = 2.468311585127545 --------\n",
      "-------- epoch = 533/1000 --------\n",
      "batch: 1; loss = 2.564963162846165\n",
      "-------- loss = 2.564963162846165 --------\n",
      "-------- epoch = 534/1000 --------\n",
      "batch: 1; loss = 2.476652441731176\n",
      "-------- loss = 2.476652441731176 --------\n",
      "-------- epoch = 535/1000 --------\n",
      "batch: 1; loss = 2.429571037068412\n",
      "-------- loss = 2.429571037068412 --------\n",
      "-------- epoch = 536/1000 --------\n",
      "batch: 1; loss = 2.3203501267225186\n",
      "-------- loss = 2.3203501267225186 --------\n",
      "-------- epoch = 537/1000 --------\n",
      "batch: 1; loss = 2.30232714081703\n",
      "-------- loss = 2.30232714081703 --------\n",
      "-------- epoch = 538/1000 --------\n",
      "batch: 1; loss = 2.3408977323178104\n",
      "-------- loss = 2.3408977323178104 --------\n",
      "-------- epoch = 539/1000 --------\n",
      "batch: 1; loss = 2.371386862299628\n",
      "-------- loss = 2.371386862299628 --------\n",
      "-------- epoch = 540/1000 --------\n",
      "batch: 1; loss = 2.478785335241072\n",
      "-------- loss = 2.478785335241072 --------\n",
      "-------- epoch = 541/1000 --------\n",
      "batch: 1; loss = 2.4684501929672877\n",
      "-------- loss = 2.4684501929672877 --------\n",
      "-------- epoch = 542/1000 --------\n",
      "batch: 1; loss = 2.5172447030184055\n",
      "-------- loss = 2.5172447030184055 --------\n",
      "-------- epoch = 543/1000 --------\n",
      "batch: 1; loss = 2.4345320158771195\n",
      "-------- loss = 2.4345320158771195 --------\n",
      "-------- epoch = 544/1000 --------\n",
      "batch: 1; loss = 2.4192520394643044\n",
      "-------- loss = 2.4192520394643044 --------\n",
      "-------- epoch = 545/1000 --------\n",
      "batch: 1; loss = 2.30977072033818\n",
      "-------- loss = 2.30977072033818 --------\n",
      "-------- epoch = 546/1000 --------\n",
      "batch: 1; loss = 2.282831549065047\n",
      "-------- loss = 2.282831549065047 --------\n",
      "-------- epoch = 547/1000 --------\n",
      "batch: 1; loss = 2.285702938193702\n",
      "-------- loss = 2.285702938193702 --------\n",
      "-------- epoch = 548/1000 --------\n",
      "batch: 1; loss = 2.308240731607616\n",
      "-------- loss = 2.308240731607616 --------\n",
      "-------- epoch = 549/1000 --------\n",
      "batch: 1; loss = 2.43767271714988\n",
      "-------- loss = 2.43767271714988 --------\n",
      "-------- epoch = 550/1000 --------\n",
      "batch: 1; loss = 2.51243001879898\n",
      "-------- loss = 2.51243001879898 --------\n",
      "-------- epoch = 551/1000 --------\n",
      "batch: 1; loss = 2.7331392439626416\n",
      "-------- loss = 2.7331392439626416 --------\n",
      "-------- epoch = 552/1000 --------\n",
      "batch: 1; loss = 2.7500932063209826\n",
      "-------- loss = 2.7500932063209826 --------\n",
      "-------- epoch = 553/1000 --------\n",
      "batch: 1; loss = 2.8288155488918236\n",
      "-------- loss = 2.8288155488918236 --------\n",
      "-------- epoch = 554/1000 --------\n",
      "batch: 1; loss = 2.5267203082244643\n",
      "-------- loss = 2.5267203082244643 --------\n",
      "-------- epoch = 555/1000 --------\n",
      "batch: 1; loss = 2.3557575518642375\n",
      "-------- loss = 2.3557575518642375 --------\n",
      "-------- epoch = 556/1000 --------\n",
      "batch: 1; loss = 2.2701961672686806\n",
      "-------- loss = 2.2701961672686806 --------\n",
      "-------- epoch = 557/1000 --------\n",
      "batch: 1; loss = 2.3439313212671196\n",
      "-------- loss = 2.3439313212671196 --------\n",
      "-------- epoch = 558/1000 --------\n",
      "batch: 1; loss = 2.5553396192638744\n",
      "-------- loss = 2.5553396192638744 --------\n",
      "-------- epoch = 559/1000 --------\n",
      "batch: 1; loss = 2.5749361141306903\n",
      "-------- loss = 2.5749361141306903 --------\n",
      "-------- epoch = 560/1000 --------\n",
      "batch: 1; loss = 2.6050537847414357\n",
      "-------- loss = 2.6050537847414357 --------\n",
      "-------- epoch = 561/1000 --------\n",
      "batch: 1; loss = 2.3882257684455217\n",
      "-------- loss = 2.3882257684455217 --------\n",
      "-------- epoch = 562/1000 --------\n",
      "batch: 1; loss = 2.2909725224288504\n",
      "-------- loss = 2.2909725224288504 --------\n",
      "-------- epoch = 563/1000 --------\n",
      "batch: 1; loss = 2.261318290419735\n",
      "-------- loss = 2.261318290419735 --------\n",
      "-------- epoch = 564/1000 --------\n",
      "batch: 1; loss = 2.3148031951874715\n",
      "-------- loss = 2.3148031951874715 --------\n",
      "-------- epoch = 565/1000 --------\n",
      "batch: 1; loss = 2.463365311905638\n",
      "-------- loss = 2.463365311905638 --------\n",
      "-------- epoch = 566/1000 --------\n",
      "batch: 1; loss = 2.4092697457008305\n",
      "-------- loss = 2.4092697457008305 --------\n",
      "-------- epoch = 567/1000 --------\n",
      "batch: 1; loss = 2.412983235145076\n",
      "-------- loss = 2.412983235145076 --------\n",
      "-------- epoch = 568/1000 --------\n",
      "batch: 1; loss = 2.273607127044136\n",
      "-------- loss = 2.273607127044136 --------\n",
      "-------- epoch = 569/1000 --------\n",
      "batch: 1; loss = 2.230728330576716\n",
      "-------- loss = 2.230728330576716 --------\n",
      "-------- epoch = 570/1000 --------\n",
      "batch: 1; loss = 2.2296656189618083\n",
      "-------- loss = 2.2296656189618083 --------\n",
      "-------- epoch = 571/1000 --------\n",
      "batch: 1; loss = 2.254348394671408\n",
      "-------- loss = 2.254348394671408 --------\n",
      "-------- epoch = 572/1000 --------\n",
      "batch: 1; loss = 2.352799682807755\n",
      "-------- loss = 2.352799682807755 --------\n",
      "-------- epoch = 573/1000 --------\n",
      "batch: 1; loss = 2.347474848410313\n",
      "-------- loss = 2.347474848410313 --------\n",
      "-------- epoch = 574/1000 --------\n",
      "batch: 1; loss = 2.401377695730002\n",
      "-------- loss = 2.401377695730002 --------\n",
      "-------- epoch = 575/1000 --------\n",
      "batch: 1; loss = 2.3034452491948034\n",
      "-------- loss = 2.3034452491948034 --------\n",
      "-------- epoch = 576/1000 --------\n",
      "batch: 1; loss = 2.2716116898930405\n",
      "-------- loss = 2.2716116898930405 --------\n",
      "-------- epoch = 577/1000 --------\n",
      "batch: 1; loss = 2.188782784701775\n",
      "-------- loss = 2.188782784701775 --------\n",
      "-------- epoch = 578/1000 --------\n",
      "batch: 1; loss = 2.1768608553933233\n",
      "-------- loss = 2.1768608553933233 --------\n",
      "-------- epoch = 579/1000 --------\n",
      "batch: 1; loss = 2.2173612786867705\n",
      "-------- loss = 2.2173612786867705 --------\n",
      "-------- epoch = 580/1000 --------\n",
      "batch: 1; loss = 2.228051019780852\n",
      "-------- loss = 2.228051019780852 --------\n",
      "-------- epoch = 581/1000 --------\n",
      "batch: 1; loss = 2.3134822672974122\n",
      "-------- loss = 2.3134822672974122 --------\n",
      "-------- epoch = 582/1000 --------\n",
      "batch: 1; loss = 2.3046836606898284\n",
      "-------- loss = 2.3046836606898284 --------\n",
      "-------- epoch = 583/1000 --------\n",
      "batch: 1; loss = 2.3745244378304915\n",
      "-------- loss = 2.3745244378304915 --------\n",
      "-------- epoch = 584/1000 --------\n",
      "batch: 1; loss = 2.324412536708665\n",
      "-------- loss = 2.324412536708665 --------\n",
      "-------- epoch = 585/1000 --------\n",
      "batch: 1; loss = 2.346241619304372\n",
      "-------- loss = 2.346241619304372 --------\n",
      "-------- epoch = 586/1000 --------\n",
      "batch: 1; loss = 2.261569612355253\n",
      "-------- loss = 2.261569612355253 --------\n",
      "-------- epoch = 587/1000 --------\n",
      "batch: 1; loss = 2.248463489018228\n",
      "-------- loss = 2.248463489018228 --------\n",
      "-------- epoch = 588/1000 --------\n",
      "batch: 1; loss = 2.171993911663733\n",
      "-------- loss = 2.171993911663733 --------\n",
      "-------- epoch = 589/1000 --------\n",
      "batch: 1; loss = 2.1590783813549863\n",
      "-------- loss = 2.1590783813549863 --------\n",
      "-------- epoch = 590/1000 --------\n",
      "batch: 1; loss = 2.159431822204103\n",
      "-------- loss = 2.159431822204103 --------\n",
      "-------- epoch = 591/1000 --------\n",
      "batch: 1; loss = 2.1616265971069457\n",
      "-------- loss = 2.1616265971069457 --------\n",
      "-------- epoch = 592/1000 --------\n",
      "batch: 1; loss = 2.220348105372643\n",
      "-------- loss = 2.220348105372643 --------\n",
      "-------- epoch = 593/1000 --------\n",
      "batch: 1; loss = 2.2236859279613665\n",
      "-------- loss = 2.2236859279613665 --------\n",
      "-------- epoch = 594/1000 --------\n",
      "batch: 1; loss = 2.3132565030717345\n",
      "-------- loss = 2.3132565030717345 --------\n",
      "-------- epoch = 595/1000 --------\n",
      "batch: 1; loss = 2.2938720727502258\n",
      "-------- loss = 2.2938720727502258 --------\n",
      "-------- epoch = 596/1000 --------\n",
      "batch: 1; loss = 2.3764702858010427\n",
      "-------- loss = 2.3764702858010427 --------\n",
      "-------- epoch = 597/1000 --------\n",
      "batch: 1; loss = 2.3040065935497958\n",
      "-------- loss = 2.3040065935497958 --------\n",
      "-------- epoch = 598/1000 --------\n",
      "batch: 1; loss = 2.3337542949724086\n",
      "-------- loss = 2.3337542949724086 --------\n",
      "-------- epoch = 599/1000 --------\n",
      "batch: 1; loss = 2.2279960064401343\n",
      "-------- loss = 2.2279960064401343 --------\n",
      "-------- epoch = 600/1000 --------\n",
      "batch: 1; loss = 2.2059940260254187\n",
      "-------- loss = 2.2059940260254187 --------\n",
      "-------- epoch = 601/1000 --------\n",
      "batch: 1; loss = 2.1347452185176876\n",
      "-------- loss = 2.1347452185176876 --------\n",
      "-------- epoch = 602/1000 --------\n",
      "batch: 1; loss = 2.1249557457436064\n",
      "-------- loss = 2.1249557457436064 --------\n",
      "-------- epoch = 603/1000 --------\n",
      "batch: 1; loss = 2.1258559488616013\n",
      "-------- loss = 2.1258559488616013 --------\n",
      "-------- epoch = 604/1000 --------\n",
      "batch: 1; loss = 2.127704518322347\n",
      "-------- loss = 2.127704518322347 --------\n",
      "-------- epoch = 605/1000 --------\n",
      "batch: 1; loss = 2.1803282677827887\n",
      "-------- loss = 2.1803282677827887 --------\n",
      "-------- epoch = 606/1000 --------\n",
      "batch: 1; loss = 2.186738128070852\n",
      "-------- loss = 2.186738128070852 --------\n",
      "-------- epoch = 607/1000 --------\n",
      "batch: 1; loss = 2.282870798361809\n",
      "-------- loss = 2.282870798361809 --------\n",
      "-------- epoch = 608/1000 --------\n",
      "batch: 1; loss = 2.266540945989982\n",
      "-------- loss = 2.266540945989982 --------\n",
      "-------- epoch = 609/1000 --------\n",
      "batch: 1; loss = 2.3473679375525256\n",
      "-------- loss = 2.3473679375525256 --------\n",
      "-------- epoch = 610/1000 --------\n",
      "batch: 1; loss = 2.2661505686327135\n",
      "-------- loss = 2.2661505686327135 --------\n",
      "-------- epoch = 611/1000 --------\n",
      "batch: 1; loss = 2.2919539713538337\n",
      "-------- loss = 2.2919539713538337 --------\n",
      "-------- epoch = 612/1000 --------\n",
      "batch: 1; loss = 2.188301588602789\n",
      "-------- loss = 2.188301588602789 --------\n",
      "-------- epoch = 613/1000 --------\n",
      "batch: 1; loss = 2.1725031837770183\n",
      "-------- loss = 2.1725031837770183 --------\n",
      "-------- epoch = 614/1000 --------\n",
      "batch: 1; loss = 2.110440681660324\n",
      "-------- loss = 2.110440681660324 --------\n",
      "-------- epoch = 615/1000 --------\n",
      "batch: 1; loss = 2.1037115974091765\n",
      "-------- loss = 2.1037115974091765 --------\n",
      "-------- epoch = 616/1000 --------\n",
      "batch: 1; loss = 2.090158528343434\n",
      "-------- loss = 2.090158528343434 --------\n",
      "-------- epoch = 617/1000 --------\n",
      "batch: 1; loss = 2.0870176649335193\n",
      "-------- loss = 2.0870176649335193 --------\n",
      "-------- epoch = 618/1000 --------\n",
      "batch: 1; loss = 2.1204389651284825\n",
      "-------- loss = 2.1204389651284825 --------\n",
      "-------- epoch = 619/1000 --------\n",
      "batch: 1; loss = 2.124554747448198\n",
      "-------- loss = 2.124554747448198 --------\n",
      "-------- epoch = 620/1000 --------\n",
      "batch: 1; loss = 2.1916424394356784\n",
      "-------- loss = 2.1916424394356784 --------\n",
      "-------- epoch = 621/1000 --------\n",
      "batch: 1; loss = 2.18093927013664\n",
      "-------- loss = 2.18093927013664 --------\n",
      "-------- epoch = 622/1000 --------\n",
      "batch: 1; loss = 2.256357264830651\n",
      "-------- loss = 2.256357264830651 --------\n",
      "-------- epoch = 623/1000 --------\n",
      "batch: 1; loss = 2.227025947150124\n",
      "-------- loss = 2.227025947150124 --------\n",
      "-------- epoch = 624/1000 --------\n",
      "batch: 1; loss = 2.2969537476642077\n",
      "-------- loss = 2.2969537476642077 --------\n",
      "-------- epoch = 625/1000 --------\n",
      "batch: 1; loss = 2.2454157412610414\n",
      "-------- loss = 2.2454157412610414 --------\n",
      "-------- epoch = 626/1000 --------\n",
      "batch: 1; loss = 2.286793143981108\n",
      "-------- loss = 2.286793143981108 --------\n",
      "-------- epoch = 627/1000 --------\n",
      "batch: 1; loss = 2.1985904090752144\n",
      "-------- loss = 2.1985904090752144 --------\n",
      "-------- epoch = 628/1000 --------\n",
      "batch: 1; loss = 2.1934424844451894\n",
      "-------- loss = 2.1934424844451894 --------\n",
      "-------- epoch = 629/1000 --------\n",
      "batch: 1; loss = 2.102474175905826\n",
      "-------- loss = 2.102474175905826 --------\n",
      "-------- epoch = 630/1000 --------\n",
      "batch: 1; loss = 2.085813197256688\n",
      "-------- loss = 2.085813197256688 --------\n",
      "-------- epoch = 631/1000 --------\n",
      "batch: 1; loss = 2.0634932133493593\n",
      "-------- loss = 2.0634932133493593 --------\n",
      "-------- epoch = 632/1000 --------\n",
      "batch: 1; loss = 2.06545167641915\n",
      "-------- loss = 2.06545167641915 --------\n",
      "-------- epoch = 633/1000 --------\n",
      "batch: 1; loss = 2.0993470981790554\n",
      "-------- loss = 2.0993470981790554 --------\n",
      "-------- epoch = 634/1000 --------\n",
      "batch: 1; loss = 2.1071976423510503\n",
      "-------- loss = 2.1071976423510503 --------\n",
      "-------- epoch = 635/1000 --------\n",
      "batch: 1; loss = 2.1791219697730195\n",
      "-------- loss = 2.1791219697730195 --------\n",
      "-------- epoch = 636/1000 --------\n",
      "batch: 1; loss = 2.1865571564795805\n",
      "-------- loss = 2.1865571564795805 --------\n",
      "-------- epoch = 637/1000 --------\n",
      "batch: 1; loss = 2.2928390826970575\n",
      "-------- loss = 2.2928390826970575 --------\n",
      "-------- epoch = 638/1000 --------\n",
      "batch: 1; loss = 2.286007503497554\n",
      "-------- loss = 2.286007503497554 --------\n",
      "-------- epoch = 639/1000 --------\n",
      "batch: 1; loss = 2.3940317138766907\n",
      "-------- loss = 2.3940317138766907 --------\n",
      "-------- epoch = 640/1000 --------\n",
      "batch: 1; loss = 2.3155630014714883\n",
      "-------- loss = 2.3155630014714883 --------\n",
      "-------- epoch = 641/1000 --------\n",
      "batch: 1; loss = 2.33815178949834\n",
      "-------- loss = 2.33815178949834 --------\n",
      "-------- epoch = 642/1000 --------\n",
      "batch: 1; loss = 2.1846067514134044\n",
      "-------- loss = 2.1846067514134044 --------\n",
      "-------- epoch = 643/1000 --------\n",
      "batch: 1; loss = 2.138021805864983\n",
      "-------- loss = 2.138021805864983 --------\n",
      "-------- epoch = 644/1000 --------\n",
      "batch: 1; loss = 2.0608438514606324\n",
      "-------- loss = 2.0608438514606324 --------\n",
      "-------- epoch = 645/1000 --------\n",
      "batch: 1; loss = 2.0528483723715056\n",
      "-------- loss = 2.0528483723715056 --------\n",
      "-------- epoch = 646/1000 --------\n",
      "batch: 1; loss = 2.077565369771738\n",
      "-------- loss = 2.077565369771738 --------\n",
      "-------- epoch = 647/1000 --------\n",
      "batch: 1; loss = 2.0926852996553094\n",
      "-------- loss = 2.0926852996553094 --------\n",
      "-------- epoch = 648/1000 --------\n",
      "batch: 1; loss = 2.166849026244273\n",
      "-------- loss = 2.166849026244273 --------\n",
      "-------- epoch = 649/1000 --------\n",
      "batch: 1; loss = 2.1625477277561846\n",
      "-------- loss = 2.1625477277561846 --------\n",
      "-------- epoch = 650/1000 --------\n",
      "batch: 1; loss = 2.2377790292168753\n",
      "-------- loss = 2.2377790292168753 --------\n",
      "-------- epoch = 651/1000 --------\n",
      "batch: 1; loss = 2.1942608580205554\n",
      "-------- loss = 2.1942608580205554 --------\n",
      "-------- epoch = 652/1000 --------\n",
      "batch: 1; loss = 2.234528459428394\n",
      "-------- loss = 2.234528459428394 --------\n",
      "-------- epoch = 653/1000 --------\n",
      "batch: 1; loss = 2.148467851233824\n",
      "-------- loss = 2.148467851233824 --------\n",
      "-------- epoch = 654/1000 --------\n",
      "batch: 1; loss = 2.143276308292681\n",
      "-------- loss = 2.143276308292681 --------\n",
      "-------- epoch = 655/1000 --------\n",
      "batch: 1; loss = 2.069496406724526\n",
      "-------- loss = 2.069496406724526 --------\n",
      "-------- epoch = 656/1000 --------\n",
      "batch: 1; loss = 2.062093296119583\n",
      "-------- loss = 2.062093296119583 --------\n",
      "-------- epoch = 657/1000 --------\n",
      "batch: 1; loss = 2.0499883535680614\n",
      "-------- loss = 2.0499883535680614 --------\n",
      "-------- epoch = 658/1000 --------\n",
      "batch: 1; loss = 2.049264841136278\n",
      "-------- loss = 2.049264841136278 --------\n",
      "-------- epoch = 659/1000 --------\n",
      "batch: 1; loss = 2.0670768741537504\n",
      "-------- loss = 2.0670768741537504 --------\n",
      "-------- epoch = 660/1000 --------\n",
      "batch: 1; loss = 2.0670320926338053\n",
      "-------- loss = 2.0670320926338053 --------\n",
      "-------- epoch = 661/1000 --------\n",
      "batch: 1; loss = 2.109800321294998\n",
      "-------- loss = 2.109800321294998 --------\n",
      "-------- epoch = 662/1000 --------\n",
      "batch: 1; loss = 2.1063836843550168\n",
      "-------- loss = 2.1063836843550168 --------\n",
      "-------- epoch = 663/1000 --------\n",
      "batch: 1; loss = 2.1766313214213118\n",
      "-------- loss = 2.1766313214213118 --------\n",
      "-------- epoch = 664/1000 --------\n",
      "batch: 1; loss = 2.164099885260916\n",
      "-------- loss = 2.164099885260916 --------\n",
      "-------- epoch = 665/1000 --------\n",
      "batch: 1; loss = 2.2527748057684214\n",
      "-------- loss = 2.2527748057684214 --------\n",
      "-------- epoch = 666/1000 --------\n",
      "batch: 1; loss = 2.2402064124676397\n",
      "-------- loss = 2.2402064124676397 --------\n",
      "-------- epoch = 667/1000 --------\n",
      "batch: 1; loss = 2.359290748483873\n",
      "-------- loss = 2.359290748483873 --------\n",
      "-------- epoch = 668/1000 --------\n",
      "batch: 1; loss = 2.332800493957706\n",
      "-------- loss = 2.332800493957706 --------\n",
      "-------- epoch = 669/1000 --------\n",
      "batch: 1; loss = 2.460284291083378\n",
      "-------- loss = 2.460284291083378 --------\n",
      "-------- epoch = 670/1000 --------\n",
      "batch: 1; loss = 2.366339913138144\n",
      "-------- loss = 2.366339913138144 --------\n",
      "-------- epoch = 671/1000 --------\n",
      "batch: 1; loss = 2.4145026754531433\n",
      "-------- loss = 2.4145026754531433 --------\n",
      "-------- epoch = 672/1000 --------\n",
      "batch: 1; loss = 2.235066773115686\n",
      "-------- loss = 2.235066773115686 --------\n",
      "-------- epoch = 673/1000 --------\n",
      "batch: 1; loss = 2.178722422852967\n",
      "-------- loss = 2.178722422852967 --------\n",
      "-------- epoch = 674/1000 --------\n",
      "batch: 1; loss = 2.0582960399532553\n",
      "-------- loss = 2.0582960399532553 --------\n",
      "-------- epoch = 675/1000 --------\n",
      "batch: 1; loss = 2.0438885733319663\n",
      "-------- loss = 2.0438885733319663 --------\n",
      "-------- epoch = 676/1000 --------\n",
      "batch: 1; loss = 2.092054013762918\n",
      "-------- loss = 2.092054013762918 --------\n",
      "-------- epoch = 677/1000 --------\n",
      "batch: 1; loss = 2.115741909746703\n",
      "-------- loss = 2.115741909746703 --------\n",
      "-------- epoch = 678/1000 --------\n",
      "batch: 1; loss = 2.210295244418273\n",
      "-------- loss = 2.210295244418273 --------\n",
      "-------- epoch = 679/1000 --------\n",
      "batch: 1; loss = 2.1813439574755917\n",
      "-------- loss = 2.1813439574755917 --------\n",
      "-------- epoch = 680/1000 --------\n",
      "batch: 1; loss = 2.2294435146302294\n",
      "-------- loss = 2.2294435146302294 --------\n",
      "-------- epoch = 681/1000 --------\n",
      "batch: 1; loss = 2.1554370903496873\n",
      "-------- loss = 2.1554370903496873 --------\n",
      "-------- epoch = 682/1000 --------\n",
      "batch: 1; loss = 2.1626620210478853\n",
      "-------- loss = 2.1626620210478853 --------\n",
      "-------- epoch = 683/1000 --------\n",
      "batch: 1; loss = 2.08539638986947\n",
      "-------- loss = 2.08539638986947 --------\n",
      "-------- epoch = 684/1000 --------\n",
      "batch: 1; loss = 2.079189741999517\n",
      "-------- loss = 2.079189741999517 --------\n",
      "-------- epoch = 685/1000 --------\n",
      "batch: 1; loss = 2.0444410191736204\n",
      "-------- loss = 2.0444410191736204 --------\n",
      "-------- epoch = 686/1000 --------\n",
      "batch: 1; loss = 2.045062730297968\n",
      "-------- loss = 2.045062730297968 --------\n",
      "-------- epoch = 687/1000 --------\n",
      "batch: 1; loss = 2.051817784854762\n",
      "-------- loss = 2.051817784854762 --------\n",
      "-------- epoch = 688/1000 --------\n",
      "batch: 1; loss = 2.0517511309659002\n",
      "-------- loss = 2.0517511309659002 --------\n",
      "-------- epoch = 689/1000 --------\n",
      "batch: 1; loss = 2.0891556794706694\n",
      "-------- loss = 2.0891556794706694 --------\n",
      "-------- epoch = 690/1000 --------\n",
      "batch: 1; loss = 2.0908929681305426\n",
      "-------- loss = 2.0908929681305426 --------\n",
      "-------- epoch = 691/1000 --------\n",
      "batch: 1; loss = 2.1690550624710365\n",
      "-------- loss = 2.1690550624710365 --------\n",
      "-------- epoch = 692/1000 --------\n",
      "batch: 1; loss = 2.170549161751223\n",
      "-------- loss = 2.170549161751223 --------\n",
      "-------- epoch = 693/1000 --------\n",
      "batch: 1; loss = 2.2906326681007463\n",
      "-------- loss = 2.2906326681007463 --------\n",
      "-------- epoch = 694/1000 --------\n",
      "batch: 1; loss = 2.285720572183164\n",
      "-------- loss = 2.285720572183164 --------\n",
      "-------- epoch = 695/1000 --------\n",
      "batch: 1; loss = 2.4395720153786447\n",
      "-------- loss = 2.4395720153786447 --------\n",
      "-------- epoch = 696/1000 --------\n",
      "batch: 1; loss = 2.392566756970568\n",
      "-------- loss = 2.392566756970568 --------\n",
      "-------- epoch = 697/1000 --------\n",
      "batch: 1; loss = 2.5016860229805733\n",
      "-------- loss = 2.5016860229805733 --------\n",
      "-------- epoch = 698/1000 --------\n",
      "batch: 1; loss = 2.3473922494898334\n",
      "-------- loss = 2.3473922494898334 --------\n",
      "-------- epoch = 699/1000 --------\n",
      "batch: 1; loss = 2.32507948359433\n",
      "-------- loss = 2.32507948359433 --------\n",
      "-------- epoch = 700/1000 --------\n",
      "batch: 1; loss = 2.1370930714898373\n",
      "-------- loss = 2.1370930714898373 --------\n",
      "-------- epoch = 701/1000 --------\n",
      "batch: 1; loss = 2.0831110317354873\n",
      "-------- loss = 2.0831110317354873 --------\n",
      "-------- epoch = 702/1000 --------\n",
      "batch: 1; loss = 2.0419576746003085\n",
      "-------- loss = 2.0419576746003085 --------\n",
      "-------- epoch = 703/1000 --------\n",
      "batch: 1; loss = 2.055571314877217\n",
      "-------- loss = 2.055571314877217 --------\n",
      "-------- epoch = 704/1000 --------\n",
      "batch: 1; loss = 2.1403487360756315\n",
      "-------- loss = 2.1403487360756315 --------\n",
      "-------- epoch = 705/1000 --------\n",
      "batch: 1; loss = 2.157554579053238\n",
      "-------- loss = 2.157554579053238 --------\n",
      "-------- epoch = 706/1000 --------\n",
      "batch: 1; loss = 2.251316245919482\n",
      "-------- loss = 2.251316245919482 --------\n",
      "-------- epoch = 707/1000 --------\n",
      "batch: 1; loss = 2.2125676112284216\n",
      "-------- loss = 2.2125676112284216 --------\n",
      "-------- epoch = 708/1000 --------\n",
      "batch: 1; loss = 2.255666271854224\n",
      "-------- loss = 2.255666271854224 --------\n",
      "-------- epoch = 709/1000 --------\n",
      "batch: 1; loss = 2.1566584746454533\n",
      "-------- loss = 2.1566584746454533 --------\n",
      "-------- epoch = 710/1000 --------\n",
      "batch: 1; loss = 2.1402238704228584\n",
      "-------- loss = 2.1402238704228584 --------\n",
      "-------- epoch = 711/1000 --------\n",
      "batch: 1; loss = 2.063346208553698\n",
      "-------- loss = 2.063346208553698 --------\n",
      "-------- epoch = 712/1000 --------\n",
      "batch: 1; loss = 2.051078266563401\n",
      "-------- loss = 2.051078266563401 --------\n",
      "-------- epoch = 713/1000 --------\n",
      "batch: 1; loss = 2.0446416901468636\n",
      "-------- loss = 2.0446416901468636 --------\n",
      "-------- epoch = 714/1000 --------\n",
      "batch: 1; loss = 2.052684765041262\n",
      "-------- loss = 2.052684765041262 --------\n",
      "-------- epoch = 715/1000 --------\n",
      "batch: 1; loss = 2.1120015239210126\n",
      "-------- loss = 2.1120015239210126 --------\n",
      "-------- epoch = 716/1000 --------\n",
      "batch: 1; loss = 2.117853840460676\n",
      "-------- loss = 2.117853840460676 --------\n",
      "-------- epoch = 717/1000 --------\n",
      "batch: 1; loss = 2.1991613814492124\n",
      "-------- loss = 2.1991613814492124 --------\n",
      "-------- epoch = 718/1000 --------\n",
      "batch: 1; loss = 2.173336662176406\n",
      "-------- loss = 2.173336662176406 --------\n",
      "-------- epoch = 719/1000 --------\n",
      "batch: 1; loss = 2.2492999847690345\n",
      "-------- loss = 2.2492999847690345 --------\n",
      "-------- epoch = 720/1000 --------\n",
      "batch: 1; loss = 2.2023523088790804\n",
      "-------- loss = 2.2023523088790804 --------\n",
      "-------- epoch = 721/1000 --------\n",
      "batch: 1; loss = 2.2588749664602883\n",
      "-------- loss = 2.2588749664602883 --------\n",
      "-------- epoch = 722/1000 --------\n",
      "batch: 1; loss = 2.1922936565703215\n",
      "-------- loss = 2.1922936565703215 --------\n",
      "-------- epoch = 723/1000 --------\n",
      "batch: 1; loss = 2.2216410537386886\n",
      "-------- loss = 2.2216410537386886 --------\n",
      "-------- epoch = 724/1000 --------\n",
      "batch: 1; loss = 2.1302035316257864\n",
      "-------- loss = 2.1302035316257864 --------\n",
      "-------- epoch = 725/1000 --------\n",
      "batch: 1; loss = 2.125351185523267\n",
      "-------- loss = 2.125351185523267 --------\n",
      "-------- epoch = 726/1000 --------\n",
      "batch: 1; loss = 2.063806353606845\n",
      "-------- loss = 2.063806353606845 --------\n",
      "-------- epoch = 727/1000 --------\n",
      "batch: 1; loss = 2.0611397247574224\n",
      "-------- loss = 2.0611397247574224 --------\n",
      "-------- epoch = 728/1000 --------\n",
      "batch: 1; loss = 2.039605434910667\n",
      "-------- loss = 2.039605434910667 --------\n",
      "-------- epoch = 729/1000 --------\n",
      "batch: 1; loss = 2.0409771623048116\n",
      "-------- loss = 2.0409771623048116 --------\n",
      "-------- epoch = 730/1000 --------\n",
      "batch: 1; loss = 2.049963856042732\n",
      "-------- loss = 2.049963856042732 --------\n",
      "-------- epoch = 731/1000 --------\n",
      "batch: 1; loss = 2.047670485512002\n",
      "-------- loss = 2.047670485512002 --------\n",
      "-------- epoch = 732/1000 --------\n",
      "batch: 1; loss = 2.083558383622204\n",
      "-------- loss = 2.083558383622204 --------\n",
      "-------- epoch = 733/1000 --------\n",
      "batch: 1; loss = 2.0900170806797598\n",
      "-------- loss = 2.0900170806797598 --------\n",
      "-------- epoch = 734/1000 --------\n",
      "batch: 1; loss = 2.174685938410366\n",
      "-------- loss = 2.174685938410366 --------\n",
      "-------- epoch = 735/1000 --------\n",
      "batch: 1; loss = 2.207687560086194\n",
      "-------- loss = 2.207687560086194 --------\n",
      "-------- epoch = 736/1000 --------\n",
      "batch: 1; loss = 2.3835243892175892\n",
      "-------- loss = 2.3835243892175892 --------\n",
      "-------- epoch = 737/1000 --------\n",
      "batch: 1; loss = 2.4613272459198785\n",
      "-------- loss = 2.4613272459198785 --------\n",
      "-------- epoch = 738/1000 --------\n",
      "batch: 1; loss = 2.765154269207726\n",
      "-------- loss = 2.765154269207726 --------\n",
      "-------- epoch = 739/1000 --------\n",
      "batch: 1; loss = 2.7852821486425015\n",
      "-------- loss = 2.7852821486425015 --------\n",
      "-------- epoch = 740/1000 --------\n",
      "batch: 1; loss = 2.9868734059777293\n",
      "-------- loss = 2.9868734059777293 --------\n",
      "-------- epoch = 741/1000 --------\n",
      "batch: 1; loss = 2.635138736269029\n",
      "-------- loss = 2.635138736269029 --------\n",
      "-------- epoch = 742/1000 --------\n",
      "batch: 1; loss = 2.4060452101783825\n",
      "-------- loss = 2.4060452101783825 --------\n",
      "-------- epoch = 743/1000 --------\n",
      "batch: 1; loss = 2.069417348099973\n",
      "-------- loss = 2.069417348099973 --------\n",
      "-------- epoch = 744/1000 --------\n",
      "batch: 1; loss = 2.0614835832237373\n",
      "-------- loss = 2.0614835832237373 --------\n",
      "-------- epoch = 745/1000 --------\n",
      "batch: 1; loss = 2.3040278609394678\n",
      "-------- loss = 2.3040278609394678 --------\n",
      "-------- epoch = 746/1000 --------\n",
      "batch: 1; loss = 2.3692276911151233\n",
      "-------- loss = 2.3692276911151233 --------\n",
      "-------- epoch = 747/1000 --------\n",
      "batch: 1; loss = 2.4294949163244097\n",
      "-------- loss = 2.4294949163244097 --------\n",
      "-------- epoch = 748/1000 --------\n",
      "batch: 1; loss = 2.1606100407484132\n",
      "-------- loss = 2.1606100407484132 --------\n",
      "-------- epoch = 749/1000 --------\n",
      "batch: 1; loss = 2.057280803200278\n",
      "-------- loss = 2.057280803200278 --------\n",
      "-------- epoch = 750/1000 --------\n",
      "batch: 1; loss = 2.0920155409151353\n",
      "-------- loss = 2.0920155409151353 --------\n",
      "-------- epoch = 751/1000 --------\n",
      "batch: 1; loss = 2.171881980277281\n",
      "-------- loss = 2.171881980277281 --------\n",
      "-------- epoch = 752/1000 --------\n",
      "batch: 1; loss = 2.3087118438460585\n",
      "-------- loss = 2.3087118438460585 --------\n",
      "-------- epoch = 753/1000 --------\n",
      "batch: 1; loss = 2.204403156681776\n",
      "-------- loss = 2.204403156681776 --------\n",
      "-------- epoch = 754/1000 --------\n",
      "batch: 1; loss = 2.1451886072836595\n",
      "-------- loss = 2.1451886072836595 --------\n",
      "-------- epoch = 755/1000 --------\n",
      "batch: 1; loss = 2.04843266335866\n",
      "-------- loss = 2.04843266335866 --------\n",
      "-------- epoch = 756/1000 --------\n",
      "batch: 1; loss = 2.061978575007928\n",
      "-------- loss = 2.061978575007928 --------\n",
      "-------- epoch = 757/1000 --------\n",
      "batch: 1; loss = 2.1634813690038444\n",
      "-------- loss = 2.1634813690038444 --------\n",
      "-------- epoch = 758/1000 --------\n",
      "batch: 1; loss = 2.156957360190107\n",
      "-------- loss = 2.156957360190107 --------\n",
      "-------- epoch = 759/1000 --------\n",
      "batch: 1; loss = 2.186642490319628\n",
      "-------- loss = 2.186642490319628 --------\n",
      "-------- epoch = 760/1000 --------\n",
      "batch: 1; loss = 2.0894388118471503\n",
      "-------- loss = 2.0894388118471503 --------\n",
      "-------- epoch = 761/1000 --------\n",
      "batch: 1; loss = 2.062059343800674\n",
      "-------- loss = 2.062059343800674 --------\n",
      "-------- epoch = 762/1000 --------\n",
      "batch: 1; loss = 2.039951152146437\n",
      "-------- loss = 2.039951152146437 --------\n",
      "-------- epoch = 763/1000 --------\n",
      "batch: 1; loss = 2.049240489972411\n",
      "-------- loss = 2.049240489972411 --------\n",
      "-------- epoch = 764/1000 --------\n",
      "batch: 1; loss = 2.109402326425799\n",
      "-------- loss = 2.109402326425799 --------\n",
      "-------- epoch = 765/1000 --------\n",
      "batch: 1; loss = 2.0984817613573927\n",
      "-------- loss = 2.0984817613573927 --------\n",
      "-------- epoch = 766/1000 --------\n",
      "batch: 1; loss = 2.1399341134448107\n",
      "-------- loss = 2.1399341134448107 --------\n",
      "-------- epoch = 767/1000 --------\n",
      "batch: 1; loss = 2.095675846126026\n",
      "-------- loss = 2.095675846126026 --------\n",
      "-------- epoch = 768/1000 --------\n",
      "batch: 1; loss = 2.1065259123310773\n",
      "-------- loss = 2.1065259123310773 --------\n",
      "-------- epoch = 769/1000 --------\n",
      "batch: 1; loss = 2.0573593304482096\n",
      "-------- loss = 2.0573593304482096 --------\n",
      "-------- epoch = 770/1000 --------\n",
      "batch: 1; loss = 2.0538991384858143\n",
      "-------- loss = 2.0538991384858143 --------\n",
      "-------- epoch = 771/1000 --------\n",
      "batch: 1; loss = 2.0405397006356005\n",
      "-------- loss = 2.0405397006356005 --------\n",
      "-------- epoch = 772/1000 --------\n",
      "batch: 1; loss = 2.0401390988662107\n",
      "-------- loss = 2.0401390988662107 --------\n",
      "-------- epoch = 773/1000 --------\n",
      "batch: 1; loss = 2.062133139270292\n",
      "-------- loss = 2.062133139270292 --------\n",
      "-------- epoch = 774/1000 --------\n",
      "batch: 1; loss = 2.0585058928425974\n",
      "-------- loss = 2.0585058928425974 --------\n",
      "-------- epoch = 775/1000 --------\n",
      "batch: 1; loss = 2.1002620022667142\n",
      "-------- loss = 2.1002620022667142 --------\n",
      "-------- epoch = 776/1000 --------\n",
      "batch: 1; loss = 2.0897203055868645\n",
      "-------- loss = 2.0897203055868645 --------\n",
      "-------- epoch = 777/1000 --------\n",
      "batch: 1; loss = 2.1489025795545746\n",
      "-------- loss = 2.1489025795545746 --------\n",
      "-------- epoch = 778/1000 --------\n",
      "batch: 1; loss = 2.1405259076940806\n",
      "-------- loss = 2.1405259076940806 --------\n",
      "-------- epoch = 779/1000 --------\n",
      "batch: 1; loss = 2.232437158738563\n",
      "-------- loss = 2.232437158738563 --------\n",
      "-------- epoch = 780/1000 --------\n",
      "batch: 1; loss = 2.243174815890217\n",
      "-------- loss = 2.243174815890217 --------\n",
      "-------- epoch = 781/1000 --------\n",
      "batch: 1; loss = 2.3880517337373695\n",
      "-------- loss = 2.3880517337373695 --------\n",
      "-------- epoch = 782/1000 --------\n",
      "batch: 1; loss = 2.363211120241862\n",
      "-------- loss = 2.363211120241862 --------\n",
      "-------- epoch = 783/1000 --------\n",
      "batch: 1; loss = 2.488012113470228\n",
      "-------- loss = 2.488012113470228 --------\n",
      "-------- epoch = 784/1000 --------\n",
      "batch: 1; loss = 2.3690416820524858\n",
      "-------- loss = 2.3690416820524858 --------\n",
      "-------- epoch = 785/1000 --------\n",
      "batch: 1; loss = 2.378752070555884\n",
      "-------- loss = 2.378752070555884 --------\n",
      "-------- epoch = 786/1000 --------\n",
      "batch: 1; loss = 2.2041948050292235\n",
      "-------- loss = 2.2041948050292235 --------\n",
      "-------- epoch = 787/1000 --------\n",
      "batch: 1; loss = 2.14565361345525\n",
      "-------- loss = 2.14565361345525 --------\n",
      "-------- epoch = 788/1000 --------\n",
      "batch: 1; loss = 2.0426796944935304\n",
      "-------- loss = 2.0426796944935304 --------\n",
      "-------- epoch = 789/1000 --------\n",
      "batch: 1; loss = 2.035082397641171\n",
      "-------- loss = 2.035082397641171 --------\n",
      "-------- epoch = 790/1000 --------\n",
      "batch: 1; loss = 2.0907752985925074\n",
      "-------- loss = 2.0907752985925074 --------\n",
      "-------- epoch = 791/1000 --------\n",
      "batch: 1; loss = 2.115493627993188\n",
      "-------- loss = 2.115493627993188 --------\n",
      "-------- epoch = 792/1000 --------\n",
      "batch: 1; loss = 2.2148499165779127\n",
      "-------- loss = 2.2148499165779127 --------\n",
      "-------- epoch = 793/1000 --------\n",
      "batch: 1; loss = 2.190887462763585\n",
      "-------- loss = 2.190887462763585 --------\n",
      "-------- epoch = 794/1000 --------\n",
      "batch: 1; loss = 2.2376123423401473\n",
      "-------- loss = 2.2376123423401473 --------\n",
      "-------- epoch = 795/1000 --------\n",
      "batch: 1; loss = 2.14295182338124\n",
      "-------- loss = 2.14295182338124 --------\n",
      "-------- epoch = 796/1000 --------\n",
      "batch: 1; loss = 2.125322050973175\n",
      "-------- loss = 2.125322050973175 --------\n",
      "-------- epoch = 797/1000 --------\n",
      "batch: 1; loss = 2.047599615744209\n",
      "-------- loss = 2.047599615744209 --------\n",
      "-------- epoch = 798/1000 --------\n",
      "batch: 1; loss = 2.0368000873782712\n",
      "-------- loss = 2.0368000873782712 --------\n",
      "-------- epoch = 799/1000 --------\n",
      "batch: 1; loss = 2.0486444592854\n",
      "-------- loss = 2.0486444592854 --------\n",
      "-------- epoch = 800/1000 --------\n",
      "batch: 1; loss = 2.056433343775922\n",
      "-------- loss = 2.056433343775922 --------\n",
      "-------- epoch = 801/1000 --------\n",
      "batch: 1; loss = 2.1171611953823515\n",
      "-------- loss = 2.1171611953823515 --------\n",
      "-------- epoch = 802/1000 --------\n",
      "batch: 1; loss = 2.1165438367004605\n",
      "-------- loss = 2.1165438367004605 --------\n",
      "-------- epoch = 803/1000 --------\n",
      "batch: 1; loss = 2.191037052344426\n",
      "-------- loss = 2.191037052344426 --------\n",
      "-------- epoch = 804/1000 --------\n",
      "batch: 1; loss = 2.1619580980533537\n",
      "-------- loss = 2.1619580980533537 --------\n",
      "-------- epoch = 805/1000 --------\n",
      "batch: 1; loss = 2.227420226698451\n",
      "-------- loss = 2.227420226698451 --------\n",
      "-------- epoch = 806/1000 --------\n",
      "batch: 1; loss = 2.172118329262207\n",
      "-------- loss = 2.172118329262207 --------\n",
      "-------- epoch = 807/1000 --------\n",
      "batch: 1; loss = 2.209919358187047\n",
      "-------- loss = 2.209919358187047 --------\n",
      "-------- epoch = 808/1000 --------\n",
      "batch: 1; loss = 2.1429662586794995\n",
      "-------- loss = 2.1429662586794995 --------\n",
      "-------- epoch = 809/1000 --------\n",
      "batch: 1; loss = 2.153669556785887\n",
      "-------- loss = 2.153669556785887 --------\n",
      "-------- epoch = 810/1000 --------\n",
      "batch: 1; loss = 2.0776149417062144\n",
      "-------- loss = 2.0776149417062144 --------\n",
      "-------- epoch = 811/1000 --------\n",
      "batch: 1; loss = 2.0715554411039467\n",
      "-------- loss = 2.0715554411039467 --------\n",
      "-------- epoch = 812/1000 --------\n",
      "batch: 1; loss = 2.0350402056528596\n",
      "-------- loss = 2.0350402056528596 --------\n",
      "-------- epoch = 813/1000 --------\n",
      "batch: 1; loss = 2.0349696360113243\n",
      "-------- loss = 2.0349696360113243 --------\n",
      "-------- epoch = 814/1000 --------\n",
      "batch: 1; loss = 2.0460613856131915\n",
      "-------- loss = 2.0460613856131915 --------\n",
      "-------- epoch = 815/1000 --------\n",
      "batch: 1; loss = 2.044438692175405\n",
      "-------- loss = 2.044438692175405 --------\n",
      "-------- epoch = 816/1000 --------\n",
      "batch: 1; loss = 2.0775731307612597\n",
      "-------- loss = 2.0775731307612597 --------\n",
      "-------- epoch = 817/1000 --------\n",
      "batch: 1; loss = 2.07355782969007\n",
      "-------- loss = 2.07355782969007 --------\n",
      "-------- epoch = 818/1000 --------\n",
      "batch: 1; loss = 2.128403975049609\n",
      "-------- loss = 2.128403975049609 --------\n",
      "-------- epoch = 819/1000 --------\n",
      "batch: 1; loss = 2.129010352468212\n",
      "-------- loss = 2.129010352468212 --------\n",
      "-------- epoch = 820/1000 --------\n",
      "batch: 1; loss = 2.2262273218387265\n",
      "-------- loss = 2.2262273218387265 --------\n",
      "-------- epoch = 821/1000 --------\n",
      "batch: 1; loss = 2.219026492205289\n",
      "-------- loss = 2.219026492205289 --------\n",
      "-------- epoch = 822/1000 --------\n",
      "batch: 1; loss = 2.343910370372716\n",
      "-------- loss = 2.343910370372716 --------\n",
      "-------- epoch = 823/1000 --------\n",
      "batch: 1; loss = 2.319784422160144\n",
      "-------- loss = 2.319784422160144 --------\n",
      "-------- epoch = 824/1000 --------\n",
      "batch: 1; loss = 2.4521025770951885\n",
      "-------- loss = 2.4521025770951885 --------\n",
      "-------- epoch = 825/1000 --------\n",
      "batch: 1; loss = 2.3978148320881263\n",
      "-------- loss = 2.3978148320881263 --------\n",
      "-------- epoch = 826/1000 --------\n",
      "batch: 1; loss = 2.4905675483414846\n",
      "-------- loss = 2.4905675483414846 --------\n",
      "-------- epoch = 827/1000 --------\n",
      "batch: 1; loss = 2.3370377482129796\n",
      "-------- loss = 2.3370377482129796 --------\n",
      "-------- epoch = 828/1000 --------\n",
      "batch: 1; loss = 2.3079278622406942\n",
      "-------- loss = 2.3079278622406942 --------\n",
      "-------- epoch = 829/1000 --------\n",
      "batch: 1; loss = 2.1291710419742853\n",
      "-------- loss = 2.1291710419742853 --------\n",
      "-------- epoch = 830/1000 --------\n",
      "batch: 1; loss = 2.0750918878013125\n",
      "-------- loss = 2.0750918878013125 --------\n",
      "-------- epoch = 831/1000 --------\n",
      "batch: 1; loss = 2.03295649450083\n",
      "-------- loss = 2.03295649450083 --------\n",
      "-------- epoch = 832/1000 --------\n",
      "batch: 1; loss = 2.0466724490470587\n",
      "-------- loss = 2.0466724490470587 --------\n",
      "-------- epoch = 833/1000 --------\n",
      "batch: 1; loss = 2.1295319917376854\n",
      "-------- loss = 2.1295319917376854 --------\n",
      "-------- epoch = 834/1000 --------\n",
      "batch: 1; loss = 2.1464781876186154\n",
      "-------- loss = 2.1464781876186154 --------\n",
      "-------- epoch = 835/1000 --------\n",
      "batch: 1; loss = 2.237418881323371\n",
      "-------- loss = 2.237418881323371 --------\n",
      "-------- epoch = 836/1000 --------\n",
      "batch: 1; loss = 2.182154052843798\n",
      "-------- loss = 2.182154052843798 --------\n",
      "-------- epoch = 837/1000 --------\n",
      "batch: 1; loss = 2.205114375820974\n",
      "-------- loss = 2.205114375820974 --------\n",
      "-------- epoch = 838/1000 --------\n",
      "batch: 1; loss = 2.112491102292982\n",
      "-------- loss = 2.112491102292982 --------\n",
      "-------- epoch = 839/1000 --------\n",
      "batch: 1; loss = 2.0983836394307995\n",
      "-------- loss = 2.0983836394307995 --------\n",
      "-------- epoch = 840/1000 --------\n",
      "batch: 1; loss = 2.0427477645829306\n",
      "-------- loss = 2.0427477645829306 --------\n",
      "-------- epoch = 841/1000 --------\n",
      "batch: 1; loss = 2.0378137464438844\n",
      "-------- loss = 2.0378137464438844 --------\n",
      "-------- epoch = 842/1000 --------\n",
      "batch: 1; loss = 2.048599820428083\n",
      "-------- loss = 2.048599820428083 --------\n",
      "-------- epoch = 843/1000 --------\n",
      "batch: 1; loss = 2.0509464557770625\n",
      "-------- loss = 2.0509464557770625 --------\n",
      "-------- epoch = 844/1000 --------\n",
      "batch: 1; loss = 2.0993026124453955\n",
      "-------- loss = 2.0993026124453955 --------\n",
      "-------- epoch = 845/1000 --------\n",
      "batch: 1; loss = 2.096441361561634\n",
      "-------- loss = 2.096441361561634 --------\n",
      "-------- epoch = 846/1000 --------\n",
      "batch: 1; loss = 2.156379281184489\n",
      "-------- loss = 2.156379281184489 --------\n",
      "-------- epoch = 847/1000 --------\n",
      "batch: 1; loss = 2.1312510525249104\n",
      "-------- loss = 2.1312510525249104 --------\n",
      "-------- epoch = 848/1000 --------\n",
      "batch: 1; loss = 2.1894489368103867\n",
      "-------- loss = 2.1894489368103867 --------\n",
      "-------- epoch = 849/1000 --------\n",
      "batch: 1; loss = 2.142316692998596\n",
      "-------- loss = 2.142316692998596 --------\n",
      "-------- epoch = 850/1000 --------\n",
      "batch: 1; loss = 2.186526789409117\n",
      "-------- loss = 2.186526789409117 --------\n",
      "-------- epoch = 851/1000 --------\n",
      "batch: 1; loss = 2.130034803025766\n",
      "-------- loss = 2.130034803025766 --------\n",
      "-------- epoch = 852/1000 --------\n",
      "batch: 1; loss = 2.1617224902561825\n",
      "-------- loss = 2.1617224902561825 --------\n",
      "-------- epoch = 853/1000 --------\n",
      "batch: 1; loss = 2.1054340144381123\n",
      "-------- loss = 2.1054340144381123 --------\n",
      "-------- epoch = 854/1000 --------\n",
      "batch: 1; loss = 2.1261715455458607\n",
      "-------- loss = 2.1261715455458607 --------\n",
      "-------- epoch = 855/1000 --------\n",
      "batch: 1; loss = 2.0766981185817825\n",
      "-------- loss = 2.0766981185817825 --------\n",
      "-------- epoch = 856/1000 --------\n",
      "batch: 1; loss = 2.0932473427432376\n",
      "-------- loss = 2.0932473427432376 --------\n",
      "-------- epoch = 857/1000 --------\n",
      "batch: 1; loss = 2.056174964251575\n",
      "-------- loss = 2.056174964251575 --------\n",
      "-------- epoch = 858/1000 --------\n",
      "batch: 1; loss = 2.07030331857945\n",
      "-------- loss = 2.07030331857945 --------\n",
      "-------- epoch = 859/1000 --------\n",
      "batch: 1; loss = 2.0454149801262353\n",
      "-------- loss = 2.0454149801262353 --------\n",
      "-------- epoch = 860/1000 --------\n",
      "batch: 1; loss = 2.0587720971852357\n",
      "-------- loss = 2.0587720971852357 --------\n",
      "-------- epoch = 861/1000 --------\n",
      "batch: 1; loss = 2.0428437776198267\n",
      "-------- loss = 2.0428437776198267 --------\n",
      "-------- epoch = 862/1000 --------\n",
      "batch: 1; loss = 2.0564642368385404\n",
      "-------- loss = 2.0564642368385404 --------\n",
      "-------- epoch = 863/1000 --------\n",
      "batch: 1; loss = 2.0432456263176904\n",
      "-------- loss = 2.0432456263176904 --------\n",
      "-------- epoch = 864/1000 --------\n",
      "batch: 1; loss = 2.067098530789021\n",
      "-------- loss = 2.067098530789021 --------\n",
      "-------- epoch = 865/1000 --------\n",
      "batch: 1; loss = 2.0642264702834745\n",
      "-------- loss = 2.0642264702834745 --------\n",
      "-------- epoch = 866/1000 --------\n",
      "batch: 1; loss = 2.122361363662674\n",
      "-------- loss = 2.122361363662674 --------\n",
      "-------- epoch = 867/1000 --------\n",
      "batch: 1; loss = 2.1282849724016373\n",
      "-------- loss = 2.1282849724016373 --------\n",
      "-------- epoch = 868/1000 --------\n",
      "batch: 1; loss = 2.2517476292390755\n",
      "-------- loss = 2.2517476292390755 --------\n",
      "-------- epoch = 869/1000 --------\n",
      "batch: 1; loss = 2.3120265168937544\n",
      "-------- loss = 2.3120265168937544 --------\n",
      "-------- epoch = 870/1000 --------\n",
      "batch: 1; loss = 2.608011636430551\n",
      "-------- loss = 2.608011636430551 --------\n",
      "-------- epoch = 871/1000 --------\n",
      "batch: 1; loss = 2.7585604362560203\n",
      "-------- loss = 2.7585604362560203 --------\n",
      "-------- epoch = 872/1000 --------\n",
      "batch: 1; loss = 3.2515991373505257\n",
      "-------- loss = 3.2515991373505257 --------\n",
      "-------- epoch = 873/1000 --------\n",
      "batch: 1; loss = 3.1682947975394185\n",
      "-------- loss = 3.1682947975394185 --------\n",
      "-------- epoch = 874/1000 --------\n",
      "batch: 1; loss = 3.2360387323915423\n",
      "-------- loss = 3.2360387323915423 --------\n",
      "-------- epoch = 875/1000 --------\n",
      "batch: 1; loss = 2.507200918377237\n",
      "-------- loss = 2.507200918377237 --------\n",
      "-------- epoch = 876/1000 --------\n",
      "batch: 1; loss = 2.1201532605521187\n",
      "-------- loss = 2.1201532605521187 --------\n",
      "-------- epoch = 877/1000 --------\n",
      "batch: 1; loss = 2.092023371796814\n",
      "-------- loss = 2.092023371796814 --------\n",
      "-------- epoch = 878/1000 --------\n",
      "batch: 1; loss = 2.3339026801086122\n",
      "-------- loss = 2.3339026801086122 --------\n",
      "-------- epoch = 879/1000 --------\n",
      "batch: 1; loss = 2.6338214864573413\n",
      "-------- loss = 2.6338214864573413 --------\n",
      "-------- epoch = 880/1000 --------\n",
      "batch: 1; loss = 2.3219908938602845\n",
      "-------- loss = 2.3219908938602845 --------\n",
      "-------- epoch = 881/1000 --------\n",
      "batch: 1; loss = 2.1070250382713955\n",
      "-------- loss = 2.1070250382713955 --------\n",
      "-------- epoch = 882/1000 --------\n",
      "batch: 1; loss = 2.076893198541223\n",
      "-------- loss = 2.076893198541223 --------\n",
      "-------- epoch = 883/1000 --------\n",
      "batch: 1; loss = 2.2159275633236715\n",
      "-------- loss = 2.2159275633236715 --------\n",
      "-------- epoch = 884/1000 --------\n",
      "batch: 1; loss = 2.400618849069119\n",
      "-------- loss = 2.400618849069119 --------\n",
      "-------- epoch = 885/1000 --------\n",
      "batch: 1; loss = 2.202091223154535\n",
      "-------- loss = 2.202091223154535 --------\n",
      "-------- epoch = 886/1000 --------\n",
      "batch: 1; loss = 2.0799534201956327\n",
      "-------- loss = 2.0799534201956327 --------\n",
      "-------- epoch = 887/1000 --------\n",
      "batch: 1; loss = 2.072029913712405\n",
      "-------- loss = 2.072029913712405 --------\n",
      "-------- epoch = 888/1000 --------\n",
      "batch: 1; loss = 2.149124244168804\n",
      "-------- loss = 2.149124244168804 --------\n",
      "-------- epoch = 889/1000 --------\n",
      "batch: 1; loss = 2.265376619248219\n",
      "-------- loss = 2.265376619248219 --------\n",
      "-------- epoch = 890/1000 --------\n",
      "batch: 1; loss = 2.136960916241174\n",
      "-------- loss = 2.136960916241174 --------\n",
      "-------- epoch = 891/1000 --------\n",
      "batch: 1; loss = 2.0733019211994823\n",
      "-------- loss = 2.0733019211994823 --------\n",
      "-------- epoch = 892/1000 --------\n",
      "batch: 1; loss = 2.0439522712553018\n",
      "-------- loss = 2.0439522712553018 --------\n",
      "-------- epoch = 893/1000 --------\n",
      "batch: 1; loss = 2.0812121994735606\n",
      "-------- loss = 2.0812121994735606 --------\n",
      "-------- epoch = 894/1000 --------\n",
      "batch: 1; loss = 2.1835648507514476\n",
      "-------- loss = 2.1835648507514476 --------\n",
      "-------- epoch = 895/1000 --------\n",
      "batch: 1; loss = 2.127685148763527\n",
      "-------- loss = 2.127685148763527 --------\n",
      "-------- epoch = 896/1000 --------\n",
      "batch: 1; loss = 2.1108069881372233\n",
      "-------- loss = 2.1108069881372233 --------\n",
      "-------- epoch = 897/1000 --------\n",
      "batch: 1; loss = 2.0397730319586738\n",
      "-------- loss = 2.0397730319586738 --------\n",
      "-------- epoch = 898/1000 --------\n",
      "batch: 1; loss = 2.0344725136087565\n",
      "-------- loss = 2.0344725136087565 --------\n",
      "-------- epoch = 899/1000 --------\n",
      "batch: 1; loss = 2.074866101668073\n",
      "-------- loss = 2.074866101668073 --------\n",
      "-------- epoch = 900/1000 --------\n",
      "batch: 1; loss = 2.083932549677749\n",
      "-------- loss = 2.083932549677749 --------\n",
      "-------- epoch = 901/1000 --------\n",
      "batch: 1; loss = 2.1371282375088647\n",
      "-------- loss = 2.1371282375088647 --------\n",
      "-------- epoch = 902/1000 --------\n",
      "batch: 1; loss = 2.089234220827735\n",
      "-------- loss = 2.089234220827735 --------\n",
      "-------- epoch = 903/1000 --------\n",
      "batch: 1; loss = 2.094077144700861\n",
      "-------- loss = 2.094077144700861 --------\n",
      "-------- epoch = 904/1000 --------\n",
      "batch: 1; loss = 2.0427687846430826\n",
      "-------- loss = 2.0427687846430826 --------\n",
      "-------- epoch = 905/1000 --------\n",
      "batch: 1; loss = 2.037775751411693\n",
      "-------- loss = 2.037775751411693 --------\n",
      "-------- epoch = 906/1000 --------\n",
      "batch: 1; loss = 2.0354312713720684\n",
      "-------- loss = 2.0354312713720684 --------\n",
      "-------- epoch = 907/1000 --------\n",
      "batch: 1; loss = 2.035363527684247\n",
      "-------- loss = 2.035363527684247 --------\n",
      "-------- epoch = 908/1000 --------\n",
      "batch: 1; loss = 2.0664189991896045\n",
      "-------- loss = 2.0664189991896045 --------\n",
      "-------- epoch = 909/1000 --------\n",
      "batch: 1; loss = 2.0578454023747716\n",
      "-------- loss = 2.0578454023747716 --------\n",
      "-------- epoch = 910/1000 --------\n",
      "batch: 1; loss = 2.091879478727422\n",
      "-------- loss = 2.091879478727422 --------\n",
      "-------- epoch = 911/1000 --------\n",
      "batch: 1; loss = 2.0700349483678573\n",
      "-------- loss = 2.0700349483678573 --------\n",
      "-------- epoch = 912/1000 --------\n",
      "batch: 1; loss = 2.0971123065422748\n",
      "-------- loss = 2.0971123065422748 --------\n",
      "-------- epoch = 913/1000 --------\n",
      "batch: 1; loss = 2.067569770108858\n",
      "-------- loss = 2.067569770108858 --------\n",
      "-------- epoch = 914/1000 --------\n",
      "batch: 1; loss = 2.0889420284041664\n",
      "-------- loss = 2.0889420284041664 --------\n",
      "-------- epoch = 915/1000 --------\n",
      "batch: 1; loss = 2.0600056629792682\n",
      "-------- loss = 2.0600056629792682 --------\n",
      "-------- epoch = 916/1000 --------\n",
      "batch: 1; loss = 2.0786204577116485\n",
      "-------- loss = 2.0786204577116485 --------\n",
      "-------- epoch = 917/1000 --------\n",
      "batch: 1; loss = 2.052635361959481\n",
      "-------- loss = 2.052635361959481 --------\n",
      "-------- epoch = 918/1000 --------\n",
      "batch: 1; loss = 2.0707794028552975\n",
      "-------- loss = 2.0707794028552975 --------\n",
      "-------- epoch = 919/1000 --------\n",
      "batch: 1; loss = 2.0468690264252007\n",
      "-------- loss = 2.0468690264252007 --------\n",
      "-------- epoch = 920/1000 --------\n",
      "batch: 1; loss = 2.068309209349\n",
      "-------- loss = 2.068309209349 --------\n",
      "-------- epoch = 921/1000 --------\n",
      "batch: 1; loss = 2.052834412225046\n",
      "-------- loss = 2.052834412225046 --------\n",
      "-------- epoch = 922/1000 --------\n",
      "batch: 1; loss = 2.0890431661220226\n",
      "-------- loss = 2.0890431661220226 --------\n",
      "-------- epoch = 923/1000 --------\n",
      "batch: 1; loss = 2.080952444034694\n",
      "-------- loss = 2.080952444034694 --------\n",
      "-------- epoch = 924/1000 --------\n",
      "batch: 1; loss = 2.144361832747228\n",
      "-------- loss = 2.144361832747228 --------\n",
      "-------- epoch = 925/1000 --------\n",
      "batch: 1; loss = 2.1485682822000323\n",
      "-------- loss = 2.1485682822000323 --------\n",
      "-------- epoch = 926/1000 --------\n",
      "batch: 1; loss = 2.2651198443752794\n",
      "-------- loss = 2.2651198443752794 --------\n",
      "-------- epoch = 927/1000 --------\n",
      "batch: 1; loss = 2.274671472493331\n",
      "-------- loss = 2.274671472493331 --------\n",
      "-------- epoch = 928/1000 --------\n",
      "batch: 1; loss = 2.441674584401785\n",
      "-------- loss = 2.441674584401785 --------\n",
      "-------- epoch = 929/1000 --------\n",
      "batch: 1; loss = 2.444594287641896\n",
      "-------- loss = 2.444594287641896 --------\n",
      "-------- epoch = 930/1000 --------\n",
      "batch: 1; loss = 2.613509227351964\n",
      "-------- loss = 2.613509227351964 --------\n",
      "-------- epoch = 931/1000 --------\n",
      "batch: 1; loss = 2.476635959607131\n",
      "-------- loss = 2.476635959607131 --------\n",
      "-------- epoch = 932/1000 --------\n",
      "batch: 1; loss = 2.4633741587572366\n",
      "-------- loss = 2.4633741587572366 --------\n",
      "-------- epoch = 933/1000 --------\n",
      "batch: 1; loss = 2.2029890444638625\n",
      "-------- loss = 2.2029890444638625 --------\n",
      "-------- epoch = 934/1000 --------\n",
      "batch: 1; loss = 2.0968506213211104\n",
      "-------- loss = 2.0968506213211104 --------\n",
      "-------- epoch = 935/1000 --------\n",
      "batch: 1; loss = 2.026658806302593\n",
      "-------- loss = 2.026658806302593 --------\n",
      "-------- epoch = 936/1000 --------\n",
      "batch: 1; loss = 2.0623502767964754\n",
      "-------- loss = 2.0623502767964754 --------\n",
      "-------- epoch = 937/1000 --------\n",
      "batch: 1; loss = 2.204206220079976\n",
      "-------- loss = 2.204206220079976 --------\n",
      "-------- epoch = 938/1000 --------\n",
      "batch: 1; loss = 2.206299060415786\n",
      "-------- loss = 2.206299060415786 --------\n",
      "-------- epoch = 939/1000 --------\n",
      "batch: 1; loss = 2.252299566790995\n",
      "-------- loss = 2.252299566790995 --------\n",
      "-------- epoch = 940/1000 --------\n",
      "batch: 1; loss = 2.1167582393726345\n",
      "-------- loss = 2.1167582393726345 --------\n",
      "-------- epoch = 941/1000 --------\n",
      "batch: 1; loss = 2.0738341182118005\n",
      "-------- loss = 2.0738341182118005 --------\n",
      "-------- epoch = 942/1000 --------\n",
      "batch: 1; loss = 2.027276856359755\n",
      "-------- loss = 2.027276856359755 --------\n",
      "-------- epoch = 943/1000 --------\n",
      "batch: 1; loss = 2.0411129851134504\n",
      "-------- loss = 2.0411129851134504 --------\n",
      "-------- epoch = 944/1000 --------\n",
      "batch: 1; loss = 2.122585196373041\n",
      "-------- loss = 2.122585196373041 --------\n",
      "-------- epoch = 945/1000 --------\n",
      "batch: 1; loss = 2.124708501622445\n",
      "-------- loss = 2.124708501622445 --------\n",
      "-------- epoch = 946/1000 --------\n",
      "batch: 1; loss = 2.1775283657548665\n",
      "-------- loss = 2.1775283657548665 --------\n",
      "-------- epoch = 947/1000 --------\n",
      "batch: 1; loss = 2.105164028600358\n",
      "-------- loss = 2.105164028600358 --------\n",
      "-------- epoch = 948/1000 --------\n",
      "batch: 1; loss = 2.0936841149453698\n",
      "-------- loss = 2.0936841149453698 --------\n",
      "-------- epoch = 949/1000 --------\n",
      "batch: 1; loss = 2.0335505847256417\n",
      "-------- loss = 2.0335505847256417 --------\n",
      "-------- epoch = 950/1000 --------\n",
      "batch: 1; loss = 2.026308652710273\n",
      "-------- loss = 2.026308652710273 --------\n",
      "-------- epoch = 951/1000 --------\n",
      "batch: 1; loss = 2.0452991387536508\n",
      "-------- loss = 2.0452991387536508 --------\n",
      "-------- epoch = 952/1000 --------\n",
      "batch: 1; loss = 2.047864356624791\n",
      "-------- loss = 2.047864356624791 --------\n",
      "-------- epoch = 953/1000 --------\n",
      "batch: 1; loss = 2.09428542403608\n",
      "-------- loss = 2.09428542403608 --------\n",
      "-------- epoch = 954/1000 --------\n",
      "batch: 1; loss = 2.0818448646667753\n",
      "-------- loss = 2.0818448646667753 --------\n",
      "-------- epoch = 955/1000 --------\n",
      "batch: 1; loss = 2.122060331707516\n",
      "-------- loss = 2.122060331707516 --------\n",
      "-------- epoch = 956/1000 --------\n",
      "batch: 1; loss = 2.083776417462014\n",
      "-------- loss = 2.083776417462014 --------\n",
      "-------- epoch = 957/1000 --------\n",
      "batch: 1; loss = 2.106176757367761\n",
      "-------- loss = 2.106176757367761 --------\n",
      "-------- epoch = 958/1000 --------\n",
      "batch: 1; loss = 2.058869328946582\n",
      "-------- loss = 2.058869328946582 --------\n",
      "-------- epoch = 959/1000 --------\n",
      "batch: 1; loss = 2.069061181709158\n",
      "-------- loss = 2.069061181709158 --------\n",
      "-------- epoch = 960/1000 --------\n",
      "batch: 1; loss = 2.0394389281819385\n",
      "-------- loss = 2.0394389281819385 --------\n",
      "-------- epoch = 961/1000 --------\n",
      "batch: 1; loss = 2.047824441403597\n",
      "-------- loss = 2.047824441403597 --------\n",
      "-------- epoch = 962/1000 --------\n",
      "batch: 1; loss = 2.030451094445648\n",
      "-------- loss = 2.030451094445648 --------\n",
      "-------- epoch = 963/1000 --------\n",
      "batch: 1; loss = 2.033572162875225\n",
      "-------- loss = 2.033572162875225 --------\n",
      "-------- epoch = 964/1000 --------\n",
      "batch: 1; loss = 2.026332453596608\n",
      "-------- loss = 2.026332453596608 --------\n",
      "-------- epoch = 965/1000 --------\n",
      "batch: 1; loss = 2.0284117189004323\n",
      "-------- loss = 2.0284117189004323 --------\n",
      "-------- epoch = 966/1000 --------\n",
      "batch: 1; loss = 2.0332932381585382\n",
      "-------- loss = 2.0332932381585382 --------\n",
      "-------- epoch = 967/1000 --------\n",
      "batch: 1; loss = 2.032112330931856\n",
      "-------- loss = 2.032112330931856 --------\n",
      "-------- epoch = 968/1000 --------\n",
      "batch: 1; loss = 2.039501080049273\n",
      "-------- loss = 2.039501080049273 --------\n",
      "-------- epoch = 969/1000 --------\n",
      "batch: 1; loss = 2.0314415167870084\n",
      "-------- loss = 2.0314415167870084 --------\n",
      "-------- epoch = 970/1000 --------\n",
      "batch: 1; loss = 2.040823951038214\n",
      "-------- loss = 2.040823951038214 --------\n",
      "-------- epoch = 971/1000 --------\n",
      "batch: 1; loss = 2.033027421726504\n",
      "-------- loss = 2.033027421726504 --------\n",
      "-------- epoch = 972/1000 --------\n",
      "batch: 1; loss = 2.055591121968046\n",
      "-------- loss = 2.055591121968046 --------\n",
      "-------- epoch = 973/1000 --------\n",
      "batch: 1; loss = 2.0539988840352317\n",
      "-------- loss = 2.0539988840352317 --------\n",
      "-------- epoch = 974/1000 --------\n",
      "batch: 1; loss = 2.1161406119373547\n",
      "-------- loss = 2.1161406119373547 --------\n",
      "-------- epoch = 975/1000 --------\n",
      "batch: 1; loss = 2.12936764391529\n",
      "-------- loss = 2.12936764391529 --------\n",
      "-------- epoch = 976/1000 --------\n",
      "batch: 1; loss = 2.2776768020860785\n",
      "-------- loss = 2.2776768020860785 --------\n",
      "-------- epoch = 977/1000 --------\n",
      "batch: 1; loss = 2.373711047941621\n",
      "-------- loss = 2.373711047941621 --------\n",
      "-------- epoch = 978/1000 --------\n",
      "batch: 1; loss = 2.7619424036132734\n",
      "-------- loss = 2.7619424036132734 --------\n",
      "-------- epoch = 979/1000 --------\n",
      "batch: 1; loss = 2.994424047129487\n",
      "-------- loss = 2.994424047129487 --------\n",
      "-------- epoch = 980/1000 --------\n",
      "batch: 1; loss = 3.64341484697063\n",
      "-------- loss = 3.64341484697063 --------\n",
      "-------- epoch = 981/1000 --------\n",
      "batch: 1; loss = 3.449019479920158\n",
      "-------- loss = 3.449019479920158 --------\n",
      "-------- epoch = 982/1000 --------\n",
      "batch: 1; loss = 3.333154042978499\n",
      "-------- loss = 3.333154042978499 --------\n",
      "-------- epoch = 983/1000 --------\n",
      "batch: 1; loss = 2.3565604556711377\n",
      "-------- loss = 2.3565604556711377 --------\n",
      "-------- epoch = 984/1000 --------\n",
      "batch: 1; loss = 2.020954799010178\n",
      "-------- loss = 2.020954799010178 --------\n",
      "-------- epoch = 985/1000 --------\n",
      "batch: 1; loss = 2.364392092586264\n",
      "-------- loss = 2.364392092586264 --------\n",
      "-------- epoch = 986/1000 --------\n",
      "batch: 1; loss = 2.6061442236905203\n",
      "-------- loss = 2.6061442236905203 --------\n",
      "-------- epoch = 987/1000 --------\n",
      "batch: 1; loss = 2.614612117395667\n",
      "-------- loss = 2.614612117395667 --------\n",
      "-------- epoch = 988/1000 --------\n",
      "batch: 1; loss = 2.097303516876431\n",
      "-------- loss = 2.097303516876431 --------\n",
      "-------- epoch = 989/1000 --------\n",
      "batch: 1; loss = 2.0729969079950075\n",
      "-------- loss = 2.0729969079950075 --------\n",
      "-------- epoch = 990/1000 --------\n",
      "batch: 1; loss = 2.451263745240151\n",
      "-------- loss = 2.451263745240151 --------\n",
      "-------- epoch = 991/1000 --------\n",
      "batch: 1; loss = 2.3896349815148334\n",
      "-------- loss = 2.3896349815148334 --------\n",
      "-------- epoch = 992/1000 --------\n",
      "batch: 1; loss = 2.206461220181198\n",
      "-------- loss = 2.206461220181198 --------\n",
      "-------- epoch = 993/1000 --------\n",
      "batch: 1; loss = 2.043806739153655\n",
      "-------- loss = 2.043806739153655 --------\n",
      "-------- epoch = 994/1000 --------\n",
      "batch: 1; loss = 2.164747988546931\n",
      "-------- loss = 2.164747988546931 --------\n",
      "-------- epoch = 995/1000 --------\n",
      "batch: 1; loss = 2.3610388339577195\n",
      "-------- loss = 2.3610388339577195 --------\n",
      "-------- epoch = 996/1000 --------\n",
      "batch: 1; loss = 2.164116253148029\n",
      "-------- loss = 2.164116253148029 --------\n",
      "-------- epoch = 997/1000 --------\n",
      "batch: 1; loss = 2.0467094323335004\n",
      "-------- loss = 2.0467094323335004 --------\n",
      "-------- epoch = 998/1000 --------\n",
      "batch: 1; loss = 2.0990788383495875\n",
      "-------- loss = 2.0990788383495875 --------\n",
      "-------- epoch = 999/1000 --------\n",
      "batch: 1; loss = 2.1741988871290254\n",
      "-------- loss = 2.1741988871290254 --------\n",
      "-------- epoch = 1000/1000 --------\n",
      "batch: 1; loss = 2.229268184995443\n",
      "-------- loss = 2.229268184995443 --------\n"
     ]
    }
   ],
   "source": [
    "dl = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.01, weight_decay=0.1)\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    loss_sum = 0\n",
    "    print(f'-------- epoch = {epoch+1}/{epochs} --------')\n",
    "    for batch, (X, Y) in enumerate(dl):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X.to(device))\n",
    "        loss = loss_fn(pred, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 1 == 0:\n",
    "            loss, current = loss.item(), (batch + 1)*len(X)\n",
    "            loss_sum += loss\n",
    "            print(f'batch: {batch+1}; loss = {loss}')\n",
    "    print(f'-------- loss = {loss_sum/(int(len(data)/batch_size))} --------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c4a0a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = DataPreprocessing(x_train, y_train, x_test, y_test, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f3189ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7663],\n",
       "        [-0.6371],\n",
       "        [-0.2041],\n",
       "        [ 0.4036],\n",
       "        [-2.1327]], dtype=torch.float64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87f39e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_test = torch.utils.data.DataLoader(data_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4874d16-02ab-4193-996f-6f3baa796772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_error_reg_nn(model, data):\n",
    "    dl = torch.utils.data.DataLoader(data, batch_size=len(data))\n",
    "    for x, y in dl:\n",
    "        for i in range(len(df.columns[5:37])):\n",
    "            preds = model(x)[:, i].detach().numpy()\n",
    "            true = y[:, i].numpy()\n",
    "            answrs = mean_absolute_error(true, preds)\n",
    "            print(df.columns[i+5], ':', (answrs/np.mean(true))*100, '%')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba9e1ac4-8154-4f34-abcc-f660a2234c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B : 704.86767018488 %\n",
      "C : 66.02890566000183 %\n",
      "N : 120.37515999822841 %\n",
      "O : 170.8055951084988 %\n",
      "Mg : 241.83249744885495 %\n",
      "Al : 122.34067387299558 %\n",
      "Si : 76.32236106093936 %\n",
      "P : 69.93244289633648 %\n",
      "S : 81.54778389868761 %\n",
      "K : 426.4694223831471 %\n",
      "Ti : 138.7195632586058 %\n",
      "V : 156.64186897209672 %\n",
      "Cr : 69.64338666688913 %\n",
      "Mn : 54.17111272488258 %\n",
      "Fe : 2.8655552356793823 %\n",
      "Co : 160.27919535775303 %\n",
      "Ni : 70.82453133162375 %\n",
      "Cu : 86.44248693065667 %\n",
      "As : 201.74741597634252 %\n",
      "Se : 652.1272289502051 %\n",
      "Mo : 53.85011525789073 %\n",
      "Nb : 128.3310854514689 %\n",
      "Zr : 1769.0387705136309 %\n",
      "Ce : 290.3549146439024 %\n",
      "W : 133.27486639936458 %\n",
      "Pb : 137.04478538703052 %\n",
      "Sb : 1019.8533745194208 %\n",
      "Bi : 3772.5965445323864 %\n",
      "Cd : 55062.595650052834 %\n",
      "Te : 681.436404180095 %\n",
      "Sn : 266.9254931252574 %\n",
      "Ta : 172.0872135061738 %\n"
     ]
    }
   ],
   "source": [
    "show_error_reg_nn(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "657bd36b-30cc-4709-a9e4-3468d89ab0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B : 110.7676979179494 %\n",
      "C : 93.9697131593504 %\n",
      "N : 137.22382399306866 %\n",
      "O : 95.64005689577931 %\n",
      "Mg : 131.67306491215152 %\n",
      "Al : 111.13566440569483 %\n",
      "Si : 77.82603873340697 %\n",
      "P : 67.96327394886808 %\n",
      "S : 107.20523060177129 %\n",
      "K : 176.20497874488152 %\n",
      "Ti : 184.93576061813948 %\n",
      "V : 136.0113349768299 %\n",
      "Cr : 43.034225993771116 %\n",
      "Mn : 54.49521698411519 %\n",
      "Fe : 4.671571305447276 %\n",
      "Co : 138.56909968749324 %\n",
      "Ni : 72.12910920124416 %\n",
      "Cu : 43.03708231717657 %\n",
      "As : 128.03532444914262 %\n",
      "Se : 341.76458207803546 %\n",
      "Mo : 73.86126276259826 %\n",
      "Nb : 136.83314123763165 %\n",
      "Zr : 118.79188745448899 %\n",
      "Ce : 179.7318892522582 %\n",
      "W : 93.77192415034405 %\n",
      "Pb : 111.6589405647358 %\n",
      "Sb : 349.97166593836806 %\n",
      "Bi : 1083.7317027263787 %\n",
      "Cd : 14788.031946802084 %\n",
      "Te : 256.4008269674984 %\n",
      "Sn : 145.7505539688833 %\n",
      "Ta : 168.4510793357547 %\n"
     ]
    }
   ],
   "source": [
    "show_error_reg_nn(model, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b3f8d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.69 % :  B\n",
      "0.18 % :  C\n",
      "1.08 % :  N\n",
      "0.89 % :  O\n",
      "3.61 % :  Mg\n",
      "0.23 % :  Al\n",
      "0.1 % :  Si\n",
      "1.12 % :  P\n",
      "0.93 % :  S\n",
      "13.86 % :  K\n",
      "0.45 % :  Ti\n",
      "0.19 % :  V\n",
      "0.01 % :  Cr\n",
      "0.07 % :  Mn\n",
      "0.0 % :  Fe\n",
      "0.04 % :  Co\n",
      "0.02 % :  Ni\n",
      "0.12 % :  Cu\n",
      "7.47 % :  As\n",
      "5.39 % :  Se\n",
      "0.06 % :  Mo\n",
      "0.41 % :  Nb\n",
      "15.69 % :  Zr\n",
      "0.38 % :  Ce\n",
      "0.11 % :  W\n",
      "3.63 % :  Pb\n",
      "8.41 % :  Sb\n",
      "75.8 % :  Bi\n",
      "33.83 % :  Cd\n",
      "7.47 % :  Te\n",
      "0.31 % :  Sn\n",
      "1.13 % :  Ta\n"
     ]
    }
   ],
   "source": [
    "showing_error(bst, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5f11578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.42 % :  B\n",
      "51.45 % :  C\n",
      "94.38 % :  N\n",
      "100.09 % :  O\n",
      "100.4 % :  Mg\n",
      "108.29 % :  Al\n",
      "30.55 % :  Si\n",
      "32.32 % :  P\n",
      "119.01 % :  S\n",
      "126.38 % :  K\n",
      "117.58 % :  Ti\n",
      "94.35 % :  V\n",
      "43.72 % :  Cr\n",
      "40.94 % :  Mn\n",
      "4.59 % :  Fe\n",
      "243.17 % :  Co\n",
      "71.86 % :  Ni\n",
      "33.52 % :  Cu\n",
      "100.87 % :  As\n",
      "82.0 % :  Se\n",
      "38.09 % :  Mo\n",
      "125.44 % :  Nb\n",
      "102.69 % :  Zr\n",
      "96.48 % :  Ce\n",
      "131.43 % :  W\n",
      "139.94 % :  Pb\n",
      "100.98 % :  Sb\n",
      "108.87 % :  Bi\n",
      "103.96 % :  Cd\n",
      "100.87 % :  Te\n",
      "113.59 % :  Sn\n",
      "312.47 % :  Ta\n"
     ]
    }
   ],
   "source": [
    "showing_error(bst, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a5cce4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.22 % :  B\n",
      "19.67 % :  C\n",
      "54.01 % :  N\n",
      "64.21 % :  O\n",
      "35.0 % :  Mg\n",
      "38.3 % :  Al\n",
      "34.98 % :  Si\n",
      "18.43 % :  P\n",
      "29.22 % :  S\n",
      "85.0 % :  K\n",
      "44.65 % :  Ti\n",
      "54.21 % :  V\n",
      "35.7 % :  Cr\n",
      "23.57 % :  Mn\n",
      "1.66 % :  Fe\n",
      "56.61 % :  Co\n",
      "31.91 % :  Ni\n",
      "30.08 % :  Cu\n",
      "35.0 % :  As\n",
      "69.15 % :  Se\n",
      "26.25 % :  Mo\n",
      "55.17 % :  Nb\n",
      "47.3 % :  Zr\n",
      "77.3 % :  Ce\n",
      "72.36 % :  W\n",
      "64.88 % :  Pb\n",
      "35.0 % :  Sb\n",
      "35.0 % :  Bi\n",
      "35.0 % :  Cd\n",
      "35.0 % :  Te\n",
      "67.77 % :  Sn\n",
      "53.0 % :  Ta\n"
     ]
    }
   ],
   "source": [
    "showing_error(rnd_reg, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "521db1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.24 % :  B\n",
      "55.99 % :  C\n",
      "54.39 % :  N\n",
      "105.66 % :  O\n",
      "66.94 % :  Mg\n",
      "90.9 % :  Al\n",
      "55.05 % :  Si\n",
      "41.19 % :  P\n",
      "73.69 % :  S\n",
      "105.0 % :  K\n",
      "110.67 % :  Ti\n",
      "55.27 % :  V\n",
      "64.67 % :  Cr\n",
      "41.46 % :  Mn\n",
      "3.88 % :  Fe\n",
      "128.22 % :  Co\n",
      "77.95 % :  Ni\n",
      "68.09 % :  Cu\n",
      "65.0 % :  As\n",
      "92.88 % :  Se\n",
      "52.92 % :  Mo\n",
      "70.6 % :  Nb\n",
      "100.24 % :  Zr\n",
      "85.98 % :  Ce\n",
      "97.14 % :  W\n",
      "112.28 % :  Pb\n",
      "65.0 % :  Sb\n",
      "65.0 % :  Bi\n",
      "65.0 % :  Cd\n",
      "65.0 % :  Te\n",
      "92.4 % :  Sn\n",
      "128.19 % :  Ta\n"
     ]
    }
   ],
   "source": [
    "showing_error(rnd_reg, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d80cf952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rnd_reg_model.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rnd_reg, 'rnd_reg_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83768f5b-a921-4866-9096-a6b386ed85a8",
   "metadata": {},
   "source": [
    "## Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed4c6171-7bfe-428b-9643-42c0785c3d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg = joblib.load('RandomForest_regression.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27ed2c50-9c90-4322-ba54-fc44f2636412",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cls = joblib.load('rnd_clf_THIS_MODEL.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "674e3020-afee-4579-89d2-ac1f252e2298",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model = tf.keras.models.load_model('NN_classifier.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e5c29d8f-bea4-4b5c-82c6-b79bafeab098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(map(lambda x: 1 if x>=0.3 else 0, list(clf_model.predict(nn_test[7].reshape(1, -1))[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "de5d6fee-cb50-417a-a204-cee3be8c6d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_class.values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e68702de-c46b-4b57-9610-ebb729ce6f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf.predict(nn_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53e323a1-9689-4c4c-8ea0-6d11fb85199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Showing(Result):\n",
    "    def __init__(self, model_reg, model_class, x, y_reg, y_class):\n",
    "        super().__init__(model=model_reg, x=x, y_reg=y_reg, y_class=y_class, typee='test')\n",
    "        self.model_class = model_class\n",
    "        self.nn_inp = np.hstack([x, model_reg.predict(x)])\n",
    "\n",
    "    def show_result(self, i):\n",
    "        result = []\n",
    "        for j in range(self.model_class.predict(self.nn_inp).shape[1]):\n",
    "            if self.model_class.predict(self.nn_inp)[i, j] >= 0.3:\n",
    "                result.append(self.model.predict(self.x)[i, j])\n",
    "            elif self.model_class.predict(self.nn_inp)[i, j] < 0.3:\n",
    "                result.append(0)\n",
    "        return self.y_reg.values[i], np.array(result)\n",
    "\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        cols = df.columns[5:37]\n",
    "        res_dict = {}\n",
    "        true_dict = {}\n",
    "        trueth, pred = self.show_result(k)\n",
    "        for j in range(len(cols)):\n",
    "            res_dict[cols[j]] = pred[j]\n",
    "        for j in range(len(cols)):\n",
    "            true_dict[cols[j]] = trueth[j]\n",
    "        interpretation = pd.DataFrame(res_dict, index=['predicted'])\n",
    "        inter_true = pd.DataFrame(true_dict, index=['true'])\n",
    "        summ = 0\n",
    "        true_sum = 0\n",
    "        for name_col in interpretation.columns:\n",
    "            summ += abs(interpretation[name_col].values[0] - inter_true[name_col].values[0])\n",
    "            true_sum += inter_true[name_col].values[0]\n",
    "        interpretation_of_true = interpretation.loc[:, (interpretation>0).all()]\n",
    "        int\n",
    "        return inter_true.loc[:, (inter_true>0).all()], interpretation_of_true, (summ/(true_sum/len(interpretation_of_true.columns)))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "026ebc65-0c2a-4e6e-b12c-b475758586c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = Showing(model_reg, model_cls, x_test, y_test_reg, y_test_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a8e3d4cc-6dfa-4ab2-9e1d-59090150aee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(          C     Si     P       S     V    Cr     Mn    Fe   Co     Ni    Cu  \\\n",
       " true  0.446  0.769  0.03  0.0178  0.89  4.44  0.454  89.6  3.5  0.703  0.25   \n",
       " \n",
       "         Mo     W  \n",
       " true  1.94  7.55  ,\n",
       "                   B         C         N        Al        Si         P  \\\n",
       " predicted  0.000164  0.782253  0.007445  0.374675  0.625775  0.019125   \n",
       " \n",
       "                   S        Ti         V        Cr        Mn         Fe  \\\n",
       " predicted  0.064989  0.032175  1.539125  5.865125  0.810088  89.625125   \n",
       " \n",
       "                Ni     Cu        Mo  \n",
       " predicted  1.5598  0.474  1.265463  ,\n",
       " 219.90454815905252)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6cf029-4da4-4489-898b-4154863a7500",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f6fe433f-142f-42b6-b0d4-3c0911b0bd7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_save = pd.DataFrame()\n",
    "#pd.concat([df1, df3], sort=False)\n",
    "\n",
    "for i in range(len(inter)):\n",
    "    df_save = pd.concat([df_save, inter[i][0]], sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "956805ba-90a1-409b-8f96-bdeff3520191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_save_pred = pd.DataFrame()\n",
    "#pd.concat([df1, df3], sort=False)\n",
    "\n",
    "for i in range(len(inter)):\n",
    "    df_save_pred = pd.concat([df_save_pred, inter[i][1]], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4066cc61-b061-4057-b6ef-6797e8ff1d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_save = df_save.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2d6bbead-99f5-4b1a-8feb-a079d70666f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_save_pred = df_save_pred.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e4876d7d-2383-471c-a463-2c8f620ffe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('true_pred.xlsx') as writer:  \n",
    "    df_save.to_excel(writer, sheet_name='Sheet_name_1')\n",
    "    df_save_pred.to_excel(writer, sheet_name='Sheet_name_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ab525f66-cb3d-449e-89e4-1ad279ecdc6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('errors.txt', 'w') as file:\n",
    "    text_arr = []\n",
    "    for i in range(len(inter)):\n",
    "        text_arr.append(str(inter[i][2]))\n",
    "    text = '; '.join(text_arr)\n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20687ea0-11f0-42ec-a3cb-d67530e3616a",
   "metadata": {},
   "source": [
    "## Save only truth values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df29d49e-1afa-49cb-9dec-81f8d725ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowingOnlyTruth(Result):\n",
    "    def __init__(self, model_reg, x, y_reg, y_class):\n",
    "        super().__init__(model=model_reg, x=x, y_reg=y_reg, y_class=y_class, typee='test')\n",
    "        self.nn_inp = np.hstack([x, model_reg.predict(x)])\n",
    "\n",
    "    def show_result(self, i):\n",
    "        result = []\n",
    "        for j in range(self.y_class.values.shape[1]):\n",
    "            if self.y_class.values[i, j] >= 0.3:\n",
    "                result.append(self.model.predict(self.x)[i, j])\n",
    "            elif self.y_class.values[i, j] < 0.3:\n",
    "                result.append(0)\n",
    "        return self.y_reg.values[i], np.array(result)\n",
    "\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        cols = df.columns[5:37]\n",
    "        res_dict = {}\n",
    "        true_dict = {}\n",
    "        trueth, pred = self.show_result(k)\n",
    "        for j in range(len(cols)):\n",
    "            true_dict[cols[j]] = trueth[j]\n",
    "        for j in range(len(cols)):\n",
    "            res_dict[cols[j]] = pred[j]\n",
    "        interpretation = pd.DataFrame(res_dict, index=['predicted'])\n",
    "        inter_true = pd.DataFrame(true_dict, index=['true'])\n",
    "        inter_true_1 = inter_true.loc[:, (inter_true>0).all()]\n",
    "        summ = 0\n",
    "        true_sum = 0\n",
    "        cols_of_true = inter_true_1.columns\n",
    "        inter_pred = interpretation[cols_of_true]\n",
    "        for name_col in inter_true_1.columns:\n",
    "            summ += abs(inter_pred[name_col].values[0] - inter_true[name_col].values[0])\n",
    "            true_sum += inter_true[name_col].values[0]\n",
    "        return inter_true_1, inter_pred, (summ/(true_sum/len(inter_pred.columns)))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdda8576-e1ac-41e9-92bb-602552f4a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_truth = ShowingOnlyTruth(model_reg, x_test, y_test_reg, y_test_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "07daed39-75af-4dc2-8deb-20692cb2aeff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_save_only = pd.DataFrame()\n",
    "#pd.concat([df1, df3], sort=False)\n",
    "\n",
    "for i in range(len(inter_truth)):\n",
    "    df_save_only = pd.concat([df_save_only, inter_truth[i][0]], sort=False)\n",
    "\n",
    "\n",
    "df_save_pred_only = pd.DataFrame()\n",
    "#pd.concat([df1, df3], sort=False)\n",
    "\n",
    "for i in range(len(inter_truth)):\n",
    "    df_save_pred_only = pd.concat([df_save_pred_only, inter_truth[i][1]], sort=False)\n",
    "\n",
    "\n",
    "with pd.ExcelWriter('true_pred_only.xlsx') as writer:  \n",
    "    df_save_only.to_excel(writer, sheet_name='Sheet_name_1')\n",
    "    df_save_pred_only.to_excel(writer, sheet_name='Sheet_name_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed16bedf-d294-46c7-b024-6918a86707ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('errors_only.txt', 'w') as file:\n",
    "    text_arr = []\n",
    "    for i in range(len(inter_truth)):\n",
    "        text_arr.append(str(inter_truth[i][2]))\n",
    "    text = '; '.join(text_arr)\n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d080e535-5c7b-4d34-9621-83fc2d4d5ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
