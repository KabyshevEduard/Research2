{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab445516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "631a5638",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06b62e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "df = pd.read_excel(f'{path}/data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f22d3af3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HB</th>\n",
       "      <th>Ultimate strength, Gpa</th>\n",
       "      <th>Youngs modulus, Gpa</th>\n",
       "      <th>Density, g/cc</th>\n",
       "      <th>Specific Heat of capacity, J/g*C</th>\n",
       "      <th>H</th>\n",
       "      <th>Li</th>\n",
       "      <th>Be</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>...</th>\n",
       "      <th>Nd</th>\n",
       "      <th>Th</th>\n",
       "      <th>Y</th>\n",
       "      <th>Hf</th>\n",
       "      <th>Au</th>\n",
       "      <th>Pd</th>\n",
       "      <th>Pt</th>\n",
       "      <th>Ru</th>\n",
       "      <th>Ir</th>\n",
       "      <th>Rh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112.0</td>\n",
       "      <td>0.380</td>\n",
       "      <td>203.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128.0</td>\n",
       "      <td>0.480</td>\n",
       "      <td>193.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110.0</td>\n",
       "      <td>0.410</td>\n",
       "      <td>220.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>248.0</td>\n",
       "      <td>0.900</td>\n",
       "      <td>190.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139.0</td>\n",
       "      <td>0.205</td>\n",
       "      <td>193.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>212.0</td>\n",
       "      <td>0.725</td>\n",
       "      <td>205.0</td>\n",
       "      <td>7.85</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>217.0</td>\n",
       "      <td>0.745</td>\n",
       "      <td>205.0</td>\n",
       "      <td>7.85</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.48</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>163.0</td>\n",
       "      <td>0.515</td>\n",
       "      <td>200.0</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>167.0</td>\n",
       "      <td>0.540</td>\n",
       "      <td>200.0</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>163.0</td>\n",
       "      <td>0.572</td>\n",
       "      <td>200.0</td>\n",
       "      <td>7.85</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       HB  Ultimate strength, Gpa  Youngs modulus, Gpa  Density, g/cc  \\\n",
       "0   112.0                   0.380                203.0            NaN   \n",
       "1   128.0                   0.480                193.0            NaN   \n",
       "2   110.0                   0.410                220.0            NaN   \n",
       "3   248.0                   0.900                190.0            NaN   \n",
       "4   139.0                   0.205                193.0            NaN   \n",
       "..    ...                     ...                  ...            ...   \n",
       "93  212.0                   0.725                205.0           7.85   \n",
       "94  217.0                   0.745                205.0           7.85   \n",
       "95  163.0                   0.515                200.0           7.87   \n",
       "96  167.0                   0.540                200.0           7.87   \n",
       "97  163.0                   0.572                200.0           7.85   \n",
       "\n",
       "    Specific Heat of capacity, J/g*C    H   Li   Be     B     C  ...   Nd  \\\n",
       "0                                NaN  0.0  0.0  0.0  0.00  0.14  ...  0.0   \n",
       "1                                NaN  0.0  0.0  0.0  0.00  0.03  ...  0.0   \n",
       "2                                NaN  0.0  0.0  0.0  0.00  0.08  ...  0.0   \n",
       "3                                NaN  0.0  0.0  0.0  0.01  0.08  ...  0.0   \n",
       "4                                NaN  0.0  0.0  0.0  0.00  0.08  ...  0.0   \n",
       "..                               ...  ...  ...  ...   ...   ...  ...  ...   \n",
       "93                             0.472  0.0  0.0  0.0  0.00  0.45  ...  0.0   \n",
       "94                             0.472  0.0  0.0  0.0  0.00  0.48  ...  0.0   \n",
       "95                             0.472  0.0  0.0  0.0  0.00  0.13  ...  0.0   \n",
       "96                             0.472  0.0  0.0  0.0  0.00  0.13  ...  0.0   \n",
       "97                             0.470  0.0  0.0  0.0  0.00  0.29  ...  0.0   \n",
       "\n",
       "     Th    Y   Hf   Au   Pd   Pt   Ru   Ir   Rh  \n",
       "0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "..  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "93  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "94  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "95  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "96  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "97  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[98 rows x 64 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "999f718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6649c9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HB</th>\n",
       "      <th>Ultimate strength, Gpa</th>\n",
       "      <th>Youngs modulus, Gpa</th>\n",
       "      <th>Density, g/cc</th>\n",
       "      <th>Specific Heat of capacity, J/g*C</th>\n",
       "      <th>H</th>\n",
       "      <th>Li</th>\n",
       "      <th>Be</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>...</th>\n",
       "      <th>Nd</th>\n",
       "      <th>Th</th>\n",
       "      <th>Y</th>\n",
       "      <th>Hf</th>\n",
       "      <th>Au</th>\n",
       "      <th>Pd</th>\n",
       "      <th>Pt</th>\n",
       "      <th>Ru</th>\n",
       "      <th>Ir</th>\n",
       "      <th>Rh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0.040</td>\n",
       "      <td>23.4</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.6315</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>146.0</td>\n",
       "      <td>0.540</td>\n",
       "      <td>200.0</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.4400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>578.0</td>\n",
       "      <td>1.970</td>\n",
       "      <td>213.0</td>\n",
       "      <td>7.84</td>\n",
       "      <td>0.4770</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>400.0</td>\n",
       "      <td>2.300</td>\n",
       "      <td>210.0</td>\n",
       "      <td>7.84</td>\n",
       "      <td>0.5190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>1.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>363.0</td>\n",
       "      <td>1.240</td>\n",
       "      <td>207.0</td>\n",
       "      <td>7.85</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.54</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>212.0</td>\n",
       "      <td>0.725</td>\n",
       "      <td>205.0</td>\n",
       "      <td>7.85</td>\n",
       "      <td>0.4720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>217.0</td>\n",
       "      <td>0.745</td>\n",
       "      <td>205.0</td>\n",
       "      <td>7.85</td>\n",
       "      <td>0.4720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.48</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>163.0</td>\n",
       "      <td>0.515</td>\n",
       "      <td>200.0</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.4720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>167.0</td>\n",
       "      <td>0.540</td>\n",
       "      <td>200.0</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.4720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>163.0</td>\n",
       "      <td>0.572</td>\n",
       "      <td>200.0</td>\n",
       "      <td>7.85</td>\n",
       "      <td>0.4700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       HB  Ultimate strength, Gpa  Youngs modulus, Gpa  Density, g/cc  \\\n",
       "6    17.0                   0.040                 23.4           1.54   \n",
       "7   146.0                   0.540                200.0           7.87   \n",
       "8   578.0                   1.970                213.0           7.84   \n",
       "9   400.0                   2.300                210.0           7.84   \n",
       "10  363.0                   1.240                207.0           7.85   \n",
       "..    ...                     ...                  ...            ...   \n",
       "93  212.0                   0.725                205.0           7.85   \n",
       "94  217.0                   0.745                205.0           7.85   \n",
       "95  163.0                   0.515                200.0           7.87   \n",
       "96  167.0                   0.540                200.0           7.87   \n",
       "97  163.0                   0.572                200.0           7.85   \n",
       "\n",
       "    Specific Heat of capacity, J/g*C    H   Li   Be      B     C  ...   Nd  \\\n",
       "6                             0.6315  0.0  0.0  0.0  0.000  0.00  ...  0.0   \n",
       "7                             0.4400  0.0  0.0  0.0  0.000  0.00  ...  0.0   \n",
       "8                             0.4770  0.0  0.0  0.0  0.000  1.29  ...  0.0   \n",
       "9                             0.5190  0.0  0.0  0.0  0.003  1.10  ...  0.0   \n",
       "10                            0.4750  0.0  0.0  0.0  0.000  0.54  ...  0.0   \n",
       "..                               ...  ...  ...  ...    ...   ...  ...  ...   \n",
       "93                            0.4720  0.0  0.0  0.0  0.000  0.45  ...  0.0   \n",
       "94                            0.4720  0.0  0.0  0.0  0.000  0.48  ...  0.0   \n",
       "95                            0.4720  0.0  0.0  0.0  0.000  0.13  ...  0.0   \n",
       "96                            0.4720  0.0  0.0  0.0  0.000  0.13  ...  0.0   \n",
       "97                            0.4700  0.0  0.0  0.0  0.000  0.29  ...  0.0   \n",
       "\n",
       "     Th    Y   Hf   Au   Pd   Pt   Ru   Ir   Rh  \n",
       "6   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "7   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "8   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "10  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "..  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "93  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "94  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "95  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "96  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "97  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[87 rows x 64 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c00186c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_dim = 5\n",
    "output_dim = len(df.columns) - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a2f8304",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.lin1 = torch.nn.Linear(input_dim, 10, dtype=torch.float64)\n",
    "        self.lin2 = torch.nn.Linear(10, 50, dtype=torch.float64)\n",
    "        self.lin3 = torch.nn.Linear(50, 100, dtype=torch.float64)\n",
    "        self.lin4 = torch.nn.Linear(100, output_dim, dtype=torch.float64)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = torch.nn.functional.tanh(self.lin1(x))\n",
    "        x1 = torch.nn.functional.tanh(self.lin2(x1))\n",
    "        x1 = torch.nn.functional.tanh(self.lin3(x1))\n",
    "        x1 = self.lin4(x1)\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04de18ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (lin1): Linear(in_features=5, out_features=10, bias=True)\n",
       "  (lin2): Linear(in_features=10, out_features=50, bias=True)\n",
       "  (lin3): Linear(in_features=50, out_features=100, bias=True)\n",
       "  (lin4): Linear(in_features=100, out_features=59, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(input_dim, output_dim)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf2ec984",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(df.columns)\n",
    "features.remove('HB')\n",
    "features.remove('Ultimate strength, Gpa')\n",
    "features.remove('Youngs modulus, Gpa')\n",
    "features.remove('Density, g/cc')\n",
    "features.remove('Specific Heat of capacity, J/g*C')\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(df[['HB', 'Ultimate strength, Gpa', 'Youngs modulus, Gpa', 'Density, g/cc', 'Specific Heat of capacity, J/g*C']], df[features], test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df814fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessing(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_train, y_train, x_test, y_test, subset='train'):\n",
    "        self.subset = subset\n",
    "        self.scaler = StandardScaler()\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.scaler.fit(self.x_train)\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.subset == 'train':\n",
    "            return len(self.x_train)\n",
    "        else:\n",
    "            return len(self.x_test)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.subset == 'train':\n",
    "            ret_x = self.scaler.transform(self.x_train)\n",
    "            return torch.from_numpy(ret_x[index]).cuda(), torch.from_numpy(self.y_train.values[index]).cuda()\n",
    "        else:\n",
    "            ret_x = self.scaler.transform(self.x_test)\n",
    "            return torch.from_numpy(ret_x[index]).cuda(), torch.from_numpy(self.y_test.values[index]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "880b01f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataPreprocessing(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6db4a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7314, -0.6815,  0.7496,  0.2200, -0.1558], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e766481",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = len(data)\n",
    "epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0de8c18e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- epoch = 1/300 --------\n",
      "batch: 1; loss = 148.6210555966341\n",
      "-------- loss = 148.6210555966341 --------\n",
      "-------- epoch = 2/300 --------\n",
      "batch: 1; loss = 146.98573417454637\n",
      "-------- loss = 146.98573417454637 --------\n",
      "-------- epoch = 3/300 --------\n",
      "batch: 1; loss = 145.20372783506505\n",
      "-------- loss = 145.20372783506505 --------\n",
      "-------- epoch = 4/300 --------\n",
      "batch: 1; loss = 143.02957320359567\n",
      "-------- loss = 143.02957320359567 --------\n",
      "-------- epoch = 5/300 --------\n",
      "batch: 1; loss = 140.37018759675772\n",
      "-------- loss = 140.37018759675772 --------\n",
      "-------- epoch = 6/300 --------\n",
      "batch: 1; loss = 137.30056681858784\n",
      "-------- loss = 137.30056681858784 --------\n",
      "-------- epoch = 7/300 --------\n",
      "batch: 1; loss = 134.01802643447462\n",
      "-------- loss = 134.01802643447462 --------\n",
      "-------- epoch = 8/300 --------\n",
      "batch: 1; loss = 130.73921900770424\n",
      "-------- loss = 130.73921900770424 --------\n",
      "-------- epoch = 9/300 --------\n",
      "batch: 1; loss = 127.61525310567278\n",
      "-------- loss = 127.61525310567278 --------\n",
      "-------- epoch = 10/300 --------\n",
      "batch: 1; loss = 124.69678013670556\n",
      "-------- loss = 124.69678013670556 --------\n",
      "-------- epoch = 11/300 --------\n",
      "batch: 1; loss = 121.95438696347678\n",
      "-------- loss = 121.95438696347678 --------\n",
      "-------- epoch = 12/300 --------\n",
      "batch: 1; loss = 119.34220794783647\n",
      "-------- loss = 119.34220794783647 --------\n",
      "-------- epoch = 13/300 --------\n",
      "batch: 1; loss = 116.82131429237222\n",
      "-------- loss = 116.82131429237222 --------\n",
      "-------- epoch = 14/300 --------\n",
      "batch: 1; loss = 114.37000207737405\n",
      "-------- loss = 114.37000207737405 --------\n",
      "-------- epoch = 15/300 --------\n",
      "batch: 1; loss = 111.97924044866805\n",
      "-------- loss = 111.97924044866805 --------\n",
      "-------- epoch = 16/300 --------\n",
      "batch: 1; loss = 109.6482052528782\n",
      "-------- loss = 109.6482052528782 --------\n",
      "-------- epoch = 17/300 --------\n",
      "batch: 1; loss = 107.37839343990123\n",
      "-------- loss = 107.37839343990123 --------\n",
      "-------- epoch = 18/300 --------\n",
      "batch: 1; loss = 105.17055987182516\n",
      "-------- loss = 105.17055987182516 --------\n",
      "-------- epoch = 19/300 --------\n",
      "batch: 1; loss = 103.02465707998103\n",
      "-------- loss = 103.02465707998103 --------\n",
      "-------- epoch = 20/300 --------\n",
      "batch: 1; loss = 100.938305755914\n",
      "-------- loss = 100.938305755914 --------\n",
      "-------- epoch = 21/300 --------\n",
      "batch: 1; loss = 98.90468016455237\n",
      "-------- loss = 98.90468016455237 --------\n",
      "-------- epoch = 22/300 --------\n",
      "batch: 1; loss = 96.91599518254094\n",
      "-------- loss = 96.91599518254094 --------\n",
      "-------- epoch = 23/300 --------\n",
      "batch: 1; loss = 94.96934595932763\n",
      "-------- loss = 94.96934595932763 --------\n",
      "-------- epoch = 24/300 --------\n",
      "batch: 1; loss = 93.06652941922873\n",
      "-------- loss = 93.06652941922873 --------\n",
      "-------- epoch = 25/300 --------\n",
      "batch: 1; loss = 91.20996062585239\n",
      "-------- loss = 91.20996062585239 --------\n",
      "-------- epoch = 26/300 --------\n",
      "batch: 1; loss = 89.40118677864504\n",
      "-------- loss = 89.40118677864504 --------\n",
      "-------- epoch = 27/300 --------\n",
      "batch: 1; loss = 87.64043799482906\n",
      "-------- loss = 87.64043799482906 --------\n",
      "-------- epoch = 28/300 --------\n",
      "batch: 1; loss = 85.92694793440018\n",
      "-------- loss = 85.92694793440018 --------\n",
      "-------- epoch = 29/300 --------\n",
      "batch: 1; loss = 84.2591827761843\n",
      "-------- loss = 84.2591827761843 --------\n",
      "-------- epoch = 30/300 --------\n",
      "batch: 1; loss = 82.63413462874185\n",
      "-------- loss = 82.63413462874185 --------\n",
      "-------- epoch = 31/300 --------\n",
      "batch: 1; loss = 81.04721997547864\n",
      "-------- loss = 81.04721997547864 --------\n",
      "-------- epoch = 32/300 --------\n",
      "batch: 1; loss = 79.49330879261225\n",
      "-------- loss = 79.49330879261225 --------\n",
      "-------- epoch = 33/300 --------\n",
      "batch: 1; loss = 77.96853848190082\n",
      "-------- loss = 77.96853848190082 --------\n",
      "-------- epoch = 34/300 --------\n",
      "batch: 1; loss = 76.47124863240172\n",
      "-------- loss = 76.47124863240172 --------\n",
      "-------- epoch = 35/300 --------\n",
      "batch: 1; loss = 75.0007836525733\n",
      "-------- loss = 75.0007836525733 --------\n",
      "-------- epoch = 36/300 --------\n",
      "batch: 1; loss = 73.55698819927328\n",
      "-------- loss = 73.55698819927328 --------\n",
      "-------- epoch = 37/300 --------\n",
      "batch: 1; loss = 72.14049533030621\n",
      "-------- loss = 72.14049533030621 --------\n",
      "-------- epoch = 38/300 --------\n",
      "batch: 1; loss = 70.75305356338869\n",
      "-------- loss = 70.75305356338869 --------\n",
      "-------- epoch = 39/300 --------\n",
      "batch: 1; loss = 69.39746625506531\n",
      "-------- loss = 69.39746625506531 --------\n",
      "-------- epoch = 40/300 --------\n",
      "batch: 1; loss = 68.07667651960618\n",
      "-------- loss = 68.07667651960618 --------\n",
      "-------- epoch = 41/300 --------\n",
      "batch: 1; loss = 66.79306955515375\n",
      "-------- loss = 66.79306955515375 --------\n",
      "-------- epoch = 42/300 --------\n",
      "batch: 1; loss = 65.5483421874016\n",
      "-------- loss = 65.5483421874016 --------\n",
      "-------- epoch = 43/300 --------\n",
      "batch: 1; loss = 64.34401064484895\n",
      "-------- loss = 64.34401064484895 --------\n",
      "-------- epoch = 44/300 --------\n",
      "batch: 1; loss = 63.181651217236016\n",
      "-------- loss = 63.181651217236016 --------\n",
      "-------- epoch = 45/300 --------\n",
      "batch: 1; loss = 62.062670759197665\n",
      "-------- loss = 62.062670759197665 --------\n",
      "-------- epoch = 46/300 --------\n",
      "batch: 1; loss = 60.98820131526584\n",
      "-------- loss = 60.98820131526584 --------\n",
      "-------- epoch = 47/300 --------\n",
      "batch: 1; loss = 59.95900004559286\n",
      "-------- loss = 59.95900004559286 --------\n",
      "-------- epoch = 48/300 --------\n",
      "batch: 1; loss = 58.97523186426508\n",
      "-------- loss = 58.97523186426508 --------\n",
      "-------- epoch = 49/300 --------\n",
      "batch: 1; loss = 58.03614629921363\n",
      "-------- loss = 58.03614629921363 --------\n",
      "-------- epoch = 50/300 --------\n",
      "batch: 1; loss = 57.14024496068358\n",
      "-------- loss = 57.14024496068358 --------\n",
      "-------- epoch = 51/300 --------\n",
      "batch: 1; loss = 56.285783336601604\n",
      "-------- loss = 56.285783336601604 --------\n",
      "-------- epoch = 52/300 --------\n",
      "batch: 1; loss = 55.47090196452958\n",
      "-------- loss = 55.47090196452958 --------\n",
      "-------- epoch = 53/300 --------\n",
      "batch: 1; loss = 54.69349478232665\n",
      "-------- loss = 54.69349478232665 --------\n",
      "-------- epoch = 54/300 --------\n",
      "batch: 1; loss = 53.951006628729644\n",
      "-------- loss = 53.951006628729644 --------\n",
      "-------- epoch = 55/300 --------\n",
      "batch: 1; loss = 53.24052440756743\n",
      "-------- loss = 53.24052440756743 --------\n",
      "-------- epoch = 56/300 --------\n",
      "batch: 1; loss = 52.55914023369213\n",
      "-------- loss = 52.55914023369213 --------\n",
      "-------- epoch = 57/300 --------\n",
      "batch: 1; loss = 51.90420614706786\n",
      "-------- loss = 51.90420614706786 --------\n",
      "-------- epoch = 58/300 --------\n",
      "batch: 1; loss = 51.27337018410599\n",
      "-------- loss = 51.27337018410599 --------\n",
      "-------- epoch = 59/300 --------\n",
      "batch: 1; loss = 50.664619405499636\n",
      "-------- loss = 50.664619405499636 --------\n",
      "-------- epoch = 60/300 --------\n",
      "batch: 1; loss = 50.076404741846744\n",
      "-------- loss = 50.076404741846744 --------\n",
      "-------- epoch = 61/300 --------\n",
      "batch: 1; loss = 49.50761820318506\n",
      "-------- loss = 49.50761820318506 --------\n",
      "-------- epoch = 62/300 --------\n",
      "batch: 1; loss = 48.9574139163867\n",
      "-------- loss = 48.9574139163867 --------\n",
      "-------- epoch = 63/300 --------\n",
      "batch: 1; loss = 48.4251518345432\n",
      "-------- loss = 48.4251518345432 --------\n",
      "-------- epoch = 64/300 --------\n",
      "batch: 1; loss = 47.910404437050204\n",
      "-------- loss = 47.910404437050204 --------\n",
      "-------- epoch = 65/300 --------\n",
      "batch: 1; loss = 47.413030032249196\n",
      "-------- loss = 47.413030032249196 --------\n",
      "-------- epoch = 66/300 --------\n",
      "batch: 1; loss = 46.93308389714519\n",
      "-------- loss = 46.93308389714519 --------\n",
      "-------- epoch = 67/300 --------\n",
      "batch: 1; loss = 46.47069101939126\n",
      "-------- loss = 46.47069101939126 --------\n",
      "-------- epoch = 68/300 --------\n",
      "batch: 1; loss = 46.02595285798044\n",
      "-------- loss = 46.02595285798044 --------\n",
      "-------- epoch = 69/300 --------\n",
      "batch: 1; loss = 45.59892122483191\n",
      "-------- loss = 45.59892122483191 --------\n",
      "-------- epoch = 70/300 --------\n",
      "batch: 1; loss = 45.18954964333826\n",
      "-------- loss = 45.18954964333826 --------\n",
      "-------- epoch = 71/300 --------\n",
      "batch: 1; loss = 44.797618497929896\n",
      "-------- loss = 44.797618497929896 --------\n",
      "-------- epoch = 72/300 --------\n",
      "batch: 1; loss = 44.42275154014274\n",
      "-------- loss = 44.42275154014274 --------\n",
      "-------- epoch = 73/300 --------\n",
      "batch: 1; loss = 44.06450551500602\n",
      "-------- loss = 44.06450551500602 --------\n",
      "-------- epoch = 74/300 --------\n",
      "batch: 1; loss = 43.72242879658328\n",
      "-------- loss = 43.72242879658328 --------\n",
      "-------- epoch = 75/300 --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1; loss = 43.396085016097835\n",
      "-------- loss = 43.396085016097835 --------\n",
      "-------- epoch = 76/300 --------\n",
      "batch: 1; loss = 43.084967805313\n",
      "-------- loss = 43.084967805313 --------\n",
      "-------- epoch = 77/300 --------\n",
      "batch: 1; loss = 42.78831795300933\n",
      "-------- loss = 42.78831795300933 --------\n",
      "-------- epoch = 78/300 --------\n",
      "batch: 1; loss = 42.50496454854818\n",
      "-------- loss = 42.50496454854818 --------\n",
      "-------- epoch = 79/300 --------\n",
      "batch: 1; loss = 42.23346331639187\n",
      "-------- loss = 42.23346331639187 --------\n",
      "-------- epoch = 80/300 --------\n",
      "batch: 1; loss = 41.97253014764072\n",
      "-------- loss = 41.97253014764072 --------\n",
      "-------- epoch = 81/300 --------\n",
      "batch: 1; loss = 41.721444585767294\n",
      "-------- loss = 41.721444585767294 --------\n",
      "-------- epoch = 82/300 --------\n",
      "batch: 1; loss = 41.480036026053114\n",
      "-------- loss = 41.480036026053114 --------\n",
      "-------- epoch = 83/300 --------\n",
      "batch: 1; loss = 41.2483620207704\n",
      "-------- loss = 41.2483620207704 --------\n",
      "-------- epoch = 84/300 --------\n",
      "batch: 1; loss = 41.026378308581016\n",
      "-------- loss = 41.026378308581016 --------\n",
      "-------- epoch = 85/300 --------\n",
      "batch: 1; loss = 40.813793749911326\n",
      "-------- loss = 40.813793749911326 --------\n",
      "-------- epoch = 86/300 --------\n",
      "batch: 1; loss = 40.61006486404674\n",
      "-------- loss = 40.61006486404674 --------\n",
      "-------- epoch = 87/300 --------\n",
      "batch: 1; loss = 40.414434320805995\n",
      "-------- loss = 40.414434320805995 --------\n",
      "-------- epoch = 88/300 --------\n",
      "batch: 1; loss = 40.22596436765478\n",
      "-------- loss = 40.22596436765478 --------\n",
      "-------- epoch = 89/300 --------\n",
      "batch: 1; loss = 40.04360897863256\n",
      "-------- loss = 40.04360897863256 --------\n",
      "-------- epoch = 90/300 --------\n",
      "batch: 1; loss = 39.866385867832385\n",
      "-------- loss = 39.866385867832385 --------\n",
      "-------- epoch = 91/300 --------\n",
      "batch: 1; loss = 39.69358082603449\n",
      "-------- loss = 39.69358082603449 --------\n",
      "-------- epoch = 92/300 --------\n",
      "batch: 1; loss = 39.52481498020534\n",
      "-------- loss = 39.52481498020534 --------\n",
      "-------- epoch = 93/300 --------\n",
      "batch: 1; loss = 39.35992305663769\n",
      "-------- loss = 39.35992305663769 --------\n",
      "-------- epoch = 94/300 --------\n",
      "batch: 1; loss = 39.19879260480108\n",
      "-------- loss = 39.19879260480108 --------\n",
      "-------- epoch = 95/300 --------\n",
      "batch: 1; loss = 39.04132964372573\n",
      "-------- loss = 39.04132964372573 --------\n",
      "-------- epoch = 96/300 --------\n",
      "batch: 1; loss = 38.887541600237995\n",
      "-------- loss = 38.887541600237995 --------\n",
      "-------- epoch = 97/300 --------\n",
      "batch: 1; loss = 38.73759800768796\n",
      "-------- loss = 38.73759800768796 --------\n",
      "-------- epoch = 98/300 --------\n",
      "batch: 1; loss = 38.591785585537195\n",
      "-------- loss = 38.591785585537195 --------\n",
      "-------- epoch = 99/300 --------\n",
      "batch: 1; loss = 38.45040103008018\n",
      "-------- loss = 38.45040103008018 --------\n",
      "-------- epoch = 100/300 --------\n",
      "batch: 1; loss = 38.31367530568551\n",
      "-------- loss = 38.31367530568551 --------\n",
      "-------- epoch = 101/300 --------\n",
      "batch: 1; loss = 38.18176228702749\n",
      "-------- loss = 38.18176228702749 --------\n",
      "-------- epoch = 102/300 --------\n",
      "batch: 1; loss = 38.054757754882736\n",
      "-------- loss = 38.054757754882736 --------\n",
      "-------- epoch = 103/300 --------\n",
      "batch: 1; loss = 37.932702288773534\n",
      "-------- loss = 37.932702288773534 --------\n",
      "-------- epoch = 104/300 --------\n",
      "batch: 1; loss = 37.81556430395607\n",
      "-------- loss = 37.81556430395607 --------\n",
      "-------- epoch = 105/300 --------\n",
      "batch: 1; loss = 37.70323588206012\n",
      "-------- loss = 37.70323588206012 --------\n",
      "-------- epoch = 106/300 --------\n",
      "batch: 1; loss = 37.59554200106314\n",
      "-------- loss = 37.59554200106314 --------\n",
      "-------- epoch = 107/300 --------\n",
      "batch: 1; loss = 37.492238613695825\n",
      "-------- loss = 37.492238613695825 --------\n",
      "-------- epoch = 108/300 --------\n",
      "batch: 1; loss = 37.393005568426794\n",
      "-------- loss = 37.393005568426794 --------\n",
      "-------- epoch = 109/300 --------\n",
      "batch: 1; loss = 37.297464156852186\n",
      "-------- loss = 37.297464156852186 --------\n",
      "-------- epoch = 110/300 --------\n",
      "batch: 1; loss = 37.205236718581965\n",
      "-------- loss = 37.205236718581965 --------\n",
      "-------- epoch = 111/300 --------\n",
      "batch: 1; loss = 37.116018122192315\n",
      "-------- loss = 37.116018122192315 --------\n",
      "-------- epoch = 112/300 --------\n",
      "batch: 1; loss = 37.02961665306923\n",
      "-------- loss = 37.02961665306923 --------\n",
      "-------- epoch = 113/300 --------\n",
      "batch: 1; loss = 36.945948000968286\n",
      "-------- loss = 36.945948000968286 --------\n",
      "-------- epoch = 114/300 --------\n",
      "batch: 1; loss = 36.86501279543658\n",
      "-------- loss = 36.86501279543658 --------\n",
      "-------- epoch = 115/300 --------\n",
      "batch: 1; loss = 36.78688808933819\n",
      "-------- loss = 36.78688808933819 --------\n",
      "-------- epoch = 116/300 --------\n",
      "batch: 1; loss = 36.71170763498076\n",
      "-------- loss = 36.71170763498076 --------\n",
      "-------- epoch = 117/300 --------\n",
      "batch: 1; loss = 36.6396135255281\n",
      "-------- loss = 36.6396135255281 --------\n",
      "-------- epoch = 118/300 --------\n",
      "batch: 1; loss = 36.57073852460052\n",
      "-------- loss = 36.57073852460052 --------\n",
      "-------- epoch = 119/300 --------\n",
      "batch: 1; loss = 36.50520250236694\n",
      "-------- loss = 36.50520250236694 --------\n",
      "-------- epoch = 120/300 --------\n",
      "batch: 1; loss = 36.44307986094979\n",
      "-------- loss = 36.44307986094979 --------\n",
      "-------- epoch = 121/300 --------\n",
      "batch: 1; loss = 36.3843494307774\n",
      "-------- loss = 36.3843494307774 --------\n",
      "-------- epoch = 122/300 --------\n",
      "batch: 1; loss = 36.32889157388368\n",
      "-------- loss = 36.32889157388368 --------\n",
      "-------- epoch = 123/300 --------\n",
      "batch: 1; loss = 36.276532892021514\n",
      "-------- loss = 36.276532892021514 --------\n",
      "-------- epoch = 124/300 --------\n",
      "batch: 1; loss = 36.227075230303434\n",
      "-------- loss = 36.227075230303434 --------\n",
      "-------- epoch = 125/300 --------\n",
      "batch: 1; loss = 36.18028864148355\n",
      "-------- loss = 36.18028864148355 --------\n",
      "-------- epoch = 126/300 --------\n",
      "batch: 1; loss = 36.13592333001191\n",
      "-------- loss = 36.13592333001191 --------\n",
      "-------- epoch = 127/300 --------\n",
      "batch: 1; loss = 36.093745975333604\n",
      "-------- loss = 36.093745975333604 --------\n",
      "-------- epoch = 128/300 --------\n",
      "batch: 1; loss = 36.05355337383185\n",
      "-------- loss = 36.05355337383185 --------\n",
      "-------- epoch = 129/300 --------\n",
      "batch: 1; loss = 36.015162484718665\n",
      "-------- loss = 36.015162484718665 --------\n",
      "-------- epoch = 130/300 --------\n",
      "batch: 1; loss = 35.97841062157361\n",
      "-------- loss = 35.97841062157361 --------\n",
      "-------- epoch = 131/300 --------\n",
      "batch: 1; loss = 35.94316698068145\n",
      "-------- loss = 35.94316698068145 --------\n",
      "-------- epoch = 132/300 --------\n",
      "batch: 1; loss = 35.90932844356209\n",
      "-------- loss = 35.90932844356209 --------\n",
      "-------- epoch = 133/300 --------\n",
      "batch: 1; loss = 35.87680453947434\n",
      "-------- loss = 35.87680453947434 --------\n",
      "-------- epoch = 134/300 --------\n",
      "batch: 1; loss = 35.845520325185625\n",
      "-------- loss = 35.845520325185625 --------\n",
      "-------- epoch = 135/300 --------\n",
      "batch: 1; loss = 35.8154296555209\n",
      "-------- loss = 35.8154296555209 --------\n",
      "-------- epoch = 136/300 --------\n",
      "batch: 1; loss = 35.78651414770301\n",
      "-------- loss = 35.78651414770301 --------\n",
      "-------- epoch = 137/300 --------\n",
      "batch: 1; loss = 35.75877389691058\n",
      "-------- loss = 35.75877389691058 --------\n",
      "-------- epoch = 138/300 --------\n",
      "batch: 1; loss = 35.7322204427948\n",
      "-------- loss = 35.7322204427948 --------\n",
      "-------- epoch = 139/300 --------\n",
      "batch: 1; loss = 35.70687188845349\n",
      "-------- loss = 35.70687188845349 --------\n",
      "-------- epoch = 140/300 --------\n",
      "batch: 1; loss = 35.68274316334861\n",
      "-------- loss = 35.68274316334861 --------\n",
      "-------- epoch = 141/300 --------\n",
      "batch: 1; loss = 35.65983312077563\n",
      "-------- loss = 35.65983312077563 --------\n",
      "-------- epoch = 142/300 --------\n",
      "batch: 1; loss = 35.63811763623928\n",
      "-------- loss = 35.63811763623928 --------\n",
      "-------- epoch = 143/300 --------\n",
      "batch: 1; loss = 35.6175518389841\n",
      "-------- loss = 35.6175518389841 --------\n",
      "-------- epoch = 144/300 --------\n",
      "batch: 1; loss = 35.59807153836192\n",
      "-------- loss = 35.59807153836192 --------\n",
      "-------- epoch = 145/300 --------\n",
      "batch: 1; loss = 35.579598042157414\n",
      "-------- loss = 35.579598042157414 --------\n",
      "-------- epoch = 146/300 --------\n",
      "batch: 1; loss = 35.56204359342659\n",
      "-------- loss = 35.56204359342659 --------\n",
      "-------- epoch = 147/300 --------\n",
      "batch: 1; loss = 35.54532610809564\n",
      "-------- loss = 35.54532610809564 --------\n",
      "-------- epoch = 148/300 --------\n",
      "batch: 1; loss = 35.52937284264173\n",
      "-------- loss = 35.52937284264173 --------\n",
      "-------- epoch = 149/300 --------\n",
      "batch: 1; loss = 35.51412980186976\n",
      "-------- loss = 35.51412980186976 --------\n",
      "-------- epoch = 150/300 --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1; loss = 35.499549781341734\n",
      "-------- loss = 35.499549781341734 --------\n",
      "-------- epoch = 151/300 --------\n",
      "batch: 1; loss = 35.485611396501014\n",
      "-------- loss = 35.485611396501014 --------\n",
      "-------- epoch = 152/300 --------\n",
      "batch: 1; loss = 35.47228602476115\n",
      "-------- loss = 35.47228602476115 --------\n",
      "-------- epoch = 153/300 --------\n",
      "batch: 1; loss = 35.45956868951027\n",
      "-------- loss = 35.45956868951027 --------\n",
      "-------- epoch = 154/300 --------\n",
      "batch: 1; loss = 35.44742929017753\n",
      "-------- loss = 35.44742929017753 --------\n",
      "-------- epoch = 155/300 --------\n",
      "batch: 1; loss = 35.435880461192234\n",
      "-------- loss = 35.435880461192234 --------\n",
      "-------- epoch = 156/300 --------\n",
      "batch: 1; loss = 35.4249079965065\n",
      "-------- loss = 35.4249079965065 --------\n",
      "-------- epoch = 157/300 --------\n",
      "batch: 1; loss = 35.41448680772415\n",
      "-------- loss = 35.41448680772415 --------\n",
      "-------- epoch = 158/300 --------\n",
      "batch: 1; loss = 35.4046170909134\n",
      "-------- loss = 35.4046170909134 --------\n",
      "-------- epoch = 159/300 --------\n",
      "batch: 1; loss = 35.395291133999535\n",
      "-------- loss = 35.395291133999535 --------\n",
      "-------- epoch = 160/300 --------\n",
      "batch: 1; loss = 35.386488995494325\n",
      "-------- loss = 35.386488995494325 --------\n",
      "-------- epoch = 161/300 --------\n",
      "batch: 1; loss = 35.37815609363677\n",
      "-------- loss = 35.37815609363677 --------\n",
      "-------- epoch = 162/300 --------\n",
      "batch: 1; loss = 35.37025397199532\n",
      "-------- loss = 35.37025397199532 --------\n",
      "-------- epoch = 163/300 --------\n",
      "batch: 1; loss = 35.36277499369472\n",
      "-------- loss = 35.36277499369472 --------\n",
      "-------- epoch = 164/300 --------\n",
      "batch: 1; loss = 35.355675484302694\n",
      "-------- loss = 35.355675484302694 --------\n",
      "-------- epoch = 165/300 --------\n",
      "batch: 1; loss = 35.348911037480505\n",
      "-------- loss = 35.348911037480505 --------\n",
      "-------- epoch = 166/300 --------\n",
      "batch: 1; loss = 35.34245846765979\n",
      "-------- loss = 35.34245846765979 --------\n",
      "-------- epoch = 167/300 --------\n",
      "batch: 1; loss = 35.33630650722941\n",
      "-------- loss = 35.33630650722941 --------\n",
      "-------- epoch = 168/300 --------\n",
      "batch: 1; loss = 35.330441582194354\n",
      "-------- loss = 35.330441582194354 --------\n",
      "-------- epoch = 169/300 --------\n",
      "batch: 1; loss = 35.3248492349084\n",
      "-------- loss = 35.3248492349084 --------\n",
      "-------- epoch = 170/300 --------\n",
      "batch: 1; loss = 35.31952633469461\n",
      "-------- loss = 35.31952633469461 --------\n",
      "-------- epoch = 171/300 --------\n",
      "batch: 1; loss = 35.3144891885753\n",
      "-------- loss = 35.3144891885753 --------\n",
      "-------- epoch = 172/300 --------\n",
      "batch: 1; loss = 35.309708277941134\n",
      "-------- loss = 35.309708277941134 --------\n",
      "-------- epoch = 173/300 --------\n",
      "batch: 1; loss = 35.30520146337517\n",
      "-------- loss = 35.30520146337517 --------\n",
      "-------- epoch = 174/300 --------\n",
      "batch: 1; loss = 35.3009388290437\n",
      "-------- loss = 35.3009388290437 --------\n",
      "-------- epoch = 175/300 --------\n",
      "batch: 1; loss = 35.29693498239278\n",
      "-------- loss = 35.29693498239278 --------\n",
      "-------- epoch = 176/300 --------\n",
      "batch: 1; loss = 35.293111786116626\n",
      "-------- loss = 35.293111786116626 --------\n",
      "-------- epoch = 177/300 --------\n",
      "batch: 1; loss = 35.289491400741994\n",
      "-------- loss = 35.289491400741994 --------\n",
      "-------- epoch = 178/300 --------\n",
      "batch: 1; loss = 35.28607321725691\n",
      "-------- loss = 35.28607321725691 --------\n",
      "-------- epoch = 179/300 --------\n",
      "batch: 1; loss = 35.28284894962812\n",
      "-------- loss = 35.28284894962812 --------\n",
      "-------- epoch = 180/300 --------\n",
      "batch: 1; loss = 35.279773338209324\n",
      "-------- loss = 35.279773338209324 --------\n",
      "-------- epoch = 181/300 --------\n",
      "batch: 1; loss = 35.27679759127658\n",
      "-------- loss = 35.27679759127658 --------\n",
      "-------- epoch = 182/300 --------\n",
      "batch: 1; loss = 35.273964141140574\n",
      "-------- loss = 35.273964141140574 --------\n",
      "-------- epoch = 183/300 --------\n",
      "batch: 1; loss = 35.271266658572046\n",
      "-------- loss = 35.271266658572046 --------\n",
      "-------- epoch = 184/300 --------\n",
      "batch: 1; loss = 35.26868495241849\n",
      "-------- loss = 35.26868495241849 --------\n",
      "-------- epoch = 185/300 --------\n",
      "batch: 1; loss = 35.26621526514064\n",
      "-------- loss = 35.26621526514064 --------\n",
      "-------- epoch = 186/300 --------\n",
      "batch: 1; loss = 35.26388935181976\n",
      "-------- loss = 35.26388935181976 --------\n",
      "-------- epoch = 187/300 --------\n",
      "batch: 1; loss = 35.261658131754466\n",
      "-------- loss = 35.261658131754466 --------\n",
      "-------- epoch = 188/300 --------\n",
      "batch: 1; loss = 35.25954843252149\n",
      "-------- loss = 35.25954843252149 --------\n",
      "-------- epoch = 189/300 --------\n",
      "batch: 1; loss = 35.257537000014075\n",
      "-------- loss = 35.257537000014075 --------\n",
      "-------- epoch = 190/300 --------\n",
      "batch: 1; loss = 35.25571195219882\n",
      "-------- loss = 35.25571195219882 --------\n",
      "-------- epoch = 191/300 --------\n",
      "batch: 1; loss = 35.25396377269532\n",
      "-------- loss = 35.25396377269532 --------\n",
      "-------- epoch = 192/300 --------\n",
      "batch: 1; loss = 35.25224954569368\n",
      "-------- loss = 35.25224954569368 --------\n",
      "-------- epoch = 193/300 --------\n",
      "batch: 1; loss = 35.25061567790535\n",
      "-------- loss = 35.25061567790535 --------\n",
      "-------- epoch = 194/300 --------\n",
      "batch: 1; loss = 35.249095036253415\n",
      "-------- loss = 35.249095036253415 --------\n",
      "-------- epoch = 195/300 --------\n",
      "batch: 1; loss = 35.24767248409261\n",
      "-------- loss = 35.24767248409261 --------\n",
      "-------- epoch = 196/300 --------\n",
      "batch: 1; loss = 35.24627748745451\n",
      "-------- loss = 35.24627748745451 --------\n",
      "-------- epoch = 197/300 --------\n",
      "batch: 1; loss = 35.24491713865203\n",
      "-------- loss = 35.24491713865203 --------\n",
      "-------- epoch = 198/300 --------\n",
      "batch: 1; loss = 35.243622399238504\n",
      "-------- loss = 35.243622399238504 --------\n",
      "-------- epoch = 199/300 --------\n",
      "batch: 1; loss = 35.24232684360765\n",
      "-------- loss = 35.24232684360765 --------\n",
      "-------- epoch = 200/300 --------\n",
      "batch: 1; loss = 35.24105881298592\n",
      "-------- loss = 35.24105881298592 --------\n",
      "-------- epoch = 201/300 --------\n",
      "batch: 1; loss = 35.23986688331587\n",
      "-------- loss = 35.23986688331587 --------\n",
      "-------- epoch = 202/300 --------\n",
      "batch: 1; loss = 35.238738516491225\n",
      "-------- loss = 35.238738516491225 --------\n",
      "-------- epoch = 203/300 --------\n",
      "batch: 1; loss = 35.23763892330353\n",
      "-------- loss = 35.23763892330353 --------\n",
      "-------- epoch = 204/300 --------\n",
      "batch: 1; loss = 35.23651757917991\n",
      "-------- loss = 35.23651757917991 --------\n",
      "-------- epoch = 205/300 --------\n",
      "batch: 1; loss = 35.23543190120783\n",
      "-------- loss = 35.23543190120783 --------\n",
      "-------- epoch = 206/300 --------\n",
      "batch: 1; loss = 35.23441836169831\n",
      "-------- loss = 35.23441836169831 --------\n",
      "-------- epoch = 207/300 --------\n",
      "batch: 1; loss = 35.23346489968924\n",
      "-------- loss = 35.23346489968924 --------\n",
      "-------- epoch = 208/300 --------\n",
      "batch: 1; loss = 35.232484957647564\n",
      "-------- loss = 35.232484957647564 --------\n",
      "-------- epoch = 209/300 --------\n",
      "batch: 1; loss = 35.231480643299285\n",
      "-------- loss = 35.231480643299285 --------\n",
      "-------- epoch = 210/300 --------\n",
      "batch: 1; loss = 35.23048386771444\n",
      "-------- loss = 35.23048386771444 --------\n",
      "-------- epoch = 211/300 --------\n",
      "batch: 1; loss = 35.22956539557651\n",
      "-------- loss = 35.22956539557651 --------\n",
      "-------- epoch = 212/300 --------\n",
      "batch: 1; loss = 35.228654248648304\n",
      "-------- loss = 35.228654248648304 --------\n",
      "-------- epoch = 213/300 --------\n",
      "batch: 1; loss = 35.22768809381912\n",
      "-------- loss = 35.22768809381912 --------\n",
      "-------- epoch = 214/300 --------\n",
      "batch: 1; loss = 35.22674354896952\n",
      "-------- loss = 35.22674354896952 --------\n",
      "-------- epoch = 215/300 --------\n",
      "batch: 1; loss = 35.22582969675625\n",
      "-------- loss = 35.22582969675625 --------\n",
      "-------- epoch = 216/300 --------\n",
      "batch: 1; loss = 35.224892802383096\n",
      "-------- loss = 35.224892802383096 --------\n",
      "-------- epoch = 217/300 --------\n",
      "batch: 1; loss = 35.22392925755529\n",
      "-------- loss = 35.22392925755529 --------\n",
      "-------- epoch = 218/300 --------\n",
      "batch: 1; loss = 35.22298960295552\n",
      "-------- loss = 35.22298960295552 --------\n",
      "-------- epoch = 219/300 --------\n",
      "batch: 1; loss = 35.22210782093129\n",
      "-------- loss = 35.22210782093129 --------\n",
      "-------- epoch = 220/300 --------\n",
      "batch: 1; loss = 35.22121393713404\n",
      "-------- loss = 35.22121393713404 --------\n",
      "-------- epoch = 221/300 --------\n",
      "batch: 1; loss = 35.22027762201282\n",
      "-------- loss = 35.22027762201282 --------\n",
      "-------- epoch = 222/300 --------\n",
      "batch: 1; loss = 35.219361952625064\n",
      "-------- loss = 35.219361952625064 --------\n",
      "-------- epoch = 223/300 --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1; loss = 35.21847106132535\n",
      "-------- loss = 35.21847106132535 --------\n",
      "-------- epoch = 224/300 --------\n",
      "batch: 1; loss = 35.21758372474629\n",
      "-------- loss = 35.21758372474629 --------\n",
      "-------- epoch = 225/300 --------\n",
      "batch: 1; loss = 35.21671444352046\n",
      "-------- loss = 35.21671444352046 --------\n",
      "-------- epoch = 226/300 --------\n",
      "batch: 1; loss = 35.21585280309845\n",
      "-------- loss = 35.21585280309845 --------\n",
      "-------- epoch = 227/300 --------\n",
      "batch: 1; loss = 35.214935922878254\n",
      "-------- loss = 35.214935922878254 --------\n",
      "-------- epoch = 228/300 --------\n",
      "batch: 1; loss = 35.21394360051841\n",
      "-------- loss = 35.21394360051841 --------\n",
      "-------- epoch = 229/300 --------\n",
      "batch: 1; loss = 35.21301425910153\n",
      "-------- loss = 35.21301425910153 --------\n",
      "-------- epoch = 230/300 --------\n",
      "batch: 1; loss = 35.21217354404134\n",
      "-------- loss = 35.21217354404134 --------\n",
      "-------- epoch = 231/300 --------\n",
      "batch: 1; loss = 35.21127339950228\n",
      "-------- loss = 35.21127339950228 --------\n",
      "-------- epoch = 232/300 --------\n",
      "batch: 1; loss = 35.21030993141214\n",
      "-------- loss = 35.21030993141214 --------\n",
      "-------- epoch = 233/300 --------\n",
      "batch: 1; loss = 35.20942228661686\n",
      "-------- loss = 35.20942228661686 --------\n",
      "-------- epoch = 234/300 --------\n",
      "batch: 1; loss = 35.208574161568045\n",
      "-------- loss = 35.208574161568045 --------\n",
      "-------- epoch = 235/300 --------\n",
      "batch: 1; loss = 35.20765671608471\n",
      "-------- loss = 35.20765671608471 --------\n",
      "-------- epoch = 236/300 --------\n",
      "batch: 1; loss = 35.20674445132734\n",
      "-------- loss = 35.20674445132734 --------\n",
      "-------- epoch = 237/300 --------\n",
      "batch: 1; loss = 35.20589453408145\n",
      "-------- loss = 35.20589453408145 --------\n",
      "-------- epoch = 238/300 --------\n",
      "batch: 1; loss = 35.20502286332054\n",
      "-------- loss = 35.20502286332054 --------\n",
      "-------- epoch = 239/300 --------\n",
      "batch: 1; loss = 35.20413207181653\n",
      "-------- loss = 35.20413207181653 --------\n",
      "-------- epoch = 240/300 --------\n",
      "batch: 1; loss = 35.20327894106062\n",
      "-------- loss = 35.20327894106062 --------\n",
      "-------- epoch = 241/300 --------\n",
      "batch: 1; loss = 35.20242798410108\n",
      "-------- loss = 35.20242798410108 --------\n",
      "-------- epoch = 242/300 --------\n",
      "batch: 1; loss = 35.20156093842211\n",
      "-------- loss = 35.20156093842211 --------\n",
      "-------- epoch = 243/300 --------\n",
      "batch: 1; loss = 35.20071450206196\n",
      "-------- loss = 35.20071450206196 --------\n",
      "-------- epoch = 244/300 --------\n",
      "batch: 1; loss = 35.19988007726272\n",
      "-------- loss = 35.19988007726272 --------\n",
      "-------- epoch = 245/300 --------\n",
      "batch: 1; loss = 35.19902351849194\n",
      "-------- loss = 35.19902351849194 --------\n",
      "-------- epoch = 246/300 --------\n",
      "batch: 1; loss = 35.198179569855014\n",
      "-------- loss = 35.198179569855014 --------\n",
      "-------- epoch = 247/300 --------\n",
      "batch: 1; loss = 35.197353334346694\n",
      "-------- loss = 35.197353334346694 --------\n",
      "-------- epoch = 248/300 --------\n",
      "batch: 1; loss = 35.19650961314469\n",
      "-------- loss = 35.19650961314469 --------\n",
      "-------- epoch = 249/300 --------\n",
      "batch: 1; loss = 35.19566626916945\n",
      "-------- loss = 35.19566626916945 --------\n",
      "-------- epoch = 250/300 --------\n",
      "batch: 1; loss = 35.194840117730664\n",
      "-------- loss = 35.194840117730664 --------\n",
      "-------- epoch = 251/300 --------\n",
      "batch: 1; loss = 35.194001129355875\n",
      "-------- loss = 35.194001129355875 --------\n",
      "-------- epoch = 252/300 --------\n",
      "batch: 1; loss = 35.19316370631094\n",
      "-------- loss = 35.19316370631094 --------\n",
      "-------- epoch = 253/300 --------\n",
      "batch: 1; loss = 35.19233173370395\n",
      "-------- loss = 35.19233173370395 --------\n",
      "-------- epoch = 254/300 --------\n",
      "batch: 1; loss = 35.191507170000634\n",
      "-------- loss = 35.191507170000634 --------\n",
      "-------- epoch = 255/300 --------\n",
      "batch: 1; loss = 35.19066224543994\n",
      "-------- loss = 35.19066224543994 --------\n",
      "-------- epoch = 256/300 --------\n",
      "batch: 1; loss = 35.18981921639161\n",
      "-------- loss = 35.18981921639161 --------\n",
      "-------- epoch = 257/300 --------\n",
      "batch: 1; loss = 35.18895489374493\n",
      "-------- loss = 35.18895489374493 --------\n",
      "-------- epoch = 258/300 --------\n",
      "batch: 1; loss = 35.18808832035862\n",
      "-------- loss = 35.18808832035862 --------\n",
      "-------- epoch = 259/300 --------\n",
      "batch: 1; loss = 35.187233510346736\n",
      "-------- loss = 35.187233510346736 --------\n",
      "-------- epoch = 260/300 --------\n",
      "batch: 1; loss = 35.18639375268465\n",
      "-------- loss = 35.18639375268465 --------\n",
      "-------- epoch = 261/300 --------\n",
      "batch: 1; loss = 35.1855616393116\n",
      "-------- loss = 35.1855616393116 --------\n",
      "-------- epoch = 262/300 --------\n",
      "batch: 1; loss = 35.184708549971674\n",
      "-------- loss = 35.184708549971674 --------\n",
      "-------- epoch = 263/300 --------\n",
      "batch: 1; loss = 35.18387926795348\n",
      "-------- loss = 35.18387926795348 --------\n",
      "-------- epoch = 264/300 --------\n",
      "batch: 1; loss = 35.18305635571012\n",
      "-------- loss = 35.18305635571012 --------\n",
      "-------- epoch = 265/300 --------\n",
      "batch: 1; loss = 35.18230942770472\n",
      "-------- loss = 35.18230942770472 --------\n",
      "-------- epoch = 266/300 --------\n",
      "batch: 1; loss = 35.18155375199981\n",
      "-------- loss = 35.18155375199981 --------\n",
      "-------- epoch = 267/300 --------\n",
      "batch: 1; loss = 35.18073044636909\n",
      "-------- loss = 35.18073044636909 --------\n",
      "-------- epoch = 268/300 --------\n",
      "batch: 1; loss = 35.1797373282476\n",
      "-------- loss = 35.1797373282476 --------\n",
      "-------- epoch = 269/300 --------\n",
      "batch: 1; loss = 35.17884644508224\n",
      "-------- loss = 35.17884644508224 --------\n",
      "-------- epoch = 270/300 --------\n",
      "batch: 1; loss = 35.178132030845134\n",
      "-------- loss = 35.178132030845134 --------\n",
      "-------- epoch = 271/300 --------\n",
      "batch: 1; loss = 35.17735197989931\n",
      "-------- loss = 35.17735197989931 --------\n",
      "-------- epoch = 272/300 --------\n",
      "batch: 1; loss = 35.17646556066958\n",
      "-------- loss = 35.17646556066958 --------\n",
      "-------- epoch = 273/300 --------\n",
      "batch: 1; loss = 35.17563929279668\n",
      "-------- loss = 35.17563929279668 --------\n",
      "-------- epoch = 274/300 --------\n",
      "batch: 1; loss = 35.17491807348095\n",
      "-------- loss = 35.17491807348095 --------\n",
      "-------- epoch = 275/300 --------\n",
      "batch: 1; loss = 35.174148956043396\n",
      "-------- loss = 35.174148956043396 --------\n",
      "-------- epoch = 276/300 --------\n",
      "batch: 1; loss = 35.173304207379445\n",
      "-------- loss = 35.173304207379445 --------\n",
      "-------- epoch = 277/300 --------\n",
      "batch: 1; loss = 35.17253970477236\n",
      "-------- loss = 35.17253970477236 --------\n",
      "-------- epoch = 278/300 --------\n",
      "batch: 1; loss = 35.17178923193277\n",
      "-------- loss = 35.17178923193277 --------\n",
      "-------- epoch = 279/300 --------\n",
      "batch: 1; loss = 35.170957660496484\n",
      "-------- loss = 35.170957660496484 --------\n",
      "-------- epoch = 280/300 --------\n",
      "batch: 1; loss = 35.1701962013303\n",
      "-------- loss = 35.1701962013303 --------\n",
      "-------- epoch = 281/300 --------\n",
      "batch: 1; loss = 35.169502215598584\n",
      "-------- loss = 35.169502215598584 --------\n",
      "-------- epoch = 282/300 --------\n",
      "batch: 1; loss = 35.16874447142135\n",
      "-------- loss = 35.16874447142135 --------\n",
      "-------- epoch = 283/300 --------\n",
      "batch: 1; loss = 35.16797074976865\n",
      "-------- loss = 35.16797074976865 --------\n",
      "-------- epoch = 284/300 --------\n",
      "batch: 1; loss = 35.16723527797096\n",
      "-------- loss = 35.16723527797096 --------\n",
      "-------- epoch = 285/300 --------\n",
      "batch: 1; loss = 35.16650902124475\n",
      "-------- loss = 35.16650902124475 --------\n",
      "-------- epoch = 286/300 --------\n",
      "batch: 1; loss = 35.165778526184695\n",
      "-------- loss = 35.165778526184695 --------\n",
      "-------- epoch = 287/300 --------\n",
      "batch: 1; loss = 35.165055204021236\n",
      "-------- loss = 35.165055204021236 --------\n",
      "-------- epoch = 288/300 --------\n",
      "batch: 1; loss = 35.164339254426686\n",
      "-------- loss = 35.164339254426686 --------\n",
      "-------- epoch = 289/300 --------\n",
      "batch: 1; loss = 35.16361293936942\n",
      "-------- loss = 35.16361293936942 --------\n",
      "-------- epoch = 290/300 --------\n",
      "batch: 1; loss = 35.16289388845263\n",
      "-------- loss = 35.16289388845263 --------\n",
      "-------- epoch = 291/300 --------\n",
      "batch: 1; loss = 35.16219832045712\n",
      "-------- loss = 35.16219832045712 --------\n",
      "-------- epoch = 292/300 --------\n",
      "batch: 1; loss = 35.16149803698147\n",
      "-------- loss = 35.16149803698147 --------\n",
      "-------- epoch = 293/300 --------\n",
      "batch: 1; loss = 35.16077747294659\n",
      "-------- loss = 35.16077747294659 --------\n",
      "-------- epoch = 294/300 --------\n",
      "batch: 1; loss = 35.160079978115185\n",
      "-------- loss = 35.160079978115185 --------\n",
      "-------- epoch = 295/300 --------\n",
      "batch: 1; loss = 35.1593930808151\n",
      "-------- loss = 35.1593930808151 --------\n",
      "-------- epoch = 296/300 --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1; loss = 35.15870016264441\n",
      "-------- loss = 35.15870016264441 --------\n",
      "-------- epoch = 297/300 --------\n",
      "batch: 1; loss = 35.15801896809646\n",
      "-------- loss = 35.15801896809646 --------\n",
      "-------- epoch = 298/300 --------\n",
      "batch: 1; loss = 35.157353375247844\n",
      "-------- loss = 35.157353375247844 --------\n",
      "-------- epoch = 299/300 --------\n",
      "batch: 1; loss = 35.15670254626553\n",
      "-------- loss = 35.15670254626553 --------\n",
      "-------- epoch = 300/300 --------\n",
      "batch: 1; loss = 35.156080610366416\n",
      "-------- loss = 35.156080610366416 --------\n"
     ]
    }
   ],
   "source": [
    "dl = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.01, weight_decay=0.12)\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    loss_sum = 0\n",
    "    print(f'-------- epoch = {epoch+1}/{epochs} --------')\n",
    "    for batch, (X, Y) in enumerate(dl):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X.to(device))\n",
    "        loss = loss_fn(pred, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 1 == 0:\n",
    "            loss, current = loss.item(), (batch + 1)*len(X)\n",
    "            loss_sum += loss\n",
    "            print(f'batch: {batch+1}; loss = {loss}')\n",
    "    print(f'-------- loss = {loss_sum/(int(len(data)/batch_size))} --------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4a0a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = DataPreprocessing(x_train, y_train, x_test, y_test, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f3189ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1129,  0.0623,  0.7496,  0.2122, -0.1558], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87f39e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_test = torch.utils.data.DataLoader(data_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "540732e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0008340848655626121 :  H\n",
      "0.1947598434840594 :  Li\n",
      "0.03640687289961461 :  Be\n",
      "0.12841287848172542 :  B\n",
      "0.41886599014835707 :  C\n",
      "0.04803317156684019 :  N\n",
      "0.06039342262667619 :  O\n",
      "3.369531130854888 :  F\n",
      "0.005072135000062211 :  Na\n",
      "1.875012851619978 :  Mg\n",
      "14.461210698028278 :  Al\n",
      "0.6696726971124252 :  Si\n",
      "0.07576222257644227 :  P\n",
      "0.06292211769770191 :  S\n",
      "2.8287302454059395e-07 :  Cl\n",
      "0.027735256416706502 :  K\n",
      "0.049194701007093826 :  Ca\n",
      "0.01146515424974801 :  Sc\n",
      "5.231934580706722 :  Ti\n",
      "0.3542265895316354 :  V\n",
      "5.1133496780562 :  Cr\n",
      "0.9401050679876672 :  Mn\n",
      "38.33357916322571 :  Fe\n",
      "1.4269272831967657 :  Co\n",
      "4.759224236758506 :  Ni\n",
      "5.699876268852481 :  Cu\n",
      "3.21769072612056 :  Zn\n",
      "0.5824504133411095 :  Ga\n",
      "0.21228446108191334 :  Ge\n",
      "0.011729742535124453 :  As\n",
      "0.01735588487098904 :  Se\n",
      "0.0003722411833214458 :  Br\n",
      "1.0498774197879541 :  Mo\n",
      "1.7712228023336165 :  Nb\n",
      "0.23322564708545884 :  Zr\n",
      "0.08034628159938145 :  Ce\n",
      "0.3639010697999856 :  W\n",
      "3.862953917348306 :  Pb\n",
      "0.3699995782933264 :  Sb\n",
      "1.8771266750645534 :  Bi\n",
      "0.5337614351093248 :  Cd\n",
      "0.01796902465263679 :  Te\n",
      "3.516772085297142 :  Sn\n",
      "0.15058016069903976 :  Ta\n",
      "0.010929616943636972 :  Ba\n",
      "1.464060455772922 :  Ag\n",
      "0.052452369518478655 :  Sr\n",
      "2.0553859152436678 :  In\n",
      "0.1534534849428222 :  Gd\n",
      "0.09098295755896015 :  Nd\n",
      "0.047136768022482185 :  Th\n",
      "0.14131004953528034 :  Y\n",
      "0.1748220063458707 :  Hf\n",
      "2.8881817724051277 :  Au\n",
      "2.0149429974350115 :  Pd\n",
      "1.7868295470579971 :  Pt\n",
      "0.22067025830425474 :  Ru\n",
      "0.2834710987165794 :  Ir\n",
      "0.27332235863988713 :  Rh\n"
     ]
    }
   ],
   "source": [
    "for x, y in dl:\n",
    "    for i in range(y_train.values.shape[1]):\n",
    "        print(mean_absolute_error(y_train.values[:, i], model(torch.from_numpy(x_train.values).cuda()).cpu().detach().numpy()[:, i]), ': ', df.columns[i+5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61489891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0796792692227024e-05 :  H\n",
      "0.04060606023522769 :  Li\n",
      "0.008494328529222335 :  Be\n",
      "0.08850554129568876 :  B\n",
      "0.23917000890740303 :  C\n",
      "0.03306345758852222 :  N\n",
      "0.02927501477778683 :  O\n",
      "1.9776409650367113 :  F\n",
      "0.0010302026330584657 :  Na\n",
      "0.4098300409970552 :  Mg\n",
      "7.804512410023256 :  Al\n",
      "0.6965237867864569 :  Si\n",
      "0.01810130593521166 :  P\n",
      "0.055130026324692896 :  S\n",
      "2.8288229558999494e-07 :  Cl\n",
      "0.015011222578048803 :  K\n",
      "5.565932945137178 :  Ca\n",
      "0.002826182071394974 :  Sc\n",
      "0.9970620271034409 :  Ti\n",
      "0.2616012785028824 :  V\n",
      "3.630924101764032 :  Cr\n",
      "0.4221406246518764 :  Mn\n",
      "23.481623259057372 :  Fe\n",
      "1.361770214785598 :  Co\n",
      "2.4219893988536647 :  Ni\n",
      "1.0569307414469507 :  Cu\n",
      "0.8574461413587269 :  Zn\n",
      "0.15456088005061103 :  Ga\n",
      "0.058527086493932075 :  Ge\n",
      "0.002418541830668267 :  As\n",
      "0.013016141752399943 :  Se\n",
      "0.00037225574555687095 :  Br\n",
      "0.6637611978700904 :  Mo\n",
      "0.33519616148439285 :  Nb\n",
      "0.03321576523174908 :  Zr\n",
      "0.049458879057095576 :  Ce\n",
      "0.7728058275885451 :  W\n",
      "1.0588991381713195 :  Pb\n",
      "0.09952107829940587 :  Sb\n",
      "0.5747564659153468 :  Bi\n",
      "0.14662424613598748 :  Cd\n",
      "0.018381539270848492 :  Te\n",
      "0.8730946154444736 :  Sn\n",
      "0.030357836030378546 :  Ta\n",
      "0.0028706626856349195 :  Ba\n",
      "0.3582332742846865 :  Ag\n",
      "0.011334824257668874 :  Sr\n",
      "0.5559335882059614 :  In\n",
      "0.03169570723660252 :  Gd\n",
      "0.018797989657129813 :  Nd\n",
      "0.009739736511466692 :  Th\n",
      "0.03502244216426742 :  Y\n",
      "0.040314144488010226 :  Hf\n",
      "0.644448510508328 :  Au\n",
      "0.4451170133577167 :  Pd\n",
      "0.32319016091974523 :  Pt\n",
      "0.05224645398209173 :  Ru\n",
      "0.04928602955622008 :  Ir\n",
      "0.04721065756031843 :  Rh\n"
     ]
    }
   ],
   "source": [
    "for x, y in dl:\n",
    "    for i in range(y_train.values.shape[1]):\n",
    "        print(mean_absolute_error(y_test.values[:, i], model(torch.from_numpy(x_test.values).cuda()).cpu().detach().numpy()[:, i]), ': ', df.columns[i+5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7ee402a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.8000e-01, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        4.0000e-02, 5.0000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 9.0000e-01, 9.8550e+01, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "tensor([[ 0.0000e+00,  1.1000e-02, -0.0000e+00,  1.1300e-01,  5.1600e-01,\n",
      "          3.8000e-02,  4.2000e-02,  2.0970e+00,  3.0000e-03,  1.1500e-01,\n",
      "          1.1840e+00,  5.1700e-01,  4.4000e-02,  8.2000e-02, -0.0000e+00,\n",
      "          1.9000e-02,  2.0000e-03,  0.0000e+00,  1.6400e-01,  1.5800e-01,\n",
      "          4.0400e+00,  1.2700e+00,  8.6293e+01,  8.0600e-01,  3.3830e+00,\n",
      "          3.9100e-01, -1.5500e-01, -3.4000e-02, -1.5000e-02,  0.0000e+00,\n",
      "          8.0000e-03,  0.0000e+00,  7.9000e-01,  7.5000e-02,  2.0000e-02,\n",
      "          5.3000e-02,  1.1500e-01, -2.4200e-01, -2.3000e-02, -1.2400e-01,\n",
      "         -3.7000e-02,  2.0000e-02, -1.3600e-01,  3.2000e-02, -1.0000e-03,\n",
      "         -4.7000e-02,  2.0000e-03, -1.3300e-01,  9.0000e-03,  6.0000e-03,\n",
      "          3.0000e-03,  7.6000e-02, -4.0000e-03, -4.0000e-02, -2.2000e-02,\n",
      "          5.6000e-02, -1.0000e-02,  1.1000e-02,  1.1000e-02]], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<RoundBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(data_test[0][1])\n",
    "print(torch.round(model(data_test[0][0].view(1, -1)), decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d278847d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:35:03] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/learner.cc:767: \n",
      "Parameters: { \"device\" } are not used.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=&#x27;cuda&#x27;, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=2, max_leaves=2,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=12, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=&#x27;cuda&#x27;, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=2, max_leaves=2,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=12, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=2, max_leaves=2,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=12, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, ...)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst = xgboost.XGBRegressor(device='cuda', max_depth=2, max_leaves=2, n_estimators=12)\n",
    "bst.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c38e4d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00769168120836434\n",
      "0.0874581782962533\n",
      "0.014714537296058473\n",
      "0.03946428819589857\n",
      "0.13797194891459702\n",
      "0.018729265232859314\n",
      "0.026061350099850392\n",
      "0.21303188357083802\n",
      "0.008460295110509016\n",
      "0.2582629195957945\n",
      "0.9232496564273556\n",
      "0.35713415485594585\n",
      "0.0344823707405035\n",
      "0.03477692712021048\n",
      "0.007446574047207832\n",
      "0.010681015701721544\n",
      "0.012010139570719957\n",
      "0.009098210791638797\n",
      "0.6354013611638027\n",
      "0.12978285687056137\n",
      "1.5547231701201287\n",
      "0.5502289598468422\n",
      "5.0968618324335075\n",
      "0.6434016281359869\n",
      "2.091537279602272\n",
      "1.4139349910127943\n",
      "0.6471963679952206\n",
      "0.13348342022541637\n",
      "0.03230823511651893\n",
      "0.009670448047095452\n",
      "0.017716030055297458\n",
      "0.007446574047207832\n",
      "0.3493002258403146\n",
      "0.41964800481087927\n",
      "0.07903929501201797\n",
      "0.01783319222873104\n",
      "0.15860368188740548\n",
      "0.4153966632253536\n",
      "0.04425011779437031\n",
      "0.24401785061929535\n",
      "0.03621197202428093\n",
      "0.008162700213902238\n",
      "0.7063356967157386\n",
      "0.06755495532839627\n",
      "0.00829632091867751\n",
      "0.5524388029471687\n",
      "0.012894081531918568\n",
      "0.18559181973446112\n",
      "0.023632075578190283\n",
      "0.016581825068679414\n",
      "0.011634291944944341\n",
      "0.021590168385315633\n",
      "0.026121937477070343\n",
      "0.31427213186803055\n",
      "0.47535629841452054\n",
      "0.26325615289634546\n",
      "0.025498491264879707\n",
      "0.04198585372860881\n",
      "0.03847467840473721\n"
     ]
    }
   ],
   "source": [
    "for i in range(y_train.values.shape[1]):\n",
    "    print(mean_absolute_error(y_train.values[:, i], bst.predict(x_train)[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "941073ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007972831527392069\n",
      "0.37686306331306696\n",
      "0.008783804144362608\n",
      "0.020983339915672937\n",
      "0.13370227871669663\n",
      "0.02067069744310445\n",
      "0.0397633075610631\n",
      "4.77590334662495\n",
      "0.008091525174677372\n",
      "4.464411143422127\n",
      "1.3914956252045103\n",
      "0.5762633233004145\n",
      "0.015754963736401662\n",
      "0.05671492678721746\n",
      "0.007446574047207832\n",
      "0.011656063735071156\n",
      "5.5591741292220025\n",
      "0.010905441569371356\n",
      "0.22527002041869693\n",
      "0.09780587590403025\n",
      "2.246111752933926\n",
      "0.3149305285811424\n",
      "7.772299834198423\n",
      "0.7603244745483001\n",
      "1.2269203242725795\n",
      "0.3901606730884976\n",
      "0.28154729336500167\n",
      "0.027361910790205002\n",
      "0.014019583817571402\n",
      "0.010085693198359676\n",
      "0.01736446669739154\n",
      "0.007446574047207832\n",
      "0.6133915789880686\n",
      "0.13674621394690542\n",
      "0.1662632893357012\n",
      "0.044576527861257396\n",
      "0.7472998434387975\n",
      "1.480289808611075\n",
      "0.32231615400976604\n",
      "1.6326557232853438\n",
      "0.2828021812666621\n",
      "0.008125683292746544\n",
      "2.891656952496204\n",
      "0.06085865184043845\n",
      "0.008042276888671849\n",
      "0.1718890600734287\n",
      "0.007373431490527259\n",
      "0.952602226804528\n",
      "0.41127865158745813\n",
      "0.24830461187391645\n",
      "0.13393682218156755\n",
      "0.3641683031390939\n",
      "0.007488509640097618\n",
      "0.016128893941640854\n",
      "0.10524521768093109\n",
      "0.0268198661506176\n",
      "0.007462582550942898\n",
      "0.008440867066383362\n",
      "0.00745445815846324\n"
     ]
    }
   ],
   "source": [
    "for i in range(y_test.values.shape[1]):\n",
    "    print(mean_absolute_error(y_test.values[:, i], bst.predict(x_test)[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "265a6d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 1.50e-01, 0.00e+00,\n",
       "       0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "       4.00e-02, 5.00e-02, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "       0.00e+00, 0.00e+00, 0.00e+00, 6.00e-01, 9.96e+01, 0.00e+00,\n",
       "       0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "       0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "       0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "       0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "       0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "       0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f980204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0. ,  0. ,  0. ,  0. ,  0.2,  0. ,  0. ,  0. ,  0. ,  0. ,  0.3,\n",
       "         0. ,  0. ,  0.1,  0. ,  0. ,  0. ,  0. ,  0.1,  0. ,  0.2,  0.9,\n",
       "        99.3,  0. , -0.1,  0.1,  0.2,  0. ,  0. ,  0. ,  0. ,  0. , -0. ,\n",
       "        -0. ,  0. ,  0. ,  0. ,  0.1,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         0. ,  0.1,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0.1,\n",
       "         0. ,  0. ,  0. ,  0. ]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(bst.predict(x_train.values[0].reshape(1, -1)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1efe1ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_reg = RandomForestRegressor()\n",
    "rnd_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b3f8d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00021189855072463728 :  H\n",
      "0.08467333333333328 :  Li\n",
      "0.024655255652173934 :  Be\n",
      "0.03872704927536232 :  B\n",
      "0.0801671014492753 :  C\n",
      "0.011236304347826083 :  N\n",
      "0.01664072463768117 :  O\n",
      "1.5942028985507246 :  F\n",
      "0.0027285507246376825 :  Na\n",
      "0.7561949420289859 :  Mg\n",
      "1.1783672695652132 :  Al\n",
      "0.2508561449275365 :  Si\n",
      "0.04202117971014495 :  P\n",
      "0.019154999999999985 :  S\n",
      "0.0 :  Cl\n",
      "0.010344927536231882 :  K\n",
      "0.02414434782608698 :  Ca\n",
      "0.005465217391304348 :  Sc\n",
      "1.0243209275362306 :  Ti\n",
      "0.13975056521739124 :  V\n",
      "1.2829586811594205 :  Cr\n",
      "0.3855888405797102 :  Mn\n",
      "2.927668739130431 :  Fe\n",
      "0.5071504347826085 :  Co\n",
      "1.3397591130434787 :  Ni\n",
      "1.8260214202898604 :  Cu\n",
      "1.063838715942029 :  Zn\n",
      "0.2921034347826086 :  Ga\n",
      "0.1046028985507247 :  Ge\n",
      "0.004473217391304352 :  As\n",
      "0.009886284057971009 :  Se\n",
      "0.0 :  Br\n",
      "0.29152971014492735 :  Mo\n",
      "0.9670050724637683 :  Nb\n",
      "0.07682057971014503 :  Zr\n",
      "0.022116956521739145 :  Ce\n",
      "0.1683463768115944 :  W\n",
      "0.7109257449275391 :  Pb\n",
      "0.05600802898550733 :  Sb\n",
      "0.5838660869565202 :  Bi\n",
      "0.1346407101449275 :  Cd\n",
      "0.0006187246376811589 :  Te\n",
      "0.7545587623188418 :  Sn\n",
      "0.0954853623188405 :  Ta\n",
      "0.005470000000000002 :  Ba\n",
      "0.6467215942028984 :  Ag\n",
      "0.02700050724637685 :  Sr\n",
      "0.30503333333333416 :  In\n",
      "0.06817391304347835 :  Gd\n",
      "0.04041739130434784 :  Nd\n",
      "0.0209391304347826 :  Th\n",
      "0.06018552173913036 :  Y\n",
      "0.09683478260869575 :  Hf\n",
      "1.4162028985507242 :  Au\n",
      "1.040403231884058 :  Pd\n",
      "0.8607739130434777 :  Pt\n",
      "0.10224057971014504 :  Ru\n",
      "0.14304202898550725 :  Ir\n",
      "0.1379130434782607 :  Rh\n"
     ]
    }
   ],
   "source": [
    "for i in range(y_train.values.shape[1]):\n",
    "    print(mean_absolute_error(y_train.values[:, i], rnd_reg.predict(x_train)[:, i]), ': ', df.columns[i+5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6cbfbaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004483333333333333 :  H\n",
      "0.11545999999999997 :  Li\n",
      "0.008584888888888888 :  Be\n",
      "0.015958833333333325 :  B\n",
      "0.19301155555555574 :  C\n",
      "0.010935888888888887 :  N\n",
      "0.02954944444444443 :  O\n",
      "3.0555555555555554 :  F\n",
      "0.0026538888888888897 :  Na\n",
      "1.1084323333333337 :  Mg\n",
      "2.768348000000002 :  Al\n",
      "0.49143633333333336 :  Si\n",
      "0.011636611111111118 :  P\n",
      "0.052160472222222184 :  S\n",
      "0.0 :  Cl\n",
      "0.012811111111111113 :  K\n",
      "5.567177777777778 :  Ca\n",
      "0.0068155555555555565 :  Sc\n",
      "2.2177391666666666 :  Ti\n",
      "0.2327031111111111 :  V\n",
      "1.0194980555555555 :  Cr\n",
      "0.3294849999999999 :  Mn\n",
      "6.984278499999975 :  Fe\n",
      "0.7113027777777778 :  Co\n",
      "1.4025625555555554 :  Ni\n",
      "1.8166443888888897 :  Cu\n",
      "2.6407021111111115 :  Zn\n",
      "0.028101611111111105 :  Ga\n",
      "0.012133333333333336 :  Ge\n",
      "0.0016067777777777775 :  As\n",
      "0.01234901111111111 :  Se\n",
      "0.0 :  Br\n",
      "0.325838888888889 :  Mo\n",
      "0.4710083333333332 :  Nb\n",
      "0.09848555555555556 :  Zr\n",
      "0.009522777777777778 :  Ce\n",
      "0.6863222222222222 :  W\n",
      "0.2693156666666666 :  Pb\n",
      "0.031650444444444446 :  Sb\n",
      "0.18104111111111115 :  Bi\n",
      "0.04301427777777776 :  Cd\n",
      "2.2222222222222223e-05 :  Te\n",
      "0.4594737222222221 :  Sn\n",
      "0.068505 :  Ta\n",
      "0.0006138888888888888 :  Ba\n",
      "0.10820166666666668 :  Ag\n",
      "0.02173805555555557 :  Sr\n",
      "0.1345777777777778 :  In\n",
      "0.09333333333333337 :  Gd\n",
      "0.055333333333333366 :  Nd\n",
      "0.028666666666666656 :  Th\n",
      "0.08236566666666666 :  Y\n",
      "0.041244444444444445 :  Hf\n",
      "0.1454444444444444 :  Au\n",
      "0.04421255555555556 :  Pd\n",
      "0.09575555555555554 :  Pt\n",
      "0.008555555555555556 :  Ru\n",
      "0.01795222222222222 :  Ir\n",
      "0.017333333333333333 :  Rh\n"
     ]
    }
   ],
   "source": [
    "for i in range(y_test.values.shape[1]):\n",
    "    print(mean_absolute_error(y_test.values[:, i], rnd_reg.predict(x_test)[:, i]), ': ', df.columns[i+5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80cf952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
